[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m an experienced Data Scientist with specialized skills in machine learning-based solutions. I enjoy staying on top of cutting-edge data technologies, including big data platforms, deep learning, optimization methods, and business analytics. My current work involves building data-driven products to enable smarter recommendations for Microsoft Partners, M365 service administrators and end-users to ensure the best usage of M365 services. Before that, I have experience working in various verticals like agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing business model."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\nVideo\n\n\n\n\n\n\n\nFeb 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nJan 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nVision\n\n\n\n\n\n\n\nOct 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nJun 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30 min\n\n\n\nRecommender System\n\n\nMachine learning\n\n\nBusiness\n\n\n\n\n\n\n\nApr 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nlaunch\n\n\nannouncement\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-02-19-launch/index.html",
    "href": "posts/2018-02-19-launch/index.html",
    "title": "Website launch",
    "section": "",
    "text": "I am intending to use this website to publish blogs, workshop and any material which would be releveant in Data science field.\nWill see you guys in next post.\n-Aayush"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "",
    "text": "In this post, I am going to write about Recommender systems, how they are used in many e-commerce websites. The post will also cover about building simple recommender system models using Matrix Factorization algorithm using lightFM package and my recommender system cookbook. The post will focus on business use cases and simple implementations. The post only cover basic intuition around algorithms and will provide links to resources if you want to understand the math behind the algorithm."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Motivation",
    "text": "Motivation\nI am an avid reader and a believer in open source education and continuously expand my knowledge around data science & computer science using online courses, blogs, Github repositories and participating in data science competitions. While searching for quality content on the internet, I have come across various learning links which either focus on the implementation of the algorithm using specific data/modeling technique in ABC language or focus on business impact/results using the broad concept of a family of algorithms(like classification, forecasting, recommender systems etc.) but don’t go into details of how to do it. So the idea is to write some blogs which can combine both business use cases with codes & algorithmic intuition to provide a holistic view of how data science is used in business scenarios. \nAs the world is becoming more digital, we are already getting used to a lot of personalized experience and the algorithm which help us achieve this falls in the family of recommender systems. Almost every web-based platform is using some recommender system to provide customized content. Following are the companies I admire the most."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "What is personalization?",
    "text": "What is personalization?\nPersonalization is a technique of dynamically tailoring your content based on needs of each user. Simple examples of personalization could be movie recommendation on Netflix, personalized email targeting/re-targeting by e-commerce platforms, item recommendation on Amazon, etc. Personalization helps us achieve these four Rs - - Recognize: Know customer’s and prospects’ profiles, including demographics, geography, and expressed and shared interests. - Remember: Recall customers’ history, primarily how they act as expressed by what they browse and buy - Reach: Deliver the right promotion, content, recommendation for a customer based on actions, preferences, and interests - Relevance: Deliver personalization within the context of the digital experience based on who customers are, where they are located and what time of year it is"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Why personalization?",
    "text": "Why personalization?\nPersonalization has a lot of benefits for both users and companies. For users, it makes their life easy as they only get to see more relevant stuff to them (unless it’s an advertisement, even they are personalized). For business benefits are countless but here are few which I would like to mention - - Enhance customer experience: Personalization reduces the clutter and enhances the customer experience by showing relevant content - Cross-sell/ Up-sell opportunities: Relevant product offerings based on customer preferences can lead to increasing products visibility and eventually selling more products - Increased basket size: Personalized experience and targeting ultimately leads to increased basket size and frequent purchases - Increased customer loyalty: In the digital world, customer retention/loyalty is the most prominent problem faced by many companies as finding a replacement for a particular service is quite easy. According to a Forbes article, Forty-four percent of consumers say they will likely repeat after a personalized experience"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Introduction to Matrix factorization",
    "text": "Introduction to Matrix factorization\nMatrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc.  \n Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it’s more female-driven the value is typically low.  \nFor more information on matrix factorization and factorization machines you can read these articles -  Matrix Factorization: A Simple Tutorial and Implementation in Python  Introductory Guide – Factorization Machines & their application on huge datasets (with codes in Python)"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "HandOn: Building recommender system using LightFM package in Python",
    "text": "HandOn: Building recommender system using LightFM package in Python\nIn the hands-on section, we will be building recommender system for different scenarios which we typically see in many companies using LightFM package and MovieLens data. We are using small size data which contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users\n\nData\nLet’s start by importing data, recommender system cookbook and preprocessing cookbook files for this hands-on section. I have written these reusable generic cookbook codes to increase productivity and write clean/modular codes; you will see we can build a recommender system using 10-15 lines of code by using these cookbooks(do more with less!).\n# Importing Libraries and cookbooks\nfrom recsys import * ## recommender system cookbook\nfrom generic_preprocessing import * ## pre-processing code\nfrom IPython.display import HTML ## Setting display options for Ipython Notebook\n# Importing rating data and having a look\nratings = pd.read_csv('./ml-latest-small/ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\n\n\nmovieId\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n0\n\n\n1\n\n\n31\n\n\n2.5\n\n\n1260759144\n\n\n\n\n1\n\n\n1\n\n\n1029\n\n\n3.0\n\n\n1260759179\n\n\n\n\n2\n\n\n1\n\n\n1061\n\n\n3.0\n\n\n1260759182\n\n\n\n\n3\n\n\n1\n\n\n1129\n\n\n2.0\n\n\n1260759185\n\n\n\n\n4\n\n\n1\n\n\n1172\n\n\n4.0\n\n\n1260759205\n\n\n\n\n\n\nAs we can see rating data contain user id, movie id and a rating between 0.5 to 5 with a timestamp representing when the rating was given.\n# Importing movie data and having a look at first five columns\nmovies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\n\n\ntitle\n\n\ngenres\n\n\n\n\n\n\n0\n\n\n1\n\n\nToy Story (1995)\n\n\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n1\n\n\n2\n\n\nJumanji (1995)\n\n\nAdventure|Children|Fantasy\n\n\n\n\n2\n\n\n3\n\n\nGrumpier Old Men (1995)\n\n\nComedy|Romance\n\n\n\n\n3\n\n\n4\n\n\nWaiting to Exhale (1995)\n\n\nComedy|Drama|Romance\n\n\n\n\n4\n\n\n5\n\n\nFather of the Bride Part II (1995)\n\n\nComedy\n\n\n\n\n\n\nMovie data consist of movie id, their title, and genre they belong.\n\n\nPreprocessing\nAs I mentioned before, to create a recommender system we need to start by creating an interaction matrix. For this task, we will use the create_interaction_matrix function from the recsys cookbook. This function requires you to input a pandas dataframe and necessary information like column name for user id, item id, and rating. It also takes an additional parameter threshold if norm=True which means any rating above the mentioned threshold is considered a positive rating. In our case, we don’t have to normalize our data, but in cases of retail data any purchase of a particular type of item can be considered a positive rating, quantity doesn’t matter.\n# Creating interaction matrix using rating data\ninteractions = create_interaction_matrix(df = ratings,\n                                         user_col = 'userId',\n                                         item_col = 'movieId',\n                                         rating_col = 'rating')\ninteractions.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n…\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n4\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n5\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n5 rows × 9066 columns\n\n\nAs we can see the data is created in an interaction format where rows represent each user and columns represent each movie id with ratings as values.  We will also create user and item dictionaries to later convert user_id to user_name or movie_id to movie_name by using create_user_dict and create_item dict function.\n# Create User Dict\nuser_dict = create_user_dict(interactions=interactions)\n# Create Item dict\nmovies_dict = create_item_dict(df = movies,\n                               id_col = 'movieId',\n                               name_col = 'title')\n\n\nBuilding Matrix Factorization model\nTo build a matrix factorization model, we will use the runMF function which will take following input -\n- interaction matrix: Interaction matrix created in the previous section - n_components: Number of embedding generated for each user and item - loss: We need to define a loss function, in this case, we are using warp loss because we mostly care about the ranking of data, i.e, which items should we show first - epoch: Number of times to run - n_jobs: Number of cores to use in parallel processing\nmf_model = runMF(interactions = interactions,\n                 n_components = 30,\n                 loss = 'warp',\n                 epoch = 30,\n                 n_jobs = 4)\nNow we have built our matrix factorization model we can now do some interesting things. There are various use cases which can be solved by using this model for a web platform let’s look into them.\n\n\nUsecase 1: Item recommendation to a user\nIn this use case, we want to show a user, items he might be interested in buying/viewing based on his/her interactions done in the past. Typical industry examples for this are like “Deals recommended for you” on Amazon or “Top pics for a user” on Netflix or personalized email campaigns. \nWe can use the sample_recommendation_user function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, user_id and the number of items as input and return the list of item id’s a user may be interested in interacting.\n## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = mf_model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = movies_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)\nKnown Likes:\n1- The Hunger Games: Catching Fire (2013)\n2- Gravity (2013)\n3- Dark Knight Rises, The (2012)\n4- The Hunger Games (2012)\n5- Town, The (2010)\n6- Exit Through the Gift Shop (2010)\n7- Bank Job, The (2008)\n8- Departed, The (2006)\n9- Bourne Identity, The (1988)\n10- Step Into Liquid (2002)\n11- SLC Punk! (1998)\n12- Last of the Mohicans, The (1992)\n13- Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)\n14- Robin Hood: Prince of Thieves (1991)\n15- Citizen Kane (1941)\n16- Trainspotting (1996)\n17- Pulp Fiction (1994)\n18- Usual Suspects, The (1995)\n\n Recommended Items:\n1- Dark Knight, The (2008)\n2- Inception (2010)\n3- Iron Man (2008)\n4- Shutter Island (2010)\n5- Fight Club (1999)\n6- Avatar (2009)\n7- Forrest Gump (1994)\n8- District 9 (2009)\n9- WALL·E (2008)\n10- Matrix, The (1999)\nprint(rec_list)\n[593L, 260L, 110L, 480L, 47L, 527L, 344L, 858L, 231L, 780L]\nAs we can see in this case user is interested in “Dark Knight Rises(2012)” so the first recommendation is “The Dark Knight(2008)”. This user also seems to have a strong liking towards movies in drama, sci-fi and thriller genre and there are many movies recommended in the same genre like Dark Knight(Drama/Crime), Inception(Sci-Fi, Thriller), Iron Man(Sci-FI thriller), Shutter Island(Drame/Thriller), Fight club(drama), Avatar(Sci-fi), Forrest Gump(Drama), District 9(Thriller), Wall-E(Sci-fi), The Matrix(Sci-Fi) \nSimilar models can also be used for building sections like “Based on your recent browsing history” recommendations by just changing the rating matrix only to contain interaction which is recent and based on browsing history visits on specific items.\n\n\nUsecase 2: User recommendation to a item\nIn this use case, we will discuss how we can recommend a list of users specific to a particular item. Example of such cases is when you are running a promotion on an item and want to run an e-mail campaign around this promotional item to only 10,000 users who might be interested in this item.\n\nWe can use the sample_recommendation_item function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, item_id and the number of users as input and return the list of user id’s who are more likely be interested in the item.\n## Calling 15 user recommendation for item id 1\nsample_recommendation_item(model = mf_model,\n                           interactions = interactions,\n                           item_id = 1,\n                           user_dict = user_dict,\n                           item_dict = movies_dict,\n                           number_of_user = 15)\n[116, 410, 449, 657, 448, 633, 172, 109, 513, 44, 498, 459, 317, 415, 495]\nAs you can see function return a list of userID who might be interested in item id 1. Another example why you might need such model is when there is an old inventory sitting in your warehouse which needs to clear up otherwise you might have to write it off, and you want to clear it by giving some discount to users who might be interested in buying.\n\n\nUsecase 3: Item recommendation to items\nIn this use case, we will discuss how we can recommend a list of items specific to a particular item. This kind of models will help you to find similar/related items or items which can be bundled together. Typical industry use case for such models are in cross-selling and up-selling opportunities on product page like “Products related to this item”, “Frequently bought together”, “Customers who bought this also bought this” and “Customers who viewed this item also viewed”.  “Customers who bought this also bought this” and “Customers who viewed this item also viewed” can also be solved through market basket analysis.\n\nTo achieve this use case, we will create a cosine distance matrix using item embeddings generated by matrix factorization model. This will help us calculate similarity b/w items, and then we can recommend top N similar item to an item of interest. First step is to create a item-item distance matrix using the create_item_emdedding_distance_matrix function. This function takes matrix factorization models and interaction matrix as input and returns an item_embedding_distance_matrix.\n## Creating item-item distance matrix\nitem_item_dist = create_item_emdedding_distance_matrix(model = mf_model,\n                                                       interactions = interactions)\n## Checking item embedding distance matrix\nitem_item_dist.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n…\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nmovieId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1.000000\n\n\n0.760719\n\n\n0.491280\n\n\n0.427250\n\n\n0.484597\n\n\n0.740024\n\n\n0.486644\n\n\n0.094009\n\n\n-0.083986\n\n\n0.567389\n\n\n…\n\n\n-0.732112\n\n\n-0.297997\n\n\n-0.451733\n\n\n-0.767141\n\n\n-0.501647\n\n\n-0.270280\n\n\n-0.455277\n\n\n-0.292823\n\n\n-0.337935\n\n\n-0.636147\n\n\n\n\n2\n\n\n0.760719\n\n\n1.000000\n\n\n0.446414\n\n\n0.504502\n\n\n0.525171\n\n\n0.572113\n\n\n0.364393\n\n\n0.290633\n\n\n0.231926\n\n\n0.653033\n\n\n…\n\n\n-0.748452\n\n\n-0.307634\n\n\n-0.165400\n\n\n-0.526614\n\n\n-0.146751\n\n\n-0.156305\n\n\n-0.223818\n\n\n-0.138412\n\n\n-0.209538\n\n\n-0.733489\n\n\n\n\n3\n\n\n0.491280\n\n\n0.446414\n\n\n1.000000\n\n\n0.627473\n\n\n0.769991\n\n\n0.544175\n\n\n0.632008\n\n\n0.336824\n\n\n0.392284\n\n\n0.510592\n\n\n…\n\n\n-0.331028\n\n\n-0.264556\n\n\n-0.308592\n\n\n-0.285085\n\n\n-0.046424\n\n\n-0.165821\n\n\n-0.183842\n\n\n-0.143613\n\n\n-0.156418\n\n\n-0.378811\n\n\n\n\n4\n\n\n0.427250\n\n\n0.504502\n\n\n0.627473\n\n\n1.000000\n\n\n0.582582\n\n\n0.543208\n\n\n0.602390\n\n\n0.655708\n\n\n0.527346\n\n\n0.471166\n\n\n…\n\n\n-0.380431\n\n\n-0.163091\n\n\n-0.232833\n\n\n-0.334746\n\n\n-0.052832\n\n\n-0.266185\n\n\n-0.158415\n\n\n-0.211618\n\n\n-0.232351\n\n\n-0.469629\n\n\n\n\n5\n\n\n0.484597\n\n\n0.525171\n\n\n0.769991\n\n\n0.582582\n\n\n1.000000\n\n\n0.354141\n\n\n0.639958\n\n\n0.396447\n\n\n0.432026\n\n\n0.385051\n\n\n…\n\n\n-0.273074\n\n\n-0.280585\n\n\n-0.306195\n\n\n-0.265243\n\n\n0.012961\n\n\n-0.225142\n\n\n-0.317043\n\n\n-0.136875\n\n\n-0.122382\n\n\n-0.312858\n\n\n\n\n\n\n5 rows × 9066 columns\n\n\nAs we can see the matrix have movies as both row and columns and the value represents the cosine distance between them. Next step is to use item_item_recommendation function to get top N items with respect to an item_id. This function takes item embedding distance matrix, item_id, item_dictionary and number of items to be recommended as input and return similar item list as output.\n## Calling 10 recommended items for item id \nrec_list = item_item_recommendation(item_emdedding_distance_matrix = item_item_dist,\n                                    item_id = 5378,\n                                    item_dict = movies_dict,\n                                    n_items = 10)\nItem of interest :Star Wars: Episode II - Attack of the Clones (2002)\nItem similar to the above item:\n1- Star Wars: Episode III - Revenge of the Sith (2005)\n2- Lord of the Rings: The Two Towers, The (2002)\n3- Lord of the Rings: The Fellowship of the Ring, The (2001)\n4- Lord of the Rings: The Return of the King, The (2003)\n5- Matrix Reloaded, The (2003)\n6- Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n7- Gladiator (2000)\n8- Spider-Man (2002)\n9- Minority Report (2002)\n10- Mission: Impossible II (2000)\nAs we can see for “Star Wars: Episode II - Attack of the Clones (2002)” movie we are getting it’s next released movies which is “Star Wars: Episode III - Revenge of the Sith (2005)” as the first recommendation."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Summary",
    "text": "Summary\nLike any other blog, this method isn’t perfect for every application, but the same ideas can work if we use it effectively. There is a lot of advancements in recommender systems with the advent of Deep learning. While there is room for improvement, I am pleased with how it has been working for me so far. I might write about deep learning based recommender systems later sometime.\nIn the meantime, I hope you enjoyed reading, and feel free to use my code to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "title": "Building Neural Network from scratch",
    "section": "",
    "text": "In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let’s start with the very basics of a neural network which is Multilayer Perceptron. You can find the same blog in notebook version here."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "title": "Building Neural Network from scratch",
    "section": "What is a neural network?",
    "text": "What is a neural network?\nA neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it’s the same with a neural network."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "title": "Building Neural Network from scratch",
    "section": "What is a Multi layer perceptron?",
    "text": "What is a Multi layer perceptron?\nMulti-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let’s start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.   \n\nFig 1: Perceptron image\n\n\n\nImage credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/\n\nNow, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology.  Dense layer,   Note that bias term is now a vector and W is a weight matrix  \n\nFig: Single dense layer perceptron network\n\n\n\nImage credit=http://www.texample.net/tikz/examples/neural-network/\n\nNow we understand dense layer let’s add a bunch of them, and that network becomes a multi-layer perceptron network.\n\n\nFig: Multi layer perceptron network\n\n\n\nImage credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2s\n\nIf you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero.  ReLU function   Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a feed-forward loop and backpropagation loop. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don’t understand the concept of gradient weight updates and SGD, I recommend you to watch week 1 of Machine learning by Andrew NG lectures.\nSo, to summarize a neural network needs few building blocks\n\nDense layer - a fully-connected layer, \nReLU layer (or any other activation function to introduce non-linearity)\nLoss function - (crossentropy in case of multi-class classification problem)\nBackprop algorithm - a stochastic gradient descent with backpropageted gradients\n\nLet’s approach them one at a time."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "title": "Building Neural Network from scratch",
    "section": "Coding Starts here:",
    "text": "Coding Starts here:\nLet’s start by importing some libraires required for creating our neural network.\nfrom __future__ import print_function\nimport numpy as np ## For numerical python\nnp.random.seed(42)\nEvery layer will have a forward pass and backpass implementation. Let’s create a main class layer which can do a forward pass .forward() and Backward pass .backward().\nclass Layer:\n    \n    #A building block. Each layer is capable of performing two things:\n\n    #- Process input to get output:           output = layer.forward(input)\n    \n    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n    \n    #Some layers also have learnable parameters which they update during layer.backward.\n    \n    def __init__(self):\n        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n        # A dummy layer does nothing\n        pass\n    \n    def forward(self, input):\n        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        \n        # A dummy layer just returns whatever it gets as input.\n        return input\n\n    def backward(self, input, grad_output):\n        # Performs a backpropagation step through the layer, with respect to the given input.\n        \n        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n        \n        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n        \n        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n        \n        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n        \n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n        num_units = input.shape[1]\n        \n        d_layer_d_input = np.eye(num_units)\n        \n        return np.dot(grad_output, d_layer_d_input) # chain rule\n\nNonlinearity ReLU layer\nThis is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\nclass ReLU(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        relu_grad = input > 0\n        return grad_output*relu_grad \n\n\nDense layer\nNow let’s build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\nA dense layer applies affine transformation. In a vectorized form, it can be described as: \nWhere * X is an object-feature matrix of shape [batch_size, num_features], * W is a weight matrix [num_features, num_outputs] * and b is a vector of num_outputs biases.\nBoth W and b are initialized during layer creation and updated each time backward is called. Note that we are using Xavier initialization which is a trick to train our model to converge faster read more. Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        # A dense layer is a layer which performs a learned affine transformation:\n        # f(x) = <W*x> + b\n        \n        self.learning_rate = learning_rate\n        self.weights = np.random.normal(loc=0.0, \n                                        scale = np.sqrt(2/(input_units+output_units)), \n                                        size = (input_units,output_units))\n        self.biases = np.zeros(output_units)\n        \n    def forward(self,input):\n        # Perform an affine transformation:\n        # f(x) = <W*x> + b\n        \n        # input shape: [batch, input_units]\n        # output shape: [batch, output units]\n        \n        return np.dot(input,self.weights) + self.biases\n    \n    def backward(self,input,grad_output):\n        # compute d f / d x = d f / d dense * d dense / d x\n        # where d dense/ d x = weights transposed\n        grad_input = np.dot(grad_output, self.weights.T)\n        \n        # compute gradient w.r.t. weights and biases\n        grad_weights = np.dot(input.T, grad_output)\n        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n        \n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        \n        # Here we perform a stochastic gradient descent step. \n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n        \n        return grad_input\n\n\nThe loss function\nSince we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\nIf we write down the expression for crossentropy as a function of softmax logits (a), you’ll see: \n  If we take a closer look, we’ll see that it can be rewritten as: \n  It’s called Log-softmax and it’s better than naive log(softmax(a)) in all aspects: * Better numerical stability * Easier to get derivative right * Marginally faster to compute\nSo why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\ndef softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n    \n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n    \n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n    \n    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- ones_for_answers + softmax) / logits.shape[0]\n\n\nFull network\nNow let’s combine what we’ve just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let’s import it!.\nimport keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) / 255.\n    X_test = X_test.astype(float) / 255.\n\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n\n## Let's look at some example\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n\nWe’ll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\nnetwork = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\ndef forward(network, X):\n    # Compute activations of all network layers by applying them sequentially.\n    # Return a list of activations for each layer. \n    \n    activations = []\n    input = X\n\n    # Looping through each layer\n    for l in network:\n        activations.append(l.forward(input))\n        # Updating input to last layer output\n        input = activations[-1]\n    \n    assert len(activations) == len(network)\n    return activations\n\ndef predict(network,X):\n    # Compute network predictions. Returning indices of largest Logit probability\n\n    logits = forward(network,X)[-1]\n    return logits.argmax(axis=-1)\n\ndef train(network,X,y):\n    # Train our network on a given batch of X and y.\n    # We first need to run forward to get all layer activations.\n    # Then we can run layer.backward going from last to first layer.\n    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n    \n    \n    # Get the layer activations\n    layer_activations = forward(network,X)\n    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n    logits = layer_activations[-1]\n    \n    # Compute the loss and the initial gradient\n    loss = softmax_crossentropy_with_logits(logits,y)\n    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n    \n    # Propagate gradients through the network\n    # Reverse propogation as this is backprop\n    for layer_index in range(len(network))[::-1]:\n        layer = network[layer_index]\n        \n        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n        \n    return np.mean(loss)\n\n\nTraining loop\nWe split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent.\nfrom tqdm import trange\ndef iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.random.permutation(len(inputs))\n    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\nfrom IPython.display import clear_output\ntrain_log = []\nval_log = []\nfor epoch in range(25):\n\n    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n        train(network,x_batch,y_batch)\n    \n    train_log.append(np.mean(predict(network,X_train)==y_train))\n    val_log.append(np.mean(predict(network,X_val)==y_val))\n    \n    clear_output()\n    print(\"Epoch\",epoch)\n    print(\"Train accuracy:\",train_log[-1])\n    print(\"Val accuracy:\",val_log[-1])\n    plt.plot(train_log,label='train accuracy')\n    plt.plot(val_log,label='val accuracy')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n    \nEpoch 24\nTrain accuracy: 1.0\nVal accuracy: 0.9809\n\nAs we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
  },
  {
    "objectID": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "href": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "title": "Multi-Layer perceptron using Tensorflow",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "href": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "title": "Leaf Disease detection by Tranfer learning using FastAI V1 library",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "href": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "title": "MultiLayer Perceptron using Fastai and Pytorch",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "href": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "title": "Real-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "href": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "title": "Finding similar images using Deep learning and Locality Sensitive Hashing",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\nAudio\n\n\nVision\n\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nOct 18, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nCarlson School of Management, University of Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nVeritas Technologies LLC, 2815 Cleveland Ave N Roseville, MN 55113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nNov 8, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nClustering\n\n\n\n\n\n\n\nOct 11, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nKeras\n\n\n\n\n\n\n\nSep 3, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2017-09-13-introduction-to-keras/index.html",
    "href": "talks/2017-09-13-introduction-to-keras/index.html",
    "title": "Introduction to Keras",
    "section": "",
    "text": "Link to the meetup group\nLink to Git repo\nPhoto of me presenting a real time object detection application using deep learning and openCV."
  },
  {
    "objectID": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "href": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "title": "Deep dive in Hierarchical clustering",
    "section": "",
    "text": "Link to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2017-11-08-Transfer-Learning/index.html",
    "href": "talks/2017-11-08-Transfer-Learning/index.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Link to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "href": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "title": "Model building, Explainability, and Deployment",
    "section": "",
    "text": "Link to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2018-10-18-msba_devops/index.html",
    "href": "talks/2018-10-18-msba_devops/index.html",
    "title": "DevOps for Data scientist",
    "section": "",
    "text": "Link to workshop content\nWorkshop Content\n\nWhat is DevOps? Why is it needed?\nModel building using cookbooks in Python\nVersion control using GIT\nDeploying Model as an API using Flask and microservices framework\nPackaging applications using Docker\nScaling and Deploying applications in Google cloud using Docker and Kubernetes"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Youtube Link - (1:10:00)\nAI For Orcas Webiste"
  }
]