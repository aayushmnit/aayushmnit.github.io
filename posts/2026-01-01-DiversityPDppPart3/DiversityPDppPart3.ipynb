{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Diversity in Recommendations using Personalized DPP (pDPP)\n",
    "author: Aayush Agrawal\n",
    "date: \"2026-01-01\"\n",
    "categories: [Recommender System, Machine Learning, Diversity]\n",
    "image: \"./pDPP_infographics.jpg\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Part 3 of the diversity series: Why one-size-fits-all diversity doesn't work, and how to personalize it per user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure align = \"center\">\n",
    "    <img src=\"./pDPP_infographics.jpg\" style=\"width:100%\">\n",
    "<figcaption align = \"center\">\n",
    "    Figure: pDPP infographic. Credit: [NotebookLM](https://notebooklm.google.com/)\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Part 2](https://aayushmnit.com/posts/2025-12-28-DiversityDPPPart2/DiversityDPPPart2.html), we explored how DPP balances relevance and diversity using a single parameter `α`. But there's a catch: we used the same `α` for every user.\n",
    "\n",
    "Think about it. On a video platform like YouTube or Reels, you'll find:\n",
    "\n",
    "- **User A**: Watches nothing but true crime documentaries. Their last 50 videos? All true crime. They know what they like.\n",
    "- **User B**: Watches cooking tutorials on Monday, standup comedy on Wednesday, and tech reviews on weekends. Their history looks like a buffet.\n",
    "\n",
    "Should we show both users the same amount of diversity? Probably not.\n",
    "\n",
    "User A has a clear, focused preference. Pushing diverse content might feel random or irrelevant to them. User B, on the other hand, has shown they *enjoy* variety. A narrowly focused feed might bore them.\n",
    "\n",
    "This is the core limitation of standard DPP: it treats diversity as a system-level knob, not a user-level preference. The solution? **Personalized DPP (pDPP)**, which adapts the diversity-relevance tradeoff per user based on their historical behavior.\n",
    "\n",
    "The idea comes from a [Huawei research paper on personalized re-ranking](https://arxiv.org/pdf/2004.06390), and the implementation is surprisingly simple: we measure how \"diverse\" each user's past interactions have been using information theory (Shannon entropy), then use that to personalize `α`.\n",
    "\n",
    "In this post, we'll:\n",
    "\n",
    "* **Quantify user diversity appetite** using Shannon entropy over interaction history\n",
    "* **Derive a personalized α** that scales diversity based on each user's behavior\n",
    "* **Implement pDPP** by extending our DPP code from Part 2\n",
    "* **Compare results** side-by-side for narrow vs. broad-interest users\n",
    "\n",
    "Let's get into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Core Insight: Users Have Different Diversity Appetites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we measure a user's appetite for diversity? The Huawei paper uses **Shannon entropy** over the user's interaction history.\n",
    "\n",
    "Shannon entropy measures the \"surprise\" or \"spread\" in a distribution. If a user watches videos from 10 genres equally, entropy is high (maximum uncertainty about what they'll watch next). If they watch 95% from one genre, entropy is low (very predictable).\n",
    "\n",
    "This gives us a principled, interpretable metric: we're not guessing what users want; we're measuring what they've demonstrated through their actions.\n",
    "\n",
    "| User Type | Interaction Pattern | Entropy | Diversity Preference |\n",
    "|-----------|-------------------|---------|---------------------|\n",
    "| Focused (User A) | 90% one genre | Low | Prefers less diversity |\n",
    "| Explorer (User B) | Spread across genres | High | Prefers more diversity |\n",
    "\n",
    "The next step is to turn this entropy score into a personalized `α` value. A user with high entropy gets a higher `α` (more diversity weight), while a user with low entropy gets a lower `α` (more relevance weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math: Computing User Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize the intuition from the previous section. For a user `u`, we compute Shannon entropy over their genre distribution:\n",
    "\n",
    "`H(u) = -Σ P(g|u) × log(P(g|u))`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `P(g|u)` is the probability of genre `g` in user `u`'s interaction history\n",
    "- The sum is over all genres the user has interacted with\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T22:38:14.918343Z",
     "start_time": "2026-01-01T22:38:14.915511Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T22:37:12.183976Z",
     "start_time": "2026-01-01T22:37:12.179869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A(narrow taste) entropy: 0.500\n",
      "User B(diverse taste) entropy: 1.609\n"
     ]
    }
   ],
   "source": [
    "def compute_entropy(genre_list: list[str]) -> float:\n",
    "    \"\"\"Compute Shannon entropy over a list of genre interactions.\"\"\"\n",
    "    if not genre_list: return 0.0\n",
    "    \n",
    "    genre_counts = Counter(genre_list)\n",
    "    total = len(genre_list)\n",
    "    \n",
    "    entropy = 0.0\n",
    "    for count in genre_counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * np.log(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# User A: watches mostly one genre (narrow taste)\n",
    "user_a_genres = ['action', 'action', 'action', 'action', 'comedy']\n",
    "\n",
    "# User B: watches many genres (diverse taste)\n",
    "user_b_genres = ['action', 'comedy', 'drama', 'documentary', 'horror']\n",
    "\n",
    "print(f\"User A(narrow taste) entropy: {compute_entropy(user_a_genres):.3f}\")\n",
    "print(f\"User B(diverse taste) entropy: {compute_entropy(user_b_genres):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers confirm our intuition:\n",
    "\n",
    "- **User A (entropy = 0.500)**: With 4 action + 1 comedy, their behavior is highly predictable. Low entropy means low \"surprise\" in what they'll watch next.\n",
    "\n",
    "- **User B (entropy = 1.609)**: With equal spread across 5 genres, their behavior is maximally uncertain. This is actually the maximum possible entropy for 5 categories: `ln(5) ≈ 1.609`.\n",
    "\n",
    "**What the values mean practically:**\n",
    "\n",
    "- Entropy of 0 = watches only one genre (perfectly predictable)\n",
    "- Entropy of log(n) = watches n genres equally (maximum diversity)\n",
    "\n",
    "So User B's entropy being exactly log(5) tells us they have a perfectly uniform distribution across genres. User A's 0.500 is much closer to 0, reflecting their strong single-genre preference.\n",
    "\n",
    "This entropy value becomes our \"diversity propensity score\" that we'll use to personalize α in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Entropy to Personalized α"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have each user's entropy score. How do we turn it into a personalized `α`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure align = \"center\">\n",
    "    <img src=\"./paper.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\">\n",
    "    Figure: Snapshot from [Huawei's Personalized Re-ranking Paper](https://arxiv.org/pdf/2004.06390).\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Huawei paper](https://arxiv.org/pdf/2004.06390) uses a simple multiplicative formula:\n",
    "\n",
    "`α_u = f_u × α_0`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `α_0` is your baseline diversity weight (the system default)\n",
    "- `f_u` is a user-specific scaling factor between 0 and 1\n",
    "\n",
    "The scaling factor `f_u` is computed via min-max normalization of the user's entropy:\n",
    "\n",
    "`f_u = (H_u - H_min + l) / (H_max - H_min + l)`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `H_u` is the user's entropy\n",
    "- `H_min`, `H_max` are the min/max entropy across your user population\n",
    "- `l` is a smoothing parameter\n",
    "\n",
    "**The `l` parameter as a personalization dial:**\n",
    "\n",
    "- `l = 0`: Full personalization. Users at H_max get f_u = 1, users at H_min get f_u = 0\n",
    "- `l → ∞`: No personalization. Everyone gets f_u ≈ 0, so α_u ≈ 0 for all users\n",
    "- In practice, `l` in the range of `H_max - H_min` gives moderate personalization\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T22:37:16.550685Z",
     "start_time": "2026-01-01T22:37:16.547519Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_f_u(H_u, H_min, H_max, l=0):\n",
    "    return (H_u - H_min + l) / (H_max - H_min + l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T22:37:17.760535Z",
     "start_time": "2026-01-01T22:37:17.755926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of smoothing parameter l on f_u:\n",
      "--------------------------------------------------\n",
      "l          User A f_u      User B f_u      Difference\n",
      "--------------------------------------------------\n",
      "0          0.000           0.999           0.999     \n",
      "0.25       0.184           1.000           0.815     \n",
      "0.5        0.311           1.000           0.689     \n",
      "1.0        0.474           1.000           0.526     \n",
      "2.0        0.643           1.000           0.357     \n"
     ]
    }
   ],
   "source": [
    "# Show how l controls personalization strength\n",
    "print(\"Effect of smoothing parameter l on f_u:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'l':<10} {'User A f_u':<15} {'User B f_u':<15} {'Difference':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "H_a = compute_entropy(user_a_genres)  # ~0.5\n",
    "H_b = compute_entropy(user_b_genres)  # ~1.609\n",
    "\n",
    "# For this example, assume population-wide bounds\n",
    "H_min, H_max = 0.5, 1.61  # You'd compute these from your user base\n",
    "for l in [0, 0.25, 0.5, 1.0, 2.0]:\n",
    "    f_a = compute_f_u(H_a, H_min, H_max, l)\n",
    "    f_b = compute_f_u(H_b, H_min, H_max, l)\n",
    "    print(f\"{l:<10} {f_a:<15.3f} {f_b:<15.3f} {f_b - f_a:<10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `l` increases, the gap between User A and User B shrinks. At `l=0`, we get maximum personalization (f_u ranges from 0 to 1). At `l=2`, both users are clustered around 0.5, meaning they get nearly the same diversity treatment. This lets you tune how aggressively the system personalizes: start conservative with higher `l`, then decrease it as you gain confidence in the entropy signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in our DPP functions from [Part 2](https://aayushmnit.com/posts/2025-12-28-DiversityDPPPart2/DiversityDPPPart2.html#youtubes-kernel-parameterization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T23:02:35.415089Z",
     "start_time": "2026-01-01T23:02:35.408853Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def greedy_map_dpp(L, k):\n",
    "    N = L.shape[0]\n",
    "    selected = []\n",
    "    remaining = set(range(N))\n",
    "    \n",
    "    for _ in range(k):\n",
    "        best_item, best_det = None, -1\n",
    "        \n",
    "        for item in remaining:\n",
    "            candidate_set = selected + [item]\n",
    "            det_val = np.linalg.det(L[np.ix_(candidate_set, candidate_set)])\n",
    "            \n",
    "            if det_val > best_det:\n",
    "                \n",
    "                best_det = det_val\n",
    "                best_item = item\n",
    "        \n",
    "        selected.append(best_item)\n",
    "        remaining.remove(best_item)\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def youtube_dpp_kernel(relevance, embeddings, alpha=0.5, sigma=1.0):\n",
    "    N = len(relevance)\n",
    "    L = np.zeros((N, N))\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    # D_ij = ||embedding_i - embedding_j||²\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                # Diagonal: quality squared\n",
    "                \n",
    "                L[i, i] = relevance[i] ** 2\n",
    "            else:\n",
    "                # Off-diagonal: scaled similarity with RBF kernel\n",
    "                D_ij = np.sum((embeddings[i] - embeddings[j]) ** 2)\n",
    "                similarity = np.exp(-D_ij / (2 * sigma**2))\n",
    "                L[i, j] = alpha * relevance[i] * relevance[j] * similarity\n",
    "    \n",
    "    return L\n",
    "\n",
    "def youtube_dpp_ranking(relevance, embeddings, k, alpha=0.5, sigma=1.0):\n",
    "    W = list(range(len(relevance)))  # Remaining candidate indices\n",
    "    R = []  # Final ranked list\n",
    "    \n",
    "    while len(W) > 0:\n",
    "        # Build kernel for current candidate pool\n",
    "        rel_W = relevance[W]\n",
    "        emb_W = embeddings[W]\n",
    "        L = youtube_dpp_kernel(rel_W, emb_W, alpha=alpha, sigma=sigma)\n",
    "        \n",
    "        # Select up to k items from current pool\n",
    "        window_size = min(k, len(W))\n",
    "        M = greedy_map_dpp(L, k=window_size)\n",
    "        \n",
    "        # Map back to original indices\n",
    "        selected_items = [W[i] for i in M]\n",
    "        R.extend(selected_items)\n",
    "        \n",
    "        # Remove selected items from candidate pool\n",
    "        W = [w for w in W if w not in selected_items]\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the personalized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T22:37:29.692836Z",
     "start_time": "2026-01-01T22:37:29.684159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User A (narrow taste - prefers action) ===\n",
      "User alpha: 0.0004 (base alpha_0=1.0)\n",
      "Top 6: ['Action(0.9)', 'Action(0.85)', 'Action(0.8)', 'Comedy(0.7)', 'Documentary(0.6)', 'Mixed comedy/documentary(0.5)']\n",
      "=== User B (diverse taste) ===\n",
      "User alpha: 0.9995 (base alpha_0=1.0)\n",
      "Top 6: ['Action(0.9)', 'Comedy(0.7)', 'Documentary(0.6)', 'Mixed comedy/documentary(0.5)', 'Action(0.8)', 'Action(0.85)']\n"
     ]
    }
   ],
   "source": [
    "def compute_user_alpha(genre_history, H_min, H_max, alpha_0, l=0):\n",
    "    \"\"\"Compute personalized alpha for a user based on their genre history.\"\"\"\n",
    "    H_u = compute_entropy(genre_history)\n",
    "    f_u = compute_f_u(H_u, H_min, H_max, l)\n",
    "    return f_u * alpha_0\n",
    "\n",
    "def pdpp_ranking(relevance, embeddings, k, genre_history, H_min, H_max, alpha_0=1.0, l=0, sigma=1.0):\n",
    "    \"\"\"Personalized DPP ranking using user's genre history.\"\"\"\n",
    "    alpha_u = compute_user_alpha(genre_history, H_min, H_max, alpha_0, l)\n",
    "    print(f\"User alpha: {alpha_u:.4f} (base alpha_0={alpha_0})\")\n",
    "    return youtube_dpp_ranking(relevance, embeddings, k, alpha=alpha_u, sigma=sigma)\n",
    "\n",
    "# 5 videos with relevance scores and genre embeddings (2D for simplicity)\n",
    "relevance = np.array([0.9, 0.85, 0.8, 0.7, 0.6, 0.5])\n",
    "genre = ['Action', 'Action', 'Action', 'Comedy', 'Documentary', 'Mixed comedy/documentary']\n",
    "\n",
    "# Embeddings: action movies are close together, others spread out\n",
    "embeddings = np.array([\n",
    "    [1.0, 0.0, 0.0],   # video 0: pure action\n",
    "    [0.9, 0.1, 0.0],   # video 1: action (very similar to 0)\n",
    "    [0.8, 0.1, 0.1],   # video 2: action (also very similar to 0)\n",
    "    [0.0, 1.0, 0.0],   # video 3: comedy\n",
    "    [0.0, 0.0, 1.0],   # video 4: documentary\n",
    "    [0.1, 0.5, 0.5],   # video 5: mixed\n",
    "])\n",
    "\n",
    "# Global entropy bounds (you'd compute these from your user population)\n",
    "H_min, H_max = 0.5, 1.61\n",
    "\n",
    "# Compare rankings for User A (narrow taste) vs User B (diverse taste)\n",
    "print(\"=== User A (narrow taste - prefers action) ===\")\n",
    "ranking_a = pdpp_ranking(relevance, embeddings, k=6, genre_history=user_a_genres, H_min=H_min, H_max=H_max, l=0)\n",
    "print(f\"Top 6: {[genre[i]+'('+str(relevance[i])+')' for i in ranking_a[:6]]}\")\n",
    "\n",
    "print(\"=== User B (diverse taste) ===\")\n",
    "ranking_b = pdpp_ranking(relevance, embeddings, k=6, genre_history=user_b_genres, H_min=H_min, H_max=H_max, l=0)\n",
    "print(f\"Top 6: {[genre[i]+'('+str(relevance[i])+')' for i in ranking_b[:6]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show exactly what we'd expect:\n",
    "\n",
    "**User A (α ≈ 0.0)** gets the three action movies upfront, ranked purely by relevance. With α near zero, diversity has almost no influence. The algorithm respects their focused history: \"You've watched action 80% of the time, so here's the best action content first.\"\n",
    "\n",
    "**User B (α ≈ 1.0)** sees a completely different pattern. They get one action movie, then the algorithm immediately jumps to comedy, documentary, and mixed content. The other two action movies get pushed to positions 5 and 6. Their high entropy history translates to high α, which penalizes clustering similar items together.\n",
    "\n",
    "Same 6 videos. Same relevance scores. Completely different experiences.\n",
    "\n",
    "This is the power of pDPP: rather than treating diversity as a global system setting, it becomes a per-user preference learned directly from behavior. User A's narrow feed isn't a bug; it's what their history tells us they want. User B's varied feed isn't random; it matches their demonstrated exploration pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure align = \"center\">\n",
    "    <img src=\"./pDPP_example.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\">\n",
    "    Figure: pDPP example. Credit: [NotebookLM](https://notebooklm.google.com/)\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Production Variant: Bounded Additive Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplicative formula `α_u = f_u × α_0` has a problem: it can be too aggressive. Look at User A's result above: with f_u ≈ 0, they got α ≈ 0, meaning *zero* diversity consideration. In production, you probably don't want to completely eliminate diversity for anyone.\n",
    "\n",
    "The multiplicative approach also lacks intuitive bounds. If your team decides \"we never want `α` below 0.4 or above 0.8,\" the multiplicative formula doesn't give you that control directly.\n",
    "\n",
    "**The additive bounded formulation:**\n",
    "\n",
    "Instead of multiplying, we use an additive offset centered around 0.5:\n",
    "\n",
    "`α_u = α_0 + (f_u - 0.5) × α_range`\n",
    "\n",
    "Where:\n",
    "\n",
    "- `α_0` is your baseline (center point)\n",
    "- `α_range` controls how far users can deviate from baseline\n",
    "- `f_u - 0.5` shifts the range to [-0.5, +0.5], so the adjustment is symmetric\n",
    "\n",
    "This gives you explicit floor and ceiling:\n",
    "\n",
    "- Floor: `α_0 - 0.5 × α_range`\n",
    "- Ceiling: `α_0 + 0.5 × α_range`\n",
    "\n",
    "For example, with `α_0 = 0.6` and `α_range = 0.4`:\n",
    "\n",
    "- Minimum `α` = 0.6 - 0.2 = **0.4** (most focused users)\n",
    "- Maximum `α` = 0.6 + 0.2 = **0.8** (most diverse users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T23:05:40.196920Z",
     "start_time": "2026-01-01T23:05:40.188875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User A (narrow taste) - Bounded ===\n",
      "Top 6: ['Action(0.9)', 'Action(0.85)', 'Action(0.8)', 'Comedy(0.7)', 'Documentary(0.6)', 'Mixed comedy/documentary(0.5)']\n",
      "\n",
      "=== User B (diverse taste) - Bounded ===\n",
      "Top 6: ['Action(0.9)', 'Comedy(0.7)', 'Documentary(0.6)', 'Action(0.85)', 'Action(0.8)', 'Mixed comedy/documentary(0.5)']\n"
     ]
    }
   ],
   "source": [
    "def compute_user_alpha_bounded(genre_history, H_min, H_max, alpha_0=0.6, alpha_range=0.4, l=0):\n",
    "    \"\"\"Compute personalized alpha with explicit floor/ceiling bounds.\"\"\"\n",
    "    H_u = compute_entropy(genre_history)\n",
    "    f_u = compute_f_u(H_u, H_min, H_max, l)\n",
    "\n",
    "    # Additive formulation: center at alpha_0, vary by alpha_range\n",
    "    alpha_u = alpha_0 + (f_u - 0.5) * alpha_range\n",
    "    return alpha_u\n",
    "\n",
    "def pdpp_ranking_bounded(relevance, embeddings, k, genre_history, H_min, H_max, \n",
    "                         alpha_0=0.6, alpha_range=0.4, l=0, sigma=1.0):\n",
    "    \"\"\"Personalized DPP with bounded additive alpha.\"\"\"\n",
    "    alpha_u = compute_user_alpha_bounded(genre_history, H_min, H_max, alpha_0, alpha_range, l)\n",
    "    return youtube_dpp_ranking(relevance, embeddings, k, alpha=alpha_u, sigma=sigma)\n",
    "\n",
    "print(\"=== User A (narrow taste) - Bounded ===\")\n",
    "ranking_a_bounded = pdpp_ranking_bounded(\n",
    "    relevance, embeddings, k=6, genre_history=user_a_genres,\n",
    "    H_min=H_min, H_max=H_max, alpha_0=0.6, alpha_range=0.4\n",
    ")\n",
    "print(f\"Top 6: {[genre[i]+'('+str(relevance[i])+')' for i in ranking_a_bounded]}\\n\")\n",
    "\n",
    "print(\"=== User B (diverse taste) - Bounded ===\")\n",
    "ranking_b_bounded = pdpp_ranking_bounded(\n",
    "    relevance, embeddings, k=6, genre_history=user_b_genres,\n",
    "    H_min=H_min, H_max=H_max, alpha_0=0.6, alpha_range=0.4\n",
    ")\n",
    "print(f\"Top 6: {[genre[i]+'('+str(relevance[i])+')' for i in ranking_b_bounded]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both users stay within the guardrails we set (0.4 to 0.8):\n",
    "\n",
    "**User A (α = 0.4)** still gets all three action movies first, but now there's *some* diversity consideration baked in. They're at the floor, not at zero. If we had more videos in the candidate pool, this baseline diversity would prevent completely homogeneous results.\n",
    "\n",
    "**User B (α = 0.8)** gets a diversified feed, but not as extreme as before. Compare to the multiplicative version where they had α ≈ 1.0: now the second action movie appears at position 4 instead of position 5. The ceiling prevents over-diversification.\n",
    "\n",
    "**Why this matters in production:**\n",
    "\n",
    "- You can guarantee minimum diversity for all users (prevents filter bubbles)\n",
    "- You can cap maximum diversity (prevents feeds that feel random)\n",
    "- The parameters are intuitive to tune: \"baseline 0.6, users can vary ±0.2\"\n",
    "- Product and engineering can align on explicit bounds before launch\n",
    "\n",
    "The tradeoff is less dramatic personalization. The multiplicative approach gave a 10x difference between User A and B (0.0 vs 1.0). The bounded approach gives a 2x difference (0.4 vs 0.8). Which is right depends on your product goals and risk tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying pDPP requires a few decisions that don't show up in toy examples.\n",
    "\n",
    "**Computing H_min and H_max**\n",
    "\n",
    "You need population-wide entropy bounds. Two approaches:\n",
    "\n",
    "1. **Batch computation**: Run a daily/weekly job over all active users, compute their entropy, store the min/max. Simple, but lags behind distribution shifts.\n",
    "\n",
    "2. **Approximate with theory**: If you have `n` genres, the theoretical bounds are `H_min = 0` (user watches one genre) and `H_max = ln(n)` (uniform distribution). This avoids the batch job but may not reflect your actual user distribution.\n",
    "\n",
    "In practice, I'd recommend starting with theoretical bounds, then validating against your actual user distribution. If 99% of users fall between 0.3 and 1.2 but your theoretical max is 2.3, you're wasting personalization range.\n",
    "\n",
    "**Cold Start**\n",
    "\n",
    "New users have no history, so no entropy. Options:\n",
    "\n",
    "- Default to `α_0` (the baseline) until they have N interactions\n",
    "- Use cohort-level entropy (users who signed up from similar sources tend to have similar diversity preferences)\n",
    "- Start slightly above baseline to encourage exploration, then let it settle\n",
    "\n",
    "**How Often to Update f_u**\n",
    "\n",
    "Entropy is relatively stable for established users. Someone who's watched 500 videos won't see their entropy swing wildly from one more view. Options:\n",
    "\n",
    "- **Batch (daily/weekly)**: Simplest. Compute entropy offline, store in user profile.\n",
    "- **Nearline (per session)**: Recompute when user starts a session. More responsive but adds latency.\n",
    "- **Streaming**: Update incrementally with each interaction. Most responsive but adds complexity.\n",
    "\n",
    "For most systems, daily batch is sufficient. Entropy changes slowly.\n",
    "\n",
    "**What Counts as \"Genre\"?**\n",
    "\n",
    "The paper uses genre, but you can use any categorical attribute:\n",
    "\n",
    "- Content category (comedy, drama, news)\n",
    "- Creator/channel\n",
    "- Content length bucket (shorts vs. long-form)\n",
    "- Engagement type (videos you comment on vs. passive watches)\n",
    "\n",
    "You can even combine multiple: compute entropy over `(genre, creator_type)` tuples. The key is choosing attributes that meaningfully capture \"diversity\" for your product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Standard DPP's single α ignores that users have different diversity appetites.** Shannon entropy over interaction history gives us a principled way to measure each user's diversity propensity from their behavior.\n",
    "\n",
    "2. **pDPP personalizes the relevance-diversity tradeoff per user.** High-entropy users (explorers) get more diversity; low-entropy users (focused) get more relevance.\n",
    "\n",
    "3. **The bounded additive formulation is production-friendly.** It gives explicit floor/ceiling control and prevents edge cases where diversity is completely eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References & Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Personalized Re-ranking for Recommendation](https://arxiv.org/abs/2004.06390) (Pei et al., 2020) - The Huawei paper introducing pDPP. Covers the entropy-based personalization approach and the parameterized min-max normalization formula.\n",
    "- [Determinantal Point Processes for Machine Learning](https://arxiv.org/abs/1207.6083) (Kulesza & Taskar, 2012) - The definitive DPP tutorial. Essential background for understanding the kernel mechanics.\n",
    "- [Practical Diversified Recommendations on YouTube with Determinantal Point Processes](https://dl.acm.org/doi/10.1145/3269206.3272018) (Cheng et al., 2018) - YouTube's production DPP implementation that we extended in this post.\n",
    "\n",
    "**Previous posts in this series:**\n",
    "\n",
    "- [Part 1: Diversity in Recommendations using MMR](https://aayushmnit.com/posts/2025-12-25-DiversityMMRPart1/DiversityMMRPart1.html) - Maximal Marginal Relevance as a simpler diversity approach.\n",
    "- [Part 2: Diversity in Recommendations using DPP](https://aayushmnit.com/posts/2025-12-28-DiversityDPPPart2/DiversityDPPPart2.html) - DPP fundamentals and YouTube's kernel parameterization.\n",
    "\n",
    "I hope this series has been useful for thinking about diversity in your recommendation systems! If you're experimenting with pDPP or have questions, connect with me on [LinkedIn](https://www.linkedin.com/in/aayushmnit/) to share your experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
