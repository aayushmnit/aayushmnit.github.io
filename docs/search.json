[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aayush Agrawal",
    "section": "",
    "text": "I‚Äôm an experienced Machine learning engineer with specialized skills in machine learning-based solutions. I thrive on staying at the forefront of cutting-edge data technologies, including big data platforms, deep learning, optimization methods, and business analytics. Currently, I am part of the MLE team at Meta, where I focus on enhancing exploration strategies for the interest-oriented recommendation system in FB Reels. My previous work at Microsoft involved building data-driven products to enable smart recommendations for Microsoft Partners, M365 administrators, and end-users to optimize the usage of M365 services. I also have experience across various verticals, including agricultural technology, pharmaceuticals, retail, e-commerce, and the ride-sharing business model."
  },
  {
    "objectID": "about.html#technical",
    "href": "about.html#technical",
    "title": "Aayush Agrawal",
    "section": "Technical",
    "text": "Technical\n\n\nNatural Language Processing Specialization, Deeplearning.ai, Coursera\nCertificate\n\n\nOctober, 2022\n\n\n\n\nTriplebyte Engineering Certificate\nCertificate\n\n\nAugust, 2022\n\n\n\n\nParallel Programming with Dask in Python, DataCamp\nCertificate\n\n\nMarch, 2022\n\n\n\n\nMicrosoft Global Hackathon 2021 Award Winner, Microsoft\nCertificate\n\n\nDecember, 2021\n\n\n\n\nAI The LinkedIn Way, LinkedIn\nCertificate\n\n\nDecember, 2020\n\n\n\n\nLearning C#, LinkedIn\nCertificate\n\n\nJuly, 2019\n\n\n\n\nDEV262x: Logic and Computational Thinking, EDX\nCertificate\n\n\nMarch, 2019\n\n\n\n\nDevOps for Data Scientists, LinkedIn\nCertificate\n\n\nOctober, 2018\n\n\n\n\nGit Essential Training, LinkedIn\nCertificate\n\n\nOctober, 2018\n\n\n\n\nNatural Language Processing, HSE, Coursera\nCertificate\n\n\nJuly, 2018\n\n\n\n\nIntroduction to Deep Learning, HSE, Coursera\nCertificate\n\n\nMay, 2019\n\n\n\n\nDeveloping Data Products, John Hopkins University, Coursera\nCertificate\n\n\nNovember, 2014\n\n\n\n\nReproducible Research, John Hopkins University, Coursera\nCertificate\n\n\nNovember, 2014\n\n\n\n\nRegression Models, John Hopkins University, Coursera\nCertificate\n\n\nOctober, 2014\n\n\n\n\nMachine Learning, Stanford University, Coursera\nCertificate\n\n\nSeptember, 2014\n\n\n\n\nPractical Machine Learning, John Hopkins University, Coursera\nCertificate\n\n\nSeptember, 2014\n\n\n\n\nR Programming, John Hopkins University, Coursera\nCertificate\n\n\nSeptember, 2014"
  },
  {
    "objectID": "about.html#non---technical",
    "href": "about.html#non---technical",
    "title": "Aayush Agrawal",
    "section": "Non - Technical",
    "text": "Non - Technical\n\n\nCreativity at Work: A Short Course from Seth Godin, LinkedIn\nCertificate\n\n\nJuly, 2021\n\n\n\n\nLearning to Be Assertive, LinkedIn\nCertificate\n\n\nJune, 2021\n\n\n\n\nSucceeding in a New Role By Managing Up, LinkedIn\nCertificate\n\n\nMarch, 2021\n\n\n\n\nImproving Your Listening Skills, LinkedIn\nCertificate\n\n\nNovember, 2020\n\n\n\n\nGiving and Receiving Feedback, LinkedIn\nCertificate\n\n\nMay, 2020\n\n\n\n\nTime Management Fundamentals, LinkedIn\nCertificate\n\n\nApril, 2020\n\n\n\n\nCoaching Skills for Leaders and Managers, LinkedIn\nCertificate\n\n\nJune, 2019\n\n\n\n\nBecoming a Male Ally at Work, LinkedIn\nCertificate\n\n\nJune, 2019"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML Interview Prepration Guide (Draft)\n\n\n7 min\n\n\n\nML Interview Guide\n\n\n\n\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprivateGPT Walkthrough\n\n\n8 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\nLLMs\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython OOPs fundamentals\n\n\n9 min\n\n\n\nProgramming\n\n\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - DiffEdit paper implementation\n\n\n8 min\n\n\n\nStable Diffusion\n\n\nResearch\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion\n\n\n4 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Putting everything together\n\n\n3 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Looking under the hood\n\n\n9 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Introduction\n\n\n2 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel calibration for classification tasks using Python\n\n\n6 min\n\n\n\nModel Calibration\n\n\nMachine Learning\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixing art into the science of model explainability\n\n\n9 min\n\n\n\nExplainability\n\n\nMachine Learning\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal inference with Synthetic Control using Python and SparseSC\n\n\n7 min\n\n\n\nCausal Inference\n\n\nSynthetic Control\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding similar images using Deep learning and Locality Sensitive Hashing\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\nVideo\n\n\n\n\n\n\n\nFeb 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiLayer Perceptron using Fastai and Pytorch\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nJan 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Disease detection by Tranfer learning using FastAI V1 library\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nVision\n\n\n\n\n\n\n\nOct 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer perceptron using Tensorflow\n\n\n0 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Neural Network from scratch\n\n\n14 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nJun 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving business usecases by recommender system using lightFM\n\n\n30 min\n\n\n\nRecommender System\n\n\nMachine Learning\n\n\nBusiness\n\n\n\n\n\n\n\nApr 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite launch\n\n\n0 min\n\n\n\nlaunch\n\n\nannouncement\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to aayushmnit.com",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Medium\n  \n  \n    \n     Mastodon\n  \n  \n    \n     Twitter\n  \n  \n    \n     Kaggle\n  \n  \n    \n     Email\n  \n\n      \nI‚Äôm an experienced Data Scientist with specialized skills in machine learning-based solutions. I enjoy staying on top of cutting-edge data technologies, including big data platforms, deep learning, optimization methods, and business analytics. My current work involves building data-driven products to enable smarter recommendations for Microsoft Partners, M365 service administrators and end-users to ensure the best usage of M365 services. Before that, I have experience working in various verticals like agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing business model.\nRead more about me here.\nCheck out the latest draft of my book - ‚ÄúThe Pytorch Book‚Äù. If you enjoy the content, please consider giving it a star‚ú® on Github."
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome to aayushmnit.com",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n  \n\n\n\n\nML Interview Prepration Guide (Draft)\n\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\nAayush Agrawal\n\n\n\n\n\n\n  \n\n\n\n\nprivateGPT Walkthrough\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nAayush Agrawal\n\n\n\n\n\n\n  \n\n\n\n\nPython OOPs fundamentals\n\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nAayush Agrawal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "other/carlson.html",
    "href": "other/carlson.html",
    "title": "Carlson Students Q&A",
    "section": "",
    "text": "How I apply my learning from courses while my duration at the schoool?\n\n\n\n\n\nHere are a few options to apply your learning from the course -\nUniversity Curriculum\n\nLive project - MSBA program usually collaborates with companies around the Minneapolis area to run live projects every semester to allow students to work on real problems. Typically, these programs are part of some courses like Explorative Data analysis and Predictive modeling.\nExperiential learning, Carlson Analytics Lab - A six-credit course where students work in a group of four or five on a live project with a corporate partner to solve their real problem using real data. You can think of it as an internship.\n\nOther initiatives\n\nMeetup group - There are meetup groups like Social Data science,and Analyze This! etc., where you can actively participate and present.\nLocal Competitions - There are local analytics competitions organized by Minneanalytics, STATCOM, UMN StatClub, Social Data Science, etc. Keep a look out for those and participate.\nBlogging - Blogging is also an effective way to share your learning and establish your brand. Medium is a good place to start and publish in high-visibility publications like Towards Data Science, Analytics Vidhya, etc\n\n\n\n\n\n\n\n\n\n\nWhat kind of analytics/DS/ML roles are available in tech companies?\n\n\n\n\n\nThere are three typical DS/ML roles in tech companies -\n\nData scientist - Which is heavier on data analytics and experimentation.\nMachine Learning Scientist / Applied Scientist - This role is more focused on Machine learning model building, research, and software engineering.\nMachine Learning Engineers - This role is more focused on ML Ops, building data pipelines, and requires a lot more software engineering skills."
  },
  {
    "objectID": "other/carlson.html#admission",
    "href": "other/carlson.html#admission",
    "title": "Carlson Students Q&A",
    "section": "2.1 Admission",
    "text": "2.1 Admission\n\n\n\n\n\n\nI am applying to Carlson School for MSBA program. Could you help me out with interviews?\n\n\n\n\n\nVideo interview usually is a formality to ensure that you can communicate in English. Interviewer will ask you following questions like -\n\nTell me about yourself\nWhy MSBA program and Why Carlson?\nWhat are you hoping to achieve after the course?\nWalking through your work experience\n\nIt‚Äôs quite straightforward and usually just ensuring whatever you have written on your resume is actually you.\n\n\n\n\n\n\n\n\n\nI want to apply to Carlson School for MSBA program. What would be good GRE score?\n\n\n\n\n\nScore as high as you can, 320+ on GRE and 105 + on TOEFL would be good scores to aim for. You can find more about academic scores and prior background of current students on the university website."
  },
  {
    "objectID": "other/carlson.html#financial",
    "href": "other/carlson.html#financial",
    "title": "Carlson Students Q&A",
    "section": "2.2 Financial",
    "text": "2.2 Financial\n\n\n\n\n\n\nHow are the job prospects for a candidate after the course? Is the course worth it?\n\n\n\n\n\nJob placements are usually close to 100% so everybody gets a job. Read the placement reports on the website. I think the starting average salary is around $90k-$110k annually plus a 10% annual bonus, with that salary you should be able to recover your cost within 2-4 years. There are some companies that come on campus (Target, Cargill, Capital One, Ameriprise, etc.) but most of the students get jobs through their own search.\n\n\n\n\n\n\n\n\n\nIs TA/RA possible in this course? Do you think I should be approaching professors (aligned with my interests) beforehand?\n\n\n\n\n\nYes, but not in the summer semester. You can do TA/RA in Fall & Spring semesters, but I would not recommend that as the course is hectic and you should be focusing on creating a portfolio and learning. You can reach out to professors during the summer semester if you need to do TA/RA. Typically TA/RA jobs pay $18/hour, and students are allowed to do 10-20 hours per week.\n\n\n\n\n\n\n\n\n\nTuition for the course is high are there any scholarships for International students?\n\n\n\n\n\nCheck the university page for grants and scholarships. In my time there was a $10k scholarship given by the university to students. Students didn‚Äôt need to apply for it and were given automatically based on their prior background and GRE/TOEFL scores."
  },
  {
    "objectID": "other/carlson.html#accomodations",
    "href": "other/carlson.html#accomodations",
    "title": "Carlson Students Q&A",
    "section": "2.3 Accomodations",
    "text": "2.3 Accomodations\n\n\n\n\n\n\nIn which area or society(if you prefer one) should I search for accommodation? I am looking both in terms of proximity and rent?\n\n\n\n\n\nYou can check out Grand Marc seven corners that are right next to Carlson and quite affordable(<$500/month sharing, fully furnished). Many MSBA students including myself have lived there in the past. Otherwise, you can look in Minnehaha apartments or Stadium village apartments. Here is the FB page for Housing.\n\n\n\n\n\n\n\n\n\nHow much overall monthly expense do you estimate?\n\n\n\n\n\nDepends on your lifestyle ~$1000 including rent is good enough to survive.\n\n\n\n\n\n\n\n\n\nIs the weather very extreme and do you actually have to be indoors for a few months?\n\n\n\n\n\nWell someone in Minnesota will say ‚ÄúIts not cold, you just don‚Äôt have the right gear‚Äù. So it is cold in winters for atleast 4-5 months usually below -5 C. It doesn‚Äôt mean you can‚Äôt go out or life stops. People do business as usual in those conditions as well. In my time in Carlson I used to spend more than 12-14 hours in Carlson campus so none of this actually bothered me. But yeah it‚Äôs cold in winters so be prepared."
  },
  {
    "objectID": "other/carlson.html#course-and-skills-related",
    "href": "other/carlson.html#course-and-skills-related",
    "title": "Carlson Students Q&A",
    "section": "2.4 Course and Skills Related",
    "text": "2.4 Course and Skills Related\n\n\n\n\n\n\nHow did you like the course and the skills development at Carlson?\n\n\n\n\n\nI found the course useful in what I was trying to achieve. I was more looking for a business-oriented analytics/data science course and Carlson does a really good job in balancing these two aspects.\n\n\n\n\n\n\n\n\n\nWhat prerequisite would you recommend for better prospects?\n\n\n\n\n\nDepends on what you want to accomplish. If your aim is to be a data scientist then you need to be technically strong, if your aim is to become a consultant then you need to work more on soft skills. The ability to communicate with a business audience is a must and proficiency in python will give you an edge over others.\n\n\n\n\n\n\n\n\n\nIs the curriculum good for someone who is more interested in a technical ds role or more of a consulting oriented role in the field of analytics?\n\n\n\n\n\nThe course does touch upon many DS technicalities, but if you don‚Äôt have any prior experience, you need to put extra effort into honing skills in those areas. I would say the course does a much better job to prepare students for consulting oriented roles in the field of analytics. It also depends on individual aspirations and current skill level.\n\n\n\n\n\n\n\n\n\nOne of my great concern is the location. Minneapolis is not the big city and there might be less opportunity, especially for the international students. What do you think?\n\n\n\n\n\nMinneapolis is not a small city and is home to 17 Fortune 500 companies. Also, university have active alumni in many of these companies which hire almost every year from the MSBA program. Some of the companies which hire regularly are Target, Capital One, Amazon, Ameriprise, Best Buy, Slalom, etc. Also, the MSBA program have nearly 100% placement stats. Not sure if other universities or programs have such stellar stats but finding employment should not be a problem if you happen to be in the top 50% of the class."
  },
  {
    "objectID": "posts/2018-02-19-launch/index.html",
    "href": "posts/2018-02-19-launch/index.html",
    "title": "Website launch",
    "section": "",
    "text": "Finally got time to build my own website on Github. This is my first post and I would like to thank Academic Pages which provided with a wonderful repository to help me get started with building this website.\nI am intending to use this website to publish blogs, workshop and any material which would be releveant in Data science field.\nWill see you guys in next post.\n-Aayush"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "",
    "text": "In this post, I am going to write about Recommender systems, how they are used in many e-commerce websites. The post will also cover about building simple recommender system models using Matrix Factorization algorithm using lightFM package and my recommender system cookbook. The post will focus on business use cases and simple implementations. The post only cover basic intuition around algorithms and will provide links to resources if you want to understand the math behind the algorithm."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Motivation",
    "text": "Motivation\nI am an avid reader and a believer in open source education and continuously expand my knowledge around data science & computer science using online courses, blogs, Github repositories and participating in data science competitions. While searching for quality content on the internet, I have come across various learning links which either focus on the implementation of the algorithm using specific data/modeling technique in ABC language or focus on business impact/results using the broad concept of a family of algorithms(like classification, forecasting, recommender systems etc.) but don‚Äôt go into details of how to do it. So the idea is to write some blogs which can combine both business use cases with codes & algorithmic intuition to provide a holistic view of how data science is used in business scenarios. \nAs the world is becoming more digital, we are already getting used to a lot of personalized experience and the algorithm which help us achieve this falls in the family of recommender systems. Almost every web-based platform is using some recommender system to provide customized content. Following are the companies I admire the most."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "What is personalization?",
    "text": "What is personalization?\nPersonalization is a technique of dynamically tailoring your content based on needs of each user. Simple examples of personalization could be movie recommendation on Netflix, personalized email targeting/re-targeting by e-commerce platforms, item recommendation on Amazon, etc. Personalization helps us achieve these four Rs - - Recognize: Know customer‚Äôs and prospects‚Äô profiles, including demographics, geography, and expressed and shared interests. - Remember: Recall customers‚Äô history, primarily how they act as expressed by what they browse and buy - Reach: Deliver the right promotion, content, recommendation for a customer based on actions, preferences, and interests - Relevance: Deliver personalization within the context of the digital experience based on who customers are, where they are located and what time of year it is"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Why personalization?",
    "text": "Why personalization?\nPersonalization has a lot of benefits for both users and companies. For users, it makes their life easy as they only get to see more relevant stuff to them (unless it‚Äôs an advertisement, even they are personalized). For business benefits are countless but here are few which I would like to mention - - Enhance customer experience: Personalization reduces the clutter and enhances the customer experience by showing relevant content - Cross-sell/ Up-sell opportunities: Relevant product offerings based on customer preferences can lead to increasing products visibility and eventually selling more products - Increased basket size: Personalized experience and targeting ultimately leads to increased basket size and frequent purchases - Increased customer loyalty: In the digital world, customer retention/loyalty is the most prominent problem faced by many companies as finding a replacement for a particular service is quite easy. According to a Forbes article, Forty-four percent of consumers say they will likely repeat after a personalized experience"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Introduction to Matrix factorization",
    "text": "Introduction to Matrix factorization\nMatrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc.  \n Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it‚Äôs more female-driven the value is typically low.  \nFor more information on matrix factorization and factorization machines you can read these articles -  Matrix Factorization: A Simple Tutorial and Implementation in Python  Introductory Guide ‚Äì Factorization Machines & their application on huge datasets (with codes in Python)"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "HandOn: Building recommender system using LightFM package in Python",
    "text": "HandOn: Building recommender system using LightFM package in Python\nIn the hands-on section, we will be building recommender system for different scenarios which we typically see in many companies using LightFM package and MovieLens data. We are using small size data which contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users\n\nData\nLet‚Äôs start by importing data, recommender system cookbook and preprocessing cookbook files for this hands-on section. I have written these reusable generic cookbook codes to increase productivity and write clean/modular codes; you will see we can build a recommender system using 10-15 lines of code by using these cookbooks(do more with less!).\n# Importing Libraries and cookbooks\nfrom recsys import * ## recommender system cookbook\nfrom generic_preprocessing import * ## pre-processing code\nfrom IPython.display import HTML ## Setting display options for Ipython Notebook\n# Importing rating data and having a look\nratings = pd.read_csv('./ml-latest-small/ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\n\n\nmovieId\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n0\n\n\n1\n\n\n31\n\n\n2.5\n\n\n1260759144\n\n\n\n\n1\n\n\n1\n\n\n1029\n\n\n3.0\n\n\n1260759179\n\n\n\n\n2\n\n\n1\n\n\n1061\n\n\n3.0\n\n\n1260759182\n\n\n\n\n3\n\n\n1\n\n\n1129\n\n\n2.0\n\n\n1260759185\n\n\n\n\n4\n\n\n1\n\n\n1172\n\n\n4.0\n\n\n1260759205\n\n\n\n\n\n\nAs we can see rating data contain user id, movie id and a rating between 0.5 to 5 with a timestamp representing when the rating was given.\n# Importing movie data and having a look at first five columns\nmovies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\n\n\ntitle\n\n\ngenres\n\n\n\n\n\n\n0\n\n\n1\n\n\nToy Story (1995)\n\n\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n1\n\n\n2\n\n\nJumanji (1995)\n\n\nAdventure|Children|Fantasy\n\n\n\n\n2\n\n\n3\n\n\nGrumpier Old Men (1995)\n\n\nComedy|Romance\n\n\n\n\n3\n\n\n4\n\n\nWaiting to Exhale (1995)\n\n\nComedy|Drama|Romance\n\n\n\n\n4\n\n\n5\n\n\nFather of the Bride Part II (1995)\n\n\nComedy\n\n\n\n\n\n\nMovie data consist of movie id, their title, and genre they belong.\n\n\nPreprocessing\nAs I mentioned before, to create a recommender system we need to start by creating an interaction matrix. For this task, we will use the create_interaction_matrix function from the recsys cookbook. This function requires you to input a pandas dataframe and necessary information like column name for user id, item id, and rating. It also takes an additional parameter threshold if norm=True which means any rating above the mentioned threshold is considered a positive rating. In our case, we don‚Äôt have to normalize our data, but in cases of retail data any purchase of a particular type of item can be considered a positive rating, quantity doesn‚Äôt matter.\n# Creating interaction matrix using rating data\ninteractions = create_interaction_matrix(df = ratings,\n                                         user_col = 'userId',\n                                         item_col = 'movieId',\n                                         rating_col = 'rating')\ninteractions.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n‚Ä¶\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n4\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n5\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n5 rows √ó 9066 columns\n\n\nAs we can see the data is created in an interaction format where rows represent each user and columns represent each movie id with ratings as values.  We will also create user and item dictionaries to later convert user_id to user_name or movie_id to movie_name by using create_user_dict and create_item dict function.\n# Create User Dict\nuser_dict = create_user_dict(interactions=interactions)\n# Create Item dict\nmovies_dict = create_item_dict(df = movies,\n                               id_col = 'movieId',\n                               name_col = 'title')\n\n\nBuilding Matrix Factorization model\nTo build a matrix factorization model, we will use the runMF function which will take following input -\n- interaction matrix: Interaction matrix created in the previous section - n_components: Number of embedding generated for each user and item - loss: We need to define a loss function, in this case, we are using warp loss because we mostly care about the ranking of data, i.e, which items should we show first - epoch: Number of times to run - n_jobs: Number of cores to use in parallel processing\nmf_model = runMF(interactions = interactions,\n                 n_components = 30,\n                 loss = 'warp',\n                 epoch = 30,\n                 n_jobs = 4)\nNow we have built our matrix factorization model we can now do some interesting things. There are various use cases which can be solved by using this model for a web platform let‚Äôs look into them.\n\n\nUsecase 1: Item recommendation to a user\nIn this use case, we want to show a user, items he might be interested in buying/viewing based on his/her interactions done in the past. Typical industry examples for this are like ‚ÄúDeals recommended for you‚Äù on Amazon or ‚ÄúTop pics for a user‚Äù on Netflix or personalized email campaigns. \nWe can use the sample_recommendation_user function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, user_id and the number of items as input and return the list of item id‚Äôs a user may be interested in interacting.\n## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = mf_model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = movies_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)\nKnown Likes:\n1- The Hunger Games: Catching Fire (2013)\n2- Gravity (2013)\n3- Dark Knight Rises, The (2012)\n4- The Hunger Games (2012)\n5- Town, The (2010)\n6- Exit Through the Gift Shop (2010)\n7- Bank Job, The (2008)\n8- Departed, The (2006)\n9- Bourne Identity, The (1988)\n10- Step Into Liquid (2002)\n11- SLC Punk! (1998)\n12- Last of the Mohicans, The (1992)\n13- Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)\n14- Robin Hood: Prince of Thieves (1991)\n15- Citizen Kane (1941)\n16- Trainspotting (1996)\n17- Pulp Fiction (1994)\n18- Usual Suspects, The (1995)\n\n Recommended Items:\n1- Dark Knight, The (2008)\n2- Inception (2010)\n3- Iron Man (2008)\n4- Shutter Island (2010)\n5- Fight Club (1999)\n6- Avatar (2009)\n7- Forrest Gump (1994)\n8- District 9 (2009)\n9- WALL¬∑E (2008)\n10- Matrix, The (1999)\nprint(rec_list)\n[593L, 260L, 110L, 480L, 47L, 527L, 344L, 858L, 231L, 780L]\nAs we can see in this case user is interested in ‚ÄúDark Knight Rises(2012)‚Äù so the first recommendation is ‚ÄúThe Dark Knight(2008)‚Äù. This user also seems to have a strong liking towards movies in drama, sci-fi and thriller genre and there are many movies recommended in the same genre like Dark Knight(Drama/Crime), Inception(Sci-Fi, Thriller), Iron Man(Sci-FI thriller), Shutter Island(Drame/Thriller), Fight club(drama), Avatar(Sci-fi), Forrest Gump(Drama), District 9(Thriller), Wall-E(Sci-fi), The Matrix(Sci-Fi) \nSimilar models can also be used for building sections like ‚ÄúBased on your recent browsing history‚Äù recommendations by just changing the rating matrix only to contain interaction which is recent and based on browsing history visits on specific items.\n\n\nUsecase 2: User recommendation to a item\nIn this use case, we will discuss how we can recommend a list of users specific to a particular item. Example of such cases is when you are running a promotion on an item and want to run an e-mail campaign around this promotional item to only 10,000 users who might be interested in this item.\n\nWe can use the sample_recommendation_item function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, item_id and the number of users as input and return the list of user id‚Äôs who are more likely be interested in the item.\n## Calling 15 user recommendation for item id 1\nsample_recommendation_item(model = mf_model,\n                           interactions = interactions,\n                           item_id = 1,\n                           user_dict = user_dict,\n                           item_dict = movies_dict,\n                           number_of_user = 15)\n[116, 410, 449, 657, 448, 633, 172, 109, 513, 44, 498, 459, 317, 415, 495]\nAs you can see function return a list of userID who might be interested in item id 1. Another example why you might need such model is when there is an old inventory sitting in your warehouse which needs to clear up otherwise you might have to write it off, and you want to clear it by giving some discount to users who might be interested in buying.\n\n\nUsecase 3: Item recommendation to items\nIn this use case, we will discuss how we can recommend a list of items specific to a particular item. This kind of models will help you to find similar/related items or items which can be bundled together. Typical industry use case for such models are in cross-selling and up-selling opportunities on product page like ‚ÄúProducts related to this item‚Äù, ‚ÄúFrequently bought together‚Äù, ‚ÄúCustomers who bought this also bought this‚Äù and ‚ÄúCustomers who viewed this item also viewed‚Äù.  ‚ÄúCustomers who bought this also bought this‚Äù and ‚ÄúCustomers who viewed this item also viewed‚Äù can also be solved through market basket analysis.\n\nTo achieve this use case, we will create a cosine distance matrix using item embeddings generated by matrix factorization model. This will help us calculate similarity b/w items, and then we can recommend top N similar item to an item of interest. First step is to create a item-item distance matrix using the create_item_emdedding_distance_matrix function. This function takes matrix factorization models and interaction matrix as input and returns an item_embedding_distance_matrix.\n## Creating item-item distance matrix\nitem_item_dist = create_item_emdedding_distance_matrix(model = mf_model,\n                                                       interactions = interactions)\n## Checking item embedding distance matrix\nitem_item_dist.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n‚Ä¶\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nmovieId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1.000000\n\n\n0.760719\n\n\n0.491280\n\n\n0.427250\n\n\n0.484597\n\n\n0.740024\n\n\n0.486644\n\n\n0.094009\n\n\n-0.083986\n\n\n0.567389\n\n\n‚Ä¶\n\n\n-0.732112\n\n\n-0.297997\n\n\n-0.451733\n\n\n-0.767141\n\n\n-0.501647\n\n\n-0.270280\n\n\n-0.455277\n\n\n-0.292823\n\n\n-0.337935\n\n\n-0.636147\n\n\n\n\n2\n\n\n0.760719\n\n\n1.000000\n\n\n0.446414\n\n\n0.504502\n\n\n0.525171\n\n\n0.572113\n\n\n0.364393\n\n\n0.290633\n\n\n0.231926\n\n\n0.653033\n\n\n‚Ä¶\n\n\n-0.748452\n\n\n-0.307634\n\n\n-0.165400\n\n\n-0.526614\n\n\n-0.146751\n\n\n-0.156305\n\n\n-0.223818\n\n\n-0.138412\n\n\n-0.209538\n\n\n-0.733489\n\n\n\n\n3\n\n\n0.491280\n\n\n0.446414\n\n\n1.000000\n\n\n0.627473\n\n\n0.769991\n\n\n0.544175\n\n\n0.632008\n\n\n0.336824\n\n\n0.392284\n\n\n0.510592\n\n\n‚Ä¶\n\n\n-0.331028\n\n\n-0.264556\n\n\n-0.308592\n\n\n-0.285085\n\n\n-0.046424\n\n\n-0.165821\n\n\n-0.183842\n\n\n-0.143613\n\n\n-0.156418\n\n\n-0.378811\n\n\n\n\n4\n\n\n0.427250\n\n\n0.504502\n\n\n0.627473\n\n\n1.000000\n\n\n0.582582\n\n\n0.543208\n\n\n0.602390\n\n\n0.655708\n\n\n0.527346\n\n\n0.471166\n\n\n‚Ä¶\n\n\n-0.380431\n\n\n-0.163091\n\n\n-0.232833\n\n\n-0.334746\n\n\n-0.052832\n\n\n-0.266185\n\n\n-0.158415\n\n\n-0.211618\n\n\n-0.232351\n\n\n-0.469629\n\n\n\n\n5\n\n\n0.484597\n\n\n0.525171\n\n\n0.769991\n\n\n0.582582\n\n\n1.000000\n\n\n0.354141\n\n\n0.639958\n\n\n0.396447\n\n\n0.432026\n\n\n0.385051\n\n\n‚Ä¶\n\n\n-0.273074\n\n\n-0.280585\n\n\n-0.306195\n\n\n-0.265243\n\n\n0.012961\n\n\n-0.225142\n\n\n-0.317043\n\n\n-0.136875\n\n\n-0.122382\n\n\n-0.312858\n\n\n\n\n\n\n5 rows √ó 9066 columns\n\n\nAs we can see the matrix have movies as both row and columns and the value represents the cosine distance between them. Next step is to use item_item_recommendation function to get top N items with respect to an item_id. This function takes item embedding distance matrix, item_id, item_dictionary and number of items to be recommended as input and return similar item list as output.\n## Calling 10 recommended items for item id \nrec_list = item_item_recommendation(item_emdedding_distance_matrix = item_item_dist,\n                                    item_id = 5378,\n                                    item_dict = movies_dict,\n                                    n_items = 10)\nItem of interest :Star Wars: Episode II - Attack of the Clones (2002)\nItem similar to the above item:\n1- Star Wars: Episode III - Revenge of the Sith (2005)\n2- Lord of the Rings: The Two Towers, The (2002)\n3- Lord of the Rings: The Fellowship of the Ring, The (2001)\n4- Lord of the Rings: The Return of the King, The (2003)\n5- Matrix Reloaded, The (2003)\n6- Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n7- Gladiator (2000)\n8- Spider-Man (2002)\n9- Minority Report (2002)\n10- Mission: Impossible II (2000)\nAs we can see for ‚ÄúStar Wars: Episode II - Attack of the Clones (2002)‚Äù movie we are getting it‚Äôs next released movies which is ‚ÄúStar Wars: Episode III - Revenge of the Sith (2005)‚Äù as the first recommendation."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Summary",
    "text": "Summary\nLike any other blog, this method isn‚Äôt perfect for every application, but the same ideas can work if we use it effectively. There is a lot of advancements in recommender systems with the advent of Deep learning. While there is room for improvement, I am pleased with how it has been working for me so far. I might write about deep learning based recommender systems later sometime.\nIn the meantime, I hope you enjoyed reading, and feel free to use my code to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "title": "Building Neural Network from scratch",
    "section": "",
    "text": "In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let‚Äôs start with the very basics of a neural network which is Multilayer Perceptron. You can find the same blog in notebook version here."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "title": "Building Neural Network from scratch",
    "section": "What is a neural network?",
    "text": "What is a neural network?\nA neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it‚Äôs the same with a neural network."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "title": "Building Neural Network from scratch",
    "section": "What is a Multi layer perceptron?",
    "text": "What is a Multi layer perceptron?\nMulti-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let‚Äôs start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.   \n\nFig 1: Perceptron image\n\n\n\nImage credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/\n\nNow, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology.  Dense layer,   Note that bias term is now a vector and W is a weight matrix  \n\nFig: Single dense layer perceptron network\n\n\n\nImage credit=http://www.texample.net/tikz/examples/neural-network/\n\nNow we understand dense layer let‚Äôs add a bunch of them, and that network becomes a multi-layer perceptron network.\n\n\nFig: Multi layer perceptron network\n\n\n\nImage credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2s\n\nIf you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero.  ReLU function   Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a feed-forward loop and backpropagation loop. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don‚Äôt understand the concept of gradient weight updates and SGD, I recommend you to watch week 1 of Machine learning by Andrew NG lectures.\nSo, to summarize a neural network needs few building blocks\n\nDense layer - a fully-connected layer, \nReLU layer (or any other activation function to introduce non-linearity)\nLoss function - (crossentropy in case of multi-class classification problem)\nBackprop algorithm - a stochastic gradient descent with backpropageted gradients\n\nLet‚Äôs approach them one at a time."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "title": "Building Neural Network from scratch",
    "section": "Coding Starts here:",
    "text": "Coding Starts here:\nLet‚Äôs start by importing some libraires required for creating our neural network.\nfrom __future__ import print_function\nimport numpy as np ## For numerical python\nnp.random.seed(42)\nEvery layer will have a forward pass and backpass implementation. Let‚Äôs create a main class layer which can do a forward pass .forward() and Backward pass .backward().\nclass Layer:\n    \n    #A building block. Each layer is capable of performing two things:\n\n    #- Process input to get output:           output = layer.forward(input)\n    \n    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n    \n    #Some layers also have learnable parameters which they update during layer.backward.\n    \n    def __init__(self):\n        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n        # A dummy layer does nothing\n        pass\n    \n    def forward(self, input):\n        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        \n        # A dummy layer just returns whatever it gets as input.\n        return input\n\n    def backward(self, input, grad_output):\n        # Performs a backpropagation step through the layer, with respect to the given input.\n        \n        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n        \n        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n        \n        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n        \n        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n        \n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n        num_units = input.shape[1]\n        \n        d_layer_d_input = np.eye(num_units)\n        \n        return np.dot(grad_output, d_layer_d_input) # chain rule\n\nNonlinearity ReLU layer\nThis is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\nclass ReLU(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        relu_grad = input > 0\n        return grad_output*relu_grad \n\n\nDense layer\nNow let‚Äôs build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\nA dense layer applies affine transformation. In a vectorized form, it can be described as: \nWhere * X is an object-feature matrix of shape [batch_size, num_features], * W is a weight matrix [num_features, num_outputs] * and b is a vector of num_outputs biases.\nBoth W and b are initialized during layer creation and updated each time backward is called. Note that we are using Xavier initialization which is a trick to train our model to converge faster read more. Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        # A dense layer is a layer which performs a learned affine transformation:\n        # f(x) = <W*x> + b\n        \n        self.learning_rate = learning_rate\n        self.weights = np.random.normal(loc=0.0, \n                                        scale = np.sqrt(2/(input_units+output_units)), \n                                        size = (input_units,output_units))\n        self.biases = np.zeros(output_units)\n        \n    def forward(self,input):\n        # Perform an affine transformation:\n        # f(x) = <W*x> + b\n        \n        # input shape: [batch, input_units]\n        # output shape: [batch, output units]\n        \n        return np.dot(input,self.weights) + self.biases\n    \n    def backward(self,input,grad_output):\n        # compute d f / d x = d f / d dense * d dense / d x\n        # where d dense/ d x = weights transposed\n        grad_input = np.dot(grad_output, self.weights.T)\n        \n        # compute gradient w.r.t. weights and biases\n        grad_weights = np.dot(input.T, grad_output)\n        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n        \n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        \n        # Here we perform a stochastic gradient descent step. \n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n        \n        return grad_input\n\n\nThe loss function\nSince we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\nIf we write down the expression for crossentropy as a function of softmax logits (a), you‚Äôll see: \n  If we take a closer look, we‚Äôll see that it can be rewritten as: \n  It‚Äôs called Log-softmax and it‚Äôs better than naive log(softmax(a)) in all aspects: * Better numerical stability * Easier to get derivative right * Marginally faster to compute\nSo why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\ndef softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n    \n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n    \n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n    \n    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- ones_for_answers + softmax) / logits.shape[0]\n\n\nFull network\nNow let‚Äôs combine what we‚Äôve just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let‚Äôs import it!.\nimport keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) / 255.\n    X_test = X_test.astype(float) / 255.\n\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n\n## Let's look at some example\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n\nWe‚Äôll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\nnetwork = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\ndef forward(network, X):\n    # Compute activations of all network layers by applying them sequentially.\n    # Return a list of activations for each layer. \n    \n    activations = []\n    input = X\n\n    # Looping through each layer\n    for l in network:\n        activations.append(l.forward(input))\n        # Updating input to last layer output\n        input = activations[-1]\n    \n    assert len(activations) == len(network)\n    return activations\n\ndef predict(network,X):\n    # Compute network predictions. Returning indices of largest Logit probability\n\n    logits = forward(network,X)[-1]\n    return logits.argmax(axis=-1)\n\ndef train(network,X,y):\n    # Train our network on a given batch of X and y.\n    # We first need to run forward to get all layer activations.\n    # Then we can run layer.backward going from last to first layer.\n    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n    \n    \n    # Get the layer activations\n    layer_activations = forward(network,X)\n    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n    logits = layer_activations[-1]\n    \n    # Compute the loss and the initial gradient\n    loss = softmax_crossentropy_with_logits(logits,y)\n    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n    \n    # Propagate gradients through the network\n    # Reverse propogation as this is backprop\n    for layer_index in range(len(network))[::-1]:\n        layer = network[layer_index]\n        \n        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n        \n    return np.mean(loss)\n\n\nTraining loop\nWe split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent.\nfrom tqdm import trange\ndef iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.random.permutation(len(inputs))\n    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\nfrom IPython.display import clear_output\ntrain_log = []\nval_log = []\nfor epoch in range(25):\n\n    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n        train(network,x_batch,y_batch)\n    \n    train_log.append(np.mean(predict(network,X_train)==y_train))\n    val_log.append(np.mean(predict(network,X_val)==y_val))\n    \n    clear_output()\n    print(\"Epoch\",epoch)\n    print(\"Train accuracy:\",train_log[-1])\n    print(\"Val accuracy:\",val_log[-1])\n    plt.plot(train_log,label='train accuracy')\n    plt.plot(val_log,label='val accuracy')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n    \nEpoch 24\nTrain accuracy: 1.0\nVal accuracy: 0.9809\n\nAs we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
  },
  {
    "objectID": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "href": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "title": "Multi-Layer perceptron using Tensorflow",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "href": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "title": "Leaf Disease detection by Tranfer learning using FastAI V1 library",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "href": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "title": "MultiLayer Perceptron using Fastai and Pytorch",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "href": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "title": "Real-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "href": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "title": "Finding similar images using Deep learning and Locality Sensitive Hashing",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "",
    "text": "Understanding Synthetic Control and using Microsoft‚Äôs SparceSC package to run synthetic control on larger datasets."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "What is Synthetic Control Method?",
    "text": "What is Synthetic Control Method?\nI will try to keep this part short and focus more on why Data scientists should care about such methods and how to use them on larger datasets based on practical experience using SparseSC package.\nThe Synthetic Control (SC) method is a statistical method used to estimate causal effects from binary treatments on observational panel (longitudinal) data. The method got quite a coverage by being described as ‚Äúthe most important innovation in the policy evaluation literature in the last few years‚Äù and got an article published in Washington Post - Seriously, here‚Äôs one amazing math trick to learn what can‚Äôt be known. ‚ÄúSC is a technique to create an artificial control group by taking a weighted average of untreated units in such a way that it reproduces the characteristics of the treated units before the intervention(treatment). The SC acts as the counterfactual for a treatment unit, and the estimate of a treatment effect is the difference between the observed outcome in the post-treatment period and the SC‚Äôs outcome.‚Äù1\n‚ÄúOne way to think of SC is as an improvement upon difference-in-difference (DiD) estimation. Typical DiD will compare a treated unit to the average of the control units. But often the treated unit does not look like a typical control (e.g., it might have a different growth rate), in which case the ‚Äòparallel trend‚Äô assumption of DiD is not valid. SC remedies this by choosing a smarter linear combination, rather than the simple average, to weigh more heavily the more similar units. SC‚Äôs assumption is if there are endogenous factors that affect treatment and future outcomes then you should be able to control them by matching past outcomes. The matching that SC provides can therefore deal with some problems in estimation that DiD cannot handle.‚Äù2\nHere is the link to the Causal inference book which I found most useful to understand the math behind SC- Causal Inference for The Brave and True by Matheus Facure - Chapter 15."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Why should any Data scientist care about this method?",
    "text": "Why should any Data scientist care about this method?\nOften as a Data Scientist, you will encounter situations as follows where running A/B testing is not feasible because of -\n\nLack of infrastructure\nLack of similar groups for running A/B testing (in case of evaluation of state policies, as there is no state equivalent of other)\nProviding unwanted advantage to one group over others. Sometimes running an A/B test can give an unfair advantage and lead you into anti-trust territory. For example, what if Amazon tries to charge differential pricing for different customers or apply different margins for their sellers for the same product?\n\nAs a data scientist, stakeholders may still ask you to estimate the impact of certain changes/treatments, and Synthetic controls can come to the rescue in this situation. For this reason, it is a valuable tool to keep in your algorithmic toolkit."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Problem Overview",
    "text": "Problem Overview\nWe will use the Proposition 99 data to explain the use case for this approach and also how to use the SparceSC library and its key features. ‚ÄúIn 1988, California passed a famous Tobacco Tax and Health Protection Act, which became known as Proposition 99. Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements. To evaluate its effect, we can gather data on cigarette sales from multiple states and across a number of years. In our case, we got data from the year 1970 to 2000 from 39 states.‚Äù3\n\n\nCode\n# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport SparseSC\nfrom datetime import datetime\nimport warnings\nimport plotly.express as px\nimport plotly.graph_objects as pgo\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings('ignore')\n\n\nLet‚Äôs look at the data\n\n#Import data\ndata_dir = \"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/\"\ndf = pd.read_csv(data_dir + \"smoking_data\" + \".csv\").drop(columns=[\"lnincome\",\"beer\", \"age15to24\"])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      cigsale\n      retprice\n    \n  \n  \n    \n      0\n      Alabama\n      1970.0\n      89.8\n      39.6\n    \n    \n      1\n      Alabama\n      1971.0\n      95.4\n      42.7\n    \n    \n      2\n      Alabama\n      1972.0\n      101.1\n      42.3\n    \n    \n      3\n      Alabama\n      1973.0\n      102.9\n      42.1\n    \n    \n      4\n      Alabama\n      1974.0\n      108.2\n      43.1\n    \n  \n\n\n\n\nWe have data per state as treatment unit and yearly (year column) per-capita sales of cigarettes in packs (cigsale column) and the cigarette retail price (retprice column). We are going to pivot this data so that each row is one treatment unit(state), and columns represent the yearly cigsale value.\n\ndf = df.pivot(index= 'state', columns = 'year', values = \"cigsale\")\ndf.head()\n\n\n\n\n\n  \n    \n      year\n      1970.0\n      1971.0\n      1972.0\n      1973.0\n      1974.0\n      1975.0\n      1976.0\n      1977.0\n      1978.0\n      1979.0\n      1980.0\n      1981.0\n      1982.0\n      1983.0\n      1984.0\n      1985.0\n      1986.0\n      1987.0\n      1988.0\n      1989.0\n      1990.0\n      1991.0\n      1992.0\n      1993.0\n      1994.0\n      1995.0\n      1996.0\n      1997.0\n      1998.0\n      1999.0\n      2000.0\n    \n    \n      state\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      89.8\n      95.4\n      101.1\n      102.9\n      108.2\n      111.7\n      116.2\n      117.1\n      123.0\n      121.4\n      123.2\n      119.6\n      119.1\n      116.3\n      113.0\n      114.5\n      116.3\n      114.0\n      112.1\n      105.6\n      108.6\n      107.9\n      109.1\n      108.5\n      107.1\n      102.6\n      101.4\n      104.9\n      106.2\n      100.7\n      96.2\n    \n    \n      Arkansas\n      100.3\n      104.1\n      103.9\n      108.0\n      109.7\n      114.8\n      119.1\n      122.6\n      127.3\n      126.5\n      131.8\n      128.7\n      127.4\n      128.0\n      123.1\n      125.8\n      126.0\n      122.3\n      121.5\n      118.3\n      113.1\n      116.8\n      126.0\n      113.8\n      108.8\n      113.0\n      110.7\n      108.7\n      109.5\n      104.8\n      99.4\n    \n    \n      California\n      123.0\n      121.0\n      123.5\n      124.4\n      126.7\n      127.1\n      128.0\n      126.4\n      126.1\n      121.9\n      120.2\n      118.6\n      115.4\n      110.8\n      104.8\n      102.8\n      99.7\n      97.5\n      90.1\n      82.4\n      77.8\n      68.7\n      67.5\n      63.4\n      58.6\n      56.4\n      54.5\n      53.8\n      52.3\n      47.2\n      41.6\n    \n    \n      Colorado\n      124.8\n      125.5\n      134.3\n      137.9\n      132.8\n      131.0\n      134.2\n      132.0\n      129.2\n      131.5\n      131.0\n      133.8\n      130.5\n      125.3\n      119.7\n      112.4\n      109.9\n      102.4\n      94.6\n      88.8\n      87.4\n      90.2\n      88.3\n      88.6\n      89.1\n      85.4\n      83.1\n      81.3\n      81.2\n      79.6\n      73.0\n    \n    \n      Connecticut\n      120.0\n      117.6\n      110.8\n      109.3\n      112.4\n      110.2\n      113.4\n      117.3\n      117.5\n      117.4\n      118.0\n      116.4\n      114.7\n      114.1\n      112.5\n      111.0\n      108.5\n      109.0\n      104.8\n      100.6\n      91.5\n      86.7\n      83.5\n      79.1\n      76.6\n      79.3\n      76.0\n      75.9\n      75.5\n      73.4\n      71.4\n    \n  \n\n\n\n\nLet‚Äôs observe how cigarettes sales per capita is trending over time w.r.t California and other states.\n\n\nCode\nplot_df = df.loc[df.index == \"California\"].T.reset_index(drop=False)\nplot_df[\"OtherStates\"] = df.loc[df.index != \"California\"].mean(axis=0).values\n\n\nfig = px.line(\n        data_frame = plot_df, \n        x = \"year\", \n        y = [\"California\",\"OtherStates\"], \n        template = \"plotly_dark\")\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[plot_df.California.min()*0.98,plot_df.OtherStates.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Gap in per-capita cigarette sales(in packs)\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Cigarette Sales Trend\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 1 - Cigarette sales comparison b/w California and other states\n\n\nAs we can see from the chart above, we can see that there is a general decline in cigarette sales after the 1980s, and with the introduction of Proposition 99, the decreasing trend accelerated for the state of California. We cannot say for sure if this is happening with any statistical significance, it is just something we observed by examining the chart above.\nTo answer the question of whether Proposition 99 influenced cigarette consumption, we will use the pre-intervention period (1970-1988) to build a synthetic control group that mimics California cigarette sales trend. Then, we will see how this synthetic control behaves after the intervention."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Fitting Synthetic Control using SparseSC package",
    "text": "Fitting Synthetic Control using SparseSC package\nOn a high level SparseSC package provide two functions for fitting Synthetic controls i.e., fit() method and fit_fast() method. On a high level -\n\nfit() - This method tries to compute the weight jointly and results in SCs which are ‚Äòoptimal‚Äô. This is the most common method used in most of the code/libraries I have found but it is computationally expensive and takes a long time to run. Hence, does not scale for larger datasets.\nfit_fast()- This method tries to compute the weight separately by performing some non-matching analysis. This solution is much faster and often the only feasible method with larger datasets. The authors of this package recommend the fit_fast method to start with and only move towards the fit method if needed.\n\nThe SparseSC.fit_fast() method required at least three arguments -\n\nfeatures - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from pre-treatment (1970-1988), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntargets - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from post-treatment (1999-2000), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntreatment_units - This is the list of integers containing the row index value of treated units\n\n\n\n\n\n\n\nNote\n\n\n\nNote that treatment units can be a list of multiple treatment indexes. Think of cases where the same treatment is applied to multiple groups, for example, what if proposition 99 was rolled in both California and Minnesota State, in this case, treatment_units will get [2, 15], which are the respective index of these states.\n\n\n\n## creating required features\nfeatures = df.iloc[:,df.columns <= 1988].values\ntargets = df.iloc[:,df.columns > 1988].values\ntreated_units = [idx for idx, val in enumerate(df.index.values) if val == 'California'] # [2]\n\n## Fit fast model for fitting Synthetic controls\nsc_model = SparseSC.fit_fast( \n    features=features,\n    targets=targets,\n    treated_units=treated_units\n)\n\nNow that we have fitted the model, let‚Äôs get the Synthetic Control output by using predict() function.\n\nresult = df.loc[df.index == 'California'].T.reset_index(drop=False)\nresult.columns = [\"year\", \"Observed\"] \nresult['Synthetic'] = sc_model.predict(df.values)[treated_units,:][0]\nresult.head(5)\n\n\n\n\n\n  \n    \n      \n      year\n      Observed\n      Synthetic\n    \n  \n  \n    \n      0\n      1970.0\n      123.0\n      122.394195\n    \n    \n      1\n      1971.0\n      121.0\n      125.114849\n    \n    \n      2\n      1972.0\n      123.5\n      129.704372\n    \n    \n      3\n      1973.0\n      124.4\n      126.753988\n    \n    \n      4\n      1974.0\n      126.7\n      126.276394\n    \n  \n\n\n\n\nNow that we have our synthetic control, we can plot it with the outcome variable of the State of California.\n\n\nCode\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = [\"Observed\",\"Synthetic\"], \n        template = \"plotly_dark\",)\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result.Observed.min()*0.98,result.Observed.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Synthetic Control Assessment\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig - Assessment of Proposition 99 on state of California using Synthetic Control\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the pre-intervention period, the synthetic control does not reproduce the treatment exactly but follows the curve closely. This is a good sign, as it indicates that we are not overfitting. Also, note that we do see divergence after the intervention (introduction of Proposition 99) after 1988.\n\n\nWith the synthetic control groups in hand, we can estimate the treatment effect as the gap between the treated and the synthetic control outcomes.\n\n\nCode\nresult['California Effect'] = result.Observed - result.Synthetic\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = \"California Effect\", \n        template = \"plotly_dark\",)\nfig.add_hline(0)\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result[\"California Effect\"].min()*0.98,result[\"California Effect\"].max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\n\nfig.update_layout(\n        title  = {\n            'text':\"Difference across time\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Gap in Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig - Gap in Per-capita cigarette sales in California w.r.t Synthetic Control\n\n\n\n\nCode\nprint(f\"Effect of Proposition 99 w.r.t Synthetic Control => {np.round(result.loc[result.year==2000,'California Effect'].values[0],1)} packs\")\n\n\nEffect of Proposition 99 w.r.t Synthetic Control => -28.8 packs\n\n\nLooking at the chart above, we can observe that by the year 2000, Proposition 99 has reduced the sales of cigarettes by ~29 packs. Now we will figure out if this is statistically significant."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Making Inference",
    "text": "Making Inference\nIn Synthetic control, to find if the effect we observed is significant or not, we run a placebo test. A placebo test is taking a random untreated unit and pretending all other units are in control and fit the Synthetic control over this randomly selected untreated unit to estimate the effect. Once we have repeated this placebo test multiple times, we can estimate the distribution of this randomly observed effect and see if the effect observed is significantly different from the placebo observed effect. In the SparceSC package, we can use the estimate_effects method to do this automatically for us. The estimate effects method takes a minimum of two arguments -\n\noutcomes - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), and each column is the period of both pre-treatment and post-treatment period and the value in the matrix is the metric of interest (in this case it is the cigsale value)\nunit_treatment_periods - Vector of treatment periods for each unit, (if a unit is never treated then use np.NaN if vector refers to periods by numerical index)\n\n\n## Creating unit treatment_periods\nunit_treatment_periods = np.full((df.values.shape[0]), np.nan)\nunit_treatment_periods[treated_units] = [idx for idx, colname in enumerate(df.columns) if colname == 1988][0]\n\n## fitting estimate effects method\nsc = SparseSC.estimate_effects(\n    outcomes = df.values,  \n    unit_treatment_periods = unit_treatment_periods, \n    max_n_pl=50, # Number of placebos\n    level=0.9 # Level for confidence intervals\n)\nprint(sc)\n\nPre-period fit diagnostic: Were the treated harder to match in the pre-period than the controls were.\nAverage difference in outcome for pre-period between treated and SC unit (concerning if p-value close to 0 ): \n1.8073663793403947 (p-value: 0.9743589743589743)\n\n(Investigate per-period match quality more using self.pl_res_pre.effect_vec)\n\nAverage Effect Estimation: -16.965374734951705 (p-value: 0.07692307692307693)\n\nEffect Path Estimation:\n -2.712194133284143 (p-value: 0.6923076923076923)\n-6.424223898557088 (p-value: 0.20512820512820512)\n-4.18303325159971 (p-value: 0.5128205128205128)\n-10.210104128966762 (p-value: 0.28205128205128205)\n-10.198518971790051 (p-value: 0.2564102564102564)\n-14.921163932323616 (p-value: 0.15384615384615385)\n-18.441946863077938 (p-value: 0.1282051282051282)\n-21.27793738802989 (p-value: 0.1794871794871795)\n-22.47075245885469 (p-value: 0.1794871794871795)\n-23.397467590595554 (p-value: 0.20512820512820512)\n-26.13637789807555 (p-value: 0.05128205128205128)\n-30.504443997992325 (p-value: 0.02564102564102564)\n-29.67170704122487 (p-value: 0.05128205128205128)\n\n \n\n\nThe estimate_effects method returns an object which will print the treatment effect of each post-treatment year and estimate the significance of the observed difference. The information printed can also be found in the pl_res_pre function of the returned object.\n\nprint(f\"Estimated effect of sales in California state in year 2000 because of preposition 99 is {np.round(sc.pl_res_post.effect_vec.effect[-1])}, \\\nwith a p-value of  {np.round(sc.pl_res_post.effect_vec.p[-1],2)}\")\n\nEstimated effect of sales in California state in year 2000 because of preposition 99 is -30.0, with a p-value of  0.05"
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Conclusion",
    "text": "Conclusion\nHere are some key takeaways -\n\nSynthetic control allows us to combine multiple control units to make them resemble the treated unit. With synthetic control, we can estimate what would have happened to our treated unit in the absence of treatment.\nMicrosoft SparseSC library provides a fast and easy-to-use API to run synthetic control groups and allows us to run a placebo test to estimate the significance of the effects observed."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "title": "Mixing art into the science of model explainability",
    "section": "",
    "text": "Overview on Explainable Boosting Machine and an approach for converting ML explanation to more human-friendly explanation."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "title": "Mixing art into the science of model explainability",
    "section": "1.1 The Interpretability vs Accuracy Trade-off",
    "text": "1.1 The Interpretability vs Accuracy Trade-off\nIn traditional tabular machine learning approaches, Data scientists often deal with the trade-off b/w interpretability and accuracy.\n\n\n\nFig.2: Interpretability/Intelligibility and Accuracy Tradeoff  Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs shown in the chart above, we can see that Glass-Box models like Logistic Regression, Naive Bayes, and Decision Trees are simple models to interpret, and predictions from these models are not highly accurate. On the other hand, Black-Box models like Boosted Trees, Random Forest, and Neural Nets are hard to interpret but lead to highly accurate predictions."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "1.2 Introducing EBMs",
    "text": "1.2 Introducing EBMs\nTo solve the problem just mentioned above EBMs(Explainable Boosted Machine) model was developed by Microsoft Research1. ‚ÄúExplainable Boosting Machine (EBM) is a tree-based, cyclic gradient boosting Generalized Additive Model with automatic interaction detection. EBMs are often as accurate as state-of-the-art BlackBox models while remaining completely interpretable. Although EBMs are often slower to train than other modern algorithms, EBMs are extremely compact and fast at prediction time.‚Äù2\n\n\n\nFig.3: EBMs breaking the Interpretability vs Accuracy paradox Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs we can see from the chart above, EBMs help us break out of this trade-off paradox and help us build models which are both highly interpretable and accurate. To further understand the math behind EBMs I highly encourage watching this 12-minute YouTube video -\n\n\n\n\n\nVideo - The Science Behind InterpretML: Explainable Boosting Machine"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "title": "Mixing art into the science of model explainability",
    "section": "1.3 Glass box vs Black box models. What to choose?",
    "text": "1.3 Glass box vs Black box models. What to choose?\n\n\n\n\n\n\nTip\n\n\n\nThe answer to every complex question in life is ‚ÄúIt depends‚Äù.\n\n\nThere are trade-offs b/w using Glassbox models as compared to Blackbox models. There is no clear winner in picking one model over the other but depending on the situation DS can make an educated guess on what model to pick.\n\n\n\nFig.4: Glassbox models vs BlackBox models\n\n\nTwo considerations to think about while picking glass box vs black box models are the following-\n1) Explainability Requirements - In the domain where there is no need for explanation or it is needed for a data scientist or technical audience for intuition/inspection purposes, in these cases, DS are well off using black box models. In the domain where an explanation is needed because of business or regulatory requirements or where these explanations are served to a non-technical audience (humans), glass-box models have an upper hand. This is because explanations coming out of the glass box models are exact and global.\n\n\n\n\n\n\nNote\n\n\n\nExact and global just means that a value of a particular feature will always have the same effect on each prediction explanation. For example, in the case of the prediction of income of a particular individual being above $50k with age as one of the predictors, if the age is 40 and it will impact the target variable with the same proportion let us say 5% in each observation in the data where the age is 40. This is not the case when we build explanations through LIME and Shapely for black box models. In black-box models, age with the value 40 for example can have a 10% lift in an individual probability of their income being above 50k for one observation and -10% lift in the other.\n\n\n2) Compute Requirement - DS needs to pay attention to various compute requirements for testing and training a model depending on its use case. EBMs are particularly slow in the training phase but provide fast predictions with built-in explanations. So, in cases where you need to train your model every hour, EBMs might not suffice your need. But, in cases where the training of the model happens monthly/weekly, and scores are generated on a more frequent basis (hourly/daily) EBMs might fit the use case well. Also, in cases where you might be required to produce an explanation for each prediction EBMs can save a lot of computing and might be the only feasible technique to use for millions of observations. Look below to understand the operational difference b/w EBMs and other tree-based ensemble methods.\n\n\n\nFig. 5: EBMs vs XgBoost/LightGBM"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "title": "Mixing art into the science of model explainability",
    "section": "2.1 Data Overview",
    "text": "2.1 Data Overview\nFor this example, we will use Adult Income Dataset from the UCI machine learning Repository3. The problem in this dataset is set up as a binary classification problem to predict if a certain individual income based on various census information (education level, age, gender, occupation, etc.) exceeds $50K/year. For sake of simplicity, we are only going to use observations of individuals in the United States and the following predictors -\n\nAge: continuous variable, individuals‚Äô age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\n\n\nCode\n## Importing required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nimport warnings\nimport plotly.io as pio\nimport plotly.express as px\nwarnings.filterwarnings('ignore')\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n## Loading the data\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Education\n      Age\n      Occupation\n      HoursPerWeek\n      Income\n    \n  \n  \n    \n      0\n      Bachelors\n      39\n      Adm-clerical\n      40\n      <=50K\n    \n    \n      1\n      Bachelors\n      50\n      Exec-managerial\n      13\n      <=50K\n    \n    \n      2\n      HS-grad\n      38\n      Handlers-cleaners\n      40\n      <=50K\n    \n    \n      3\n      11th\n      53\n      Handlers-cleaners\n      40\n      <=50K\n    \n    \n      5\n      Masters\n      37\n      Exec-managerial\n      40\n      <=50K\n    \n  \n\n\n\n\nLet‚Äôs look at target variable distribution.\n\n\nCode\nplot_df = df.Income.value_counts().reset_index().rename(columns = {\"index\":\"Income\", \"Income\":\"Count\"})\nfig = px.bar(plot_df, x = \"Income\", y = 'Count')\nfig.update_layout(\n        title  = {\n            'text':\"Target variable distribution\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Income\", \n        yaxis_title=\"Count of obersvations\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 1 - Target variable distribution\n\n\n\n\nCode\nprint(df.Income.value_counts(normalize=True))\n\n\n <=50K    0.754165\n >50K     0.245835\nName: Income, dtype: float64\n\n\n~24.6% of people in our dataset have income greater than $50K. The data looks good, we have the columns we need. We will use Education, Age, Occupation, and HoursPerWeek columns and predict Income. Before modeling, let us perform an 80-20 train-test split.\n\n## Train-Test Split\nX = df[df.columns[0:-1]]\ny = df[df.columns[-1]]\nseed = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\nprint(f\"Data in training {len(y_train)}, Data in testing {len(y_test)}\")\n\nData in training 23336, Data in testing 5834"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "title": "Mixing art into the science of model explainability",
    "section": "2.2 Fitting an EBM Model",
    "text": "2.2 Fitting an EBM Model\nEBMs have a scikit-compatible API, so fitting the model and making predictions are the same as any scikit learn model.\n\nebm = ExplainableBoostingClassifier(random_state=seed, interactions=0)\nebm.fit(X_train, y_train)\n\nauc = np.round(metrics.roc_auc_score((y_test != ' <=50K').astype(int).values, ebm.predict_proba(X_test)[:,1], ),3)\nprint(f\"Accuracy: {np.round(np.mean(ebm.predict(X_test) == y_test)*100,2)}%, AUC: {auc}\")\n\nAccuracy: 80.12%, AUC: 0.828\n\n\nI hope the above code block shows how similar the interpret-ml API is to the scikit learn API. Based on AUC on the validation set we can say our model is better than random predictions.\n\n\n\n\n\n\nTip\n\n\n\nIn practice, if you are dealing with millions of observations, Try doing feature selection using LightGBM/XGboost and only train your final models using EBMs. This will save you time in feature exploration."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "2.3 Explaination from EBMs",
    "text": "2.3 Explaination from EBMs\nInterpret package comes with both global and local explanations and has a variety of visualization tools to inspect what the model is learning.\n\n2.3.1 Global explanations\nGlobal explanations provide the following visualization -\n\nSummary - Feature importance plot, this chart provides the importance of each predictor in predicting the target variable\nFeature interaction with Prediction - This chart is the same look-up table EBM uses in making the actual prediction. This can help you in the inspection of how the feature value is contributing to prediction.\n\n\nebm_global = ebm.explain_global()\nshow(ebm_global, renderer='notebook')\n\n\n\n\nFig. 6: EBMs Global Explaination\n\n\n\n\n2.3.2 Local explanations\nThe local explanation is our per-observation level explanation. EBMs have a great in-built visualization for displaying this information.\n\nebm_local = ebm.explain_local(X_test.iloc[0:5,:], y_test)\nshow(ebm_local, renderer='notebook')\n\n\n\n\nFig. 7: EBMs local Explaination\n\n\nLet‚Äôs take one example of this explanation for observation at index 0 and look at it -\n\nexplainDF = pd.DataFrame.from_dict(\n    {\n        'names': ebm_local.data(0)['names'], \n        'data':ebm_local.data(0)['values'], \n        'contribution':ebm_local.data(0)['scores']\n    })\nexplainDF\n\n\n\n\n\n  \n    \n      \n      names\n      data\n      contribution\n    \n  \n  \n    \n      0\n      Education\n      Bachelors\n      0.733420\n    \n    \n      1\n      Age\n      47\n      1.048227\n    \n    \n      2\n      Occupation\n      ?\n      -0.318846\n    \n    \n      3\n      HoursPerWeek\n      18\n      -0.854202\n    \n  \n\n\n\n\nAs we can see from the data, we can see we have the Name of the columns, the actual values, and the contribution of that value to the actual prediction score. For this observation let us see what the model is learning -\n\nEducation as Bachelors is working in favor of >50K income\nAge value 47 is also in favor of >50K\nOccupation being ‚Äú?‚Äù has a negative impact on >50K income\nHoursPerWeek being 18 has a negative impact on >50K income (Average work week hours in the US are around 40, so this makes sense)\n\nYou can also do it for the entire dataset and collect the importance of each feature. Here is a sample code to do the same.\n\nscores = [x['scores'] for x in ebm_local._internal_obj['specific']]\nsummary = pd.DataFrame(scores)\nsummary.columns = ebm_local.data(0)['names']\nsummary.head()\n\n\n\n\n\n  \n    \n      \n      Education\n      Age\n      Occupation\n      HoursPerWeek\n    \n  \n  \n    \n      0\n      0.733420\n      1.048227\n      -0.318846\n      -0.854202\n    \n    \n      1\n      -0.990661\n      0.309251\n      0.171131\n      0.002109\n    \n    \n      2\n      -0.257254\n      0.735232\n      0.171131\n      0.002109\n    \n    \n      3\n      0.193118\n      0.682721\n      -0.417499\n      0.279677\n    \n    \n      4\n      0.733420\n      0.085672\n      0.389171\n      0.002109\n    \n  \n\n\n\n\nNow we can extract the importance of all data rows in our test set."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "title": "Mixing art into the science of model explainability",
    "section": "2.4 Draw back and concerns",
    "text": "2.4 Draw back and concerns\n\n\n\nCredit - XKCD\n\n\nThese kinds of explanations are still very abstract, even at the observational level reasoning is not human(non-technical) friendly. When the feature count grows this becomes even non-human friendly. Typical business consumers of your model might not be well versed in reading such charts and shy away from trying the insights/predictions the model is giving them. After all, if I don‚Äôt understand something, I don‚Äôt trust it. That is where art comes in, let‚Äôs see how we can build on the above-derived observations and make it easier to understand."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "title": "Model calibration for classification tasks using Python",
    "section": "",
    "text": "A hands-on introduction to model calibration using Python."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "What is Model Calibration?",
    "text": "What is Model Calibration?\nWhen working with classification problems, machine learning models often produce a probabilistic outcome ranging between 0 to 1. This probabilistic output is then used by people to make decisions. Unfortunately, many machine learning models‚Äô probabilistic outputs cannot be directly interpreted as the probability of an event happening. To achieve this outcome, the model needs to be calibrated.\nFormally, a model is said to be perfectly calibrated if, for any probability value p, a prediction of a class with confidence p is correct 100*p percent of the time. In more simple terms, the probabilistic output equals the probability of occurrence.\nLet us visualize a perfect calibrated and non-calibrated curve.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actual_prob\": np.arange(0,1.1, 0.1),\n    \"Calibrated\": np.arange(0,1.1, 0.1),\n    \"Non-Calibrated\":  [min(1,val - np.random.randn()*(1-idx)/20) for idx, val in enumerate(np.arange(0,1.1, 0.1))]\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"actual_prob\", \n        y = [\"Calibrated\", \"Non-Calibrated\"],\n        template = \"plotly_dark\")\n\nfig.update_layout(\n        title  = {\"text\": \"Calibrated vs Non-calibrated model\", \"y\": 0.98, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15), \n        legend=dict(yanchor=\"bottom\", y=0.95, orientation=\"h\")\n)\n\nfig.update_traces(patch={\"line\": {\"dash\": \"dash\"}}) \nfig.show(renderer=\"notebook\")\n\n\n\n                                                \nFig 1 - A visualization of calibrated and non-calibrated curve.\n\n\nOn the x-axis, we have model output p which is between 0 and 1 and on the y-axis, we have fractions of positive captured within the predicted probability bin. We expect a linear relationship with slope 1."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "Why do we need Model calibration?",
    "text": "Why do we need Model calibration?\nThere are many cases where model calibration is not required like in the case of ranking or selecting the top 20% for some targeting campaign. Calibrated models are especially important in making decision between multiple options of different magnitude or sizes, like expected value problems. In complex machine-learning decision engines, a machine-learning model might be used in conjunction with other machine-learning models.\nFor example, the business would like to prioritize upselling customer an additional product. So, a data scientist builds two ML models to estimate the probability of a customer buying two different products in addition to the products they are already purchasing.\n\n\n\nFig. 2: Upsell probability\n\n\nIn this case, Product 1 with $10 in revenue has an 80% probability of upselling, while Product 2 with $100 in revenue has a 60% chance of upselling. As a business, you might want to recommend Product 2 to the customer because of the higher expected value.\nTo do this expected value comparison between models, you need your models to be calibrated to output accurate probabilities."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#case-study-three-methods-for-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#case-study-three-methods-for-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "Case study: Three methods for calibration",
    "text": "Case study: Three methods for calibration\nFor demonstration in this article, we will be solving a binary classification problem using the Adult Income Dataset from the UCI machine learning Repository1 to predict if a certain individual income based on various census information (education level, age, gender, occupation, and more) exceeds $50K/year. For this example, we will limit our scope to the United States and the following predictors:\n\nAge: continuous variable, individuals‚Äô age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\nIn the below code we:\n\nImport the libraries needed for this project\nLoad the data from our source\nLabel our columns\nFilter data to only our desired columns, and limit our scope just to the United States for simplicity\nLabel encode our categorical columns\nSet our random seed (for reproducibility)\nSplit our dataset into test and train\n\n\n\nCode\n## Importing required libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('dark_background')\n\ndef calibration_data(y_true, y_pred):\n    df = pd.DataFrame({'y_true':y_true, 'y_pred_bucket': (y_pred//0.05)*0.05 + 0.025})\n    cdf = df.groupby(['y_pred_bucket'], as_index=False).agg({'y_true':[\"mean\",\"count\"]})\n    return cdf.y_true.values[:,0][cdf.y_true.values[:,1]>10], cdf.y_pred_bucket.values[cdf.y_true.values[:,1]>10]\n\ndef label_encoder(df,columns):\n    '''\n    Function to label encode\n    Required Input - \n        - df = Pandas DataFrame\n        - columns = List input of all the columns which needs to be label encoded\n    Expected Output -\n        - df = Pandas DataFrame with lable encoded columns\n        - le_dict = Dictionary of all the column and their label encoders\n    '''\n    le_dict = {}\n    for c in columns:\n        print(\"Label encoding column - {0}\".format(c))\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values.astype('str')))\n        df[c] = lbl.transform(list(df[c].values.astype('str')))\n        le_dict[c] = lbl\n    return df, le_dict\n\n\n\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\ndf, _ = label_encoder(df, columns = [\"Education\", \"Occupation\", \"Income\"])\nX = df.loc[:,[\"Education\",\"Age\", \"Occupation\", \"HoursPerWeek\"]]\ny = df.Income\n\nseed = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n\nLabel encoding column - Education\nLabel encoding column - Occupation\nLabel encoding column - Income\n\n\nNext, we:\n\nFit a random forest classifier on this data\nVisualize the calibration plot\n\n\n## Fitting the model on training data\nrf_model = RandomForestClassifier(random_state=seed)\nrf_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = rf_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Without calibration\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 3 - An example of non-calibrated classifier\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is over-predicting in lower deciles and under-predicting in higher deciles.\n\n\n\nCalibration method 1: Isotonic Regression\nIsotonic regression is a variation of ordinary least squares regression. Isotonic regression has the added constraints that the predicted values must always be increasing or decreasing, and the predicted values have to lie as close to the observation as possible. Isotonic regression is often used in situations where the relationship between the input and output variables is known to be monotonic, but the exact form of the relationship is not known. In our case, we want a monotonic behavior where the actual probabilities must always be increasing with increasing predicted probability.\nMathematically, we can write the Isotonic regression as following - \\[p_{calib} = \\operatorname{iso}(f(x)) + \\epsilon_i\\] Where-\n\n\\(\\operatorname{iso}\\) is the isotonic function\n\\(f(x)\\) is the predicted probability from the original classifier f\n\\(p_{calib}\\) is the calibrated probability\n\nand we want to optimize the following function - \\[\\operatorname{argmin} \\sum(p_{actual}-p_{calib})^2\\]\nIsotonic regressions is prone to overfitting, hence it works better when there is enough training data available.\nWe will be using CalibratedClassifierCVto calibrate our classifier. The isotonic method fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function. 2\n\n## training model using random forest and isotonic regression for calibration\ncalibrated_rf = CalibratedClassifierCV(RandomForestClassifier(random_state=seed), method = 'isotonic')\ncalibrated_rf.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_rf.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Isotonic\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 4 - An example of calibration using Isotonic regression\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is well calibrated once Isotonic regression is used because the actual probability lies much closer to the expected probability.\n\n\n\n\nCalibration method 2: Sigmoid method / Platt scaling\nThe sigmoid method, also known as Platt scaling, works by transforming the outputs of the classifier using a sigmoid function. The calibrated probabilities are obtained using the following sigmoid function - \\[p_{calib} = \\frac{1}{1+e^{(Af(x) + B)}}\\]\nWhere -\n\n\\(p_{calib}\\) is the calibrated probability\n\\(f(x)\\) is the predicted probability from the original classifier f\nA and B are scalar parameters that learned by the algorithm\n\nThe parameters A and B are estimated using a maximum likelihood method that optimizes on the same training set as that for the original classifier f.¬†\nSigmoid/Platt scaling was originally invented for SVMs and it works well for other methods as well. This method is less prone to overfitting and should be preferred over Isotonic regression if you have less training data.\n\n## training model using random forest and sigmoid method for calibration\ncalibrated_sigmoid = CalibratedClassifierCV(\n     RandomForestClassifier(random_state=seed), method = 'sigmoid')\ncalibrated_sigmoid.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_sigmoid.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot : Sigmoid\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 5 - An example of calibration using sigmoid method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is more calibrated than our default model but not as well as the isotonic method.\n\n\n\n\nCalibration method 3: Train with ‚ÄúLogloss‚Äù metric\nMany boosting based ensembling classifier models like GradientBoostingClassifier, LightGBM, and Xgboost use log_loss as their default loss function. Training with log_loss helps the output probabilities to be calibrated. To explain why log loss helps in calibration, let‚Äôs look at binary log loss function -\n\\[logloss = \\frac{-1}{N}\\sum_{i=1}^{N}y_i\\log{(p(y_i))} + (1-y_i)\\log{(1-p(y_i)})\\]\nNow let‚Äôs take an instance of observation where the true outcome is 1. Consider two predictions of 0.7 and 0.9 from the model. If we take the cutoff > 0.5 both of these predictions are correct but the logloss for 0.7 and 0.9 prediction is 0.36 and 0.1 respectively. As we can see log loss penalizes uncertainty in prediction forcing the model to predict close to the actual outcome.\nTraining with logloss is preferred over fitting a calibration function as model is already calibrated and reduces overhead of training and maintaining an extra model for calibration.\n\n## Fitting the model on training data\ngb_model = GradientBoostingClassifier()\ngb_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = gb_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Using boosting method\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 6 - An example of calibration using boosting method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be well calibrated. This is usually the case with most of the boosted methods as they use the log_loss function as their default loss function for binary classification."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "title": "Model calibration for classification tasks using Python",
    "section": "Conclusion and Practical Guidance",
    "text": "Conclusion and Practical Guidance\nIn this article, we discussed what calibration is, in which applications it is important and why, and three different methods for calibration.\nWe demonstrated isotonic regression which can be used when there is enough training data available and model is not pre-calibrated.\nWe also demonstrated the Sigmoid method of calibration which works better when there is not enough training data and model is not pre-calibrated.\nAnd finally, we demonstrated the Logloss metric for calibration which is the default in boosting methods and is our preferred method since the generated model is pre-calibrated and doesn‚Äôt require training and maintaining an extra model for calibration."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "",
    "text": "A brief introduction to start generating images from text prompts using ü§ó hugging face - Diffusers library.\nThis is my first post of the Stable diffusion series, which I will write on Stable diffusion and other ongoing research happening in this field. Most of my learning can be attributed to knowledge acquired while doing the ‚ÄòFrom Deep learning foundations to Stable Diffusion‚Äô course by FastAI and supplementing this with my research. The first few lessons of the FastAI course are publicly available here, and the rest will become available in early 2023. In this post, I want to give a brief introduction of how to use setup the ü§ó diffusion library and start generating images on your own. Next post, we will do a deep dive into mid-level components of this library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "1 Introduction",
    "text": "1 Introduction\nStable diffusion simply put is a deep learning model which can generate an image given a textual prompt.\n\n\n\nFig. 1: Stable diffusion overview\n\n\nAs we can see from the image above we can pass a textual prompt like ‚ÄúA dog wearing a hat‚Äù and a stable diffusion model can generate an image representative of the text. Pretty amazing!"
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "2 Using Hugging face Diffuser library",
    "text": "2 Using Hugging face Diffuser library\nAs with any python library, we need to follow certain installation steps before we can run it, here is a rundown of these steps.\n\n2.1 Accepting the license\nBefore using the model, you need to go here and log in using your Hugging face account and then accept the model license to download and use the weights.\n\n\n2.2 Token generation\nIf this is your first time using the hugging face library this might sound like a weird step. You need to go here and generate a token (preferably with write access) to download the model.\n\n\n\nFig. 2: Access token page\n\n\nOnce you have generated the token copy it. First, we will download the hugging face hub library using the following code.\n\n!pip install huggingface-hub==0.10.1\n\nRequirement already satisfied: huggingface-hub==0.10.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (0.10.1)\nRequirement already satisfied: filelock in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (3.8.0)\nRequirement already satisfied: tqdm in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.64.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.3.0)\nRequirement already satisfied: packaging>=20.9 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (21.3)\nRequirement already satisfied: requests in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub==0.10.1) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (3.4)\n\n\nThen use the following code, once you run it a widget will appear, paste your newly generated token and click login.\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nLogin successful\nYour token has been saved to /home/aayush/.huggingface/token\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\n\n\n\n\n2.3 Installing diffuser and transformer library\nOnce this process is done, install the dependencies using the following code. This will download the latest version of the diffusers and transformers library.\n\n!pip install -qq -U diffusers transformers\n\nThat‚Äôs it, now we are ready to use the diffusers library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "3 Running Stable Diffusion",
    "text": "3 Running Stable Diffusion\nThe first step is to import the StableDiffusionPipeline from the diffusers library.\n\nfrom diffusers import StableDiffusionPipeline\n\nThe next step is to initialize a pipeline to generate an image. The first time you run the following command, it will download the model from the hugging face model hub to your local machine. You will require a GPU machine to be able to run this code.\n\npipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs pass a textual prompt and generate an image.\n\n# Initialize a prompt\nprompt = \"a dog wearing hat\"\n# Pass the prompt in the pipeline\npipe(prompt).images[0]\n\n\nFig 3 - An example of image generated by the diffuser pipeline.\n\n\n\n\n\nFor further information on the diffusion pipeline read the documentation here."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we saw how to install diffusers library from hugging face and use the Stable diffusion model to generate images using a textual prompt. Read the part 2 here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "",
    "text": "An introduction into what goes on in the pipe function of ü§ó hugging face diffusers library StableDiffusionPipeline function.\nThis is my second post of the Stable diffusion series, if you haven‚Äôt checked out the first one, you can read it here -  1. Part 1 - Introduction to Stable diffusion using ü§ó Hugging Face.\nIn this post, we will understand the basic components of a stable diffusion pipeline and their purpose. Later we will reconstruct StableDiffusionPipeline.from_pretrained function using these components. Let‚Äôs get started -"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "1 Introduction",
    "text": "1 Introduction\nDiffusion models as seen in the previous post can generate high-quality images. Stable diffusion models are a special kind of diffusion model called the Latent Diffusion model. They have first proposed in this paper High-Resolution Image Synthesis with Latent Diffusion Models. The original Diffusion model tends to consume a lot more memory, so latent diffusion models were created which can do the diffusion process in lower dimension space called Latent Space. On a high level, diffusion models are machine learning models that are trained to denoise random Gaussian noise step by step, to get the result i.e., image. In latent diffusion, the model is trained to do this same process in a lower dimension. \nThere are three main components in latent diffusion - \n\nA text encoder, in this case, a CLIP Text encoder\nAn autoencoder, in this case, a Variational Auto Encoder also referred to as VAE\nA U-Net\n\nLet‚Äôs dive into each of these components and understand their use in the diffusion process. The way I will be attempting to explain these components is by talking about them in the following three stages - \n\nThe Basics: What goes in the component and what comes out of the component - This is an important, and key part of the top-down learning approach of understanding ‚Äúthe whole game‚Äù\nDeeper explanation using ü§ó code. - This part will provide more understanding of what the model produces using the code\nWhat‚Äôs their role in the Stable diffusion pipeline - This will build your intuition around how this component fits in the Stable diffusion process. This will help your intuition on the diffusion process"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "2 CLIP Text Encoder",
    "text": "2 CLIP Text Encoder\n\n2.1 Basics - What goes in and out of the component?\nCLIP(Contrastive Language‚ÄìImage Pre-training) text encoder takes the text as an input and generates text embeddings that are close in latent space as it may be if you would have encoded an image through a CLIP model.\n\n\n\nFig. 2: CLIP text encoder\n\n\n\n\n2.2 Deeper explanation using ü§ó code\nAny machine learning model doesn‚Äôt understand text data. For any model to understand text data, we need to convert this text into numbers that hold the meaning of the text, referred to as embeddings. The process of converting a text to a number can be broken down into two parts -  1. Tokenizer - Breaking down each word into sub-words and then using a lookup table to convert them into a number  2. Token_To_Embedding Encoder - Converting those numerical sub-words into a representation that contains the representation of that text \nLet‚Äôs look at it through code. We will start by importing the relevant artifacts.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nLet‚Äôs initialize a prompt and tokenize it.\n\nprompt = [\"a dog wearing hat\"]\ntok =tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \nprint(tok.input_ids.shape)\ntok\n\ntorch.Size([1, 77])\n\n\n{'input_ids': tensor([[49406,   320,  1929,  3309,  3801, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\nA tokenizer returns two objects in the form of a dictionary -  1. input_ids - A tensor of size 1x77 as one prompt was passed and padded to 77 max length. 49406 is a start token, 320 is a token given to the word ‚Äúa‚Äù, 1929 to the word dog, 3309 to the word wearing, 3801 to the word hat, and 49407 is the end of text token repeated till the pad length of 77.  2. attention_mask - 1 representing an embedded value and 0 representing padding.\n\nfor token in list(tok.input_ids[0,:7]): print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token))}\")\n\n49406:<|startoftext|>\n320:a</w>\n1929:dog</w>\n3309:wearing</w>\n3801:hat</w>\n49407:<|endoftext|>\n49407:<|endoftext|>\n\n\nSo, let‚Äôs look at the Token_To_Embedding Encoder which takes the input_ids generated by the tokenizer and converts them into embeddings -\n\nemb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\nprint(f\"Shape of embedding : {emb.shape}\")\nemb\n\nShape of embedding : torch.Size([1, 77, 768])\n\n\ntensor([[[-0.3887,  0.0229, -0.0522,  ..., -0.4902, -0.3066,  0.0673],\n         [ 0.0292, -1.3242,  0.3074,  ..., -0.5264,  0.9766,  0.6655],\n         [-1.5928,  0.5063,  1.0791,  ..., -1.5283, -0.8438,  0.1597],\n         ...,\n         [-1.4688,  0.3113,  1.1670,  ...,  0.3755,  0.5366, -1.5049],\n         [-1.4697,  0.3000,  1.1777,  ...,  0.3774,  0.5420, -1.5000],\n         [-1.4395,  0.3137,  1.1982,  ...,  0.3535,  0.5400, -1.5488]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n\n\nAs we can see above, each tokenized input of size 1x77 has now been translated to 1x77x768 shape embedding. So, each word got represented in a 768-dimensional space.\n\n\n2.3 What‚Äôs their role in the Stable diffusion pipeline\nStable diffusion only uses a CLIP trained encoder for the conversion of text to embeddings. This becomes one of the inputs to the U-net. On a high level, CLIP uses an image encoder and text encoder to create embeddings that are similar in latent space. This similarity is more precisely defined as a Contrastive objective. For more information on how CLIP is trained, please refer to this Open AI blog.\n\n\n\nFig. 3: CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. Credit - OpenAI"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "3 VAE - Variational Auto Encoder",
    "text": "3 VAE - Variational Auto Encoder\n\n3.1 Basics - What goes in and out of the component?\nAn autoencoder contains two parts -  1. Encoder takes an image as input and converts it into a low dimensional latent representation  2. Decoder takes the latent representation and converts it back into an image\n\n\n\nFig. 4: A Variational autoencoder. Original bird pic credit.\n\n\nAs we can see above, the Encoder acts like a compressor that squishes the image into lower dimensions and the decoder recreates the original image back from the compressed version.\n\n\n\n\n\n\nNote\n\n\n\nEncoder-Decoder compression-decompression is not lossless.\n\n\n\n\n3.2 Deeper explanation using ü§ó code\nLet‚Äôs start looking at VAE through code. We will start by importing the required libraries and defining some helper functions.\n\n\nCode\n## To import an image from a URL\nfrom fastdownload import FastDownload\n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## Loading a VAE model\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nLet‚Äôs download an image from the internet.\n\np = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')\nimg = load_image(p)\nprint(f\"Dimension of this image: {np.array(img).shape}\")\nimg\n\nDimension of this image: (512, 512, 3)\n\n\n\n\n\n\nFig. 5: Original bird pic credit.\n\nNow let‚Äôs compress this image by using the VAE encoder, we will be using the pil_to_latents helper function.\n\nlatent_img = pil_to_latents(img)\nprint(f\"Dimension of this latent representation: {latent_img.shape}\")\n\nDimension of this latent representation: torch.Size([1, 4, 64, 64])\n\n\nAs we can see how the VAE compressed a 3 x 512 x 512 dimension image into a 4 x 64 x 64 image. That‚Äôs a compression ratio of 48x! Let‚Äôs visualize these four channels of latent representations.\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')\n\n\n\n\n\nFig. 6: Visualization of latent representation from VAE encoder. \n\nThis latent representation in theory should capture a lot of information about the original image. Let‚Äôs use the decoder on this representation to see what we get back. For this, we will use the latents_to_pil helper function.\n\ndecoded_img = latents_to_pil(latent_img)\ndecoded_img[0]\n\n\n\n\n\nFig. 7: Visualization of decoded latent representation from VAE decoder. \n\nAs we can see from the figure above VAE decoder was able to recover the original image from a 48x compressed latent representation. That‚Äôs impressive!\n\n\n\n\n\n\nNote\n\n\n\nIf you look closely at the decoded image, it‚Äôs not the same as the original image, notice the difference around the eyes. That‚Äôs why VAE encoder/decoder is not a lossless compression.\n\n\n\n\n3.3 What‚Äôs their role in the Stable diffusion pipeline\nStable diffusion can be done without the VAE component but the reason we use VAE is to reduce the computational time to generate High-resolution images. The latent diffusion models can perform diffusion in this latent space produced by the VAE encoder and once we have our desired latent outputs produced by the diffusion process, we can convert them back to the high-resolution image by using the VAE decoder. To get a better intuitive understanding of Variation Autoencoders and how they are trained, read this blog by Irhum Shafkat."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "4 U-Net",
    "text": "4 U-Net\n\n4.1 Basics - What goes in and out of the component?\nThe U-Net model takes two inputs -  1. Noisy latent or Noise- Noisy latents are latents produced by a VAE encoder (in case an initial image is provided) with added noise or it can take pure noise input in case we want to create a random new image based solely on a textual description  2. Text embeddings - CLIP-based embedding generated by input textual prompts \nThe output of the U-Net model is the predicted noise residual which the input noisy latent contains. In other words, it predicts the noise which is subtracted from the noisy latents to return the original de-noised latents.\n\n\n\nFig. 8: A U-Net representation.\n\n\n\n\n4.2 Deeper explanation using ü§ó code\nLet‚Äôs start looking at U-Net through code. We will start by importing the required libraries and initiating our U-Net model.\n\nfrom diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initializing a scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n## Setting number of sampling steps\nscheduler.set_timesteps(51)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nAs you may have noticed from code above, we not only imported unet but also a scheduler. The purpose of a schedular is to determine how much noise to add to the latent at a given step in the diffusion process. Let‚Äôs visualize the schedular function -\n\n\nCode\nplt.plot(scheduler.sigmas)\nplt.xlabel(\"Sampling step\")\nplt.ylabel(\"sigma\")\nplt.title(\"Schedular routine\")\nplt.show()\n\n\n\n\n\n\nFig. 9: Sampling schedule visualization.\n\nThe diffusion process follows this sampling schedule where we start with high noise and gradually denoise the image. Let‚Äôs visualize this process -\n\n\nCode\nnoise = torch.randn_like(latent_img) # Random noise\nfig, axs = plt.subplots(2, 3, figsize=(16, 12))\nfor c, sampling_step in enumerate(range(0,51,10)):\n    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])\n    axs[c//3][c%3].set_title(f\"Step - {sampling_step}\")\n\n\n\n\n\n\nFig. 10: Noise progression through steps.\n\nLet‚Äôs see how a U-Net removes the noise from the image. Let‚Äôs start by adding some noise to the image.\n\n\nCode\nencoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\nlatents_to_pil(encoded_and_noised)[0]\n\n\n\n\n\n\nFig. 11: Noised Input fed to the U-Net.\n\nLet‚Äôs run through U-Net and try to de-noise this image.\n\n## Unconditional textual prompt\nprompt = [\"\"]\n\n## Using clip model to get embeddings\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad(): \n    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n    \n## Using U-Net to predict noise    \nlatent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()]).half()\nwith torch.no_grad():\n    noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n## Visualize after subtracting noise \nlatents_to_pil(encoded_and_noised- noise_pred)[0]\n\n\n\n\n\nFig. 12: De-Noised Output from U-Net\n\nAs we can see above the U-Net output is clearer than the original noisy input passed.\n\n\n4.3 What‚Äôs their role in the Stable diffusion pipeline\nLatent diffusion uses the U-Net to gradually subtract noise in the latent space over several steps to reach the desired output. With each step, the amount of noise added to the latents is reduced till we reach the final de-noised output. U-Nets were first introduced by this paper for Biomedical image segmentation. The U-Net has an encoder and a decoder which are comprised of ResNet blocks. The stable diffusion U-Net also has cross-attention layers to provide them with the ability to condition the output based on the text description provided. The Cross-attention layers are added to both the encoder and the decoder part of the U-Net usually between ResNet blocks. You can learn more about this U-Net architecture here."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn this post, we saw the key components of a Stable diffusion pipeline i.e., CLIP Text encoder, VAE, and U-Net. In the next post, we will look at the diffusion process using these components. Read the next part here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "6 References",
    "text": "6 References\n\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "",
    "text": "An introduction to the diffusion process using ü§ó hugging face diffusers library.\nThis is my third post of the Stable diffusion series, if you haven‚Äôt checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.\nIn previous posts, I went over showing how to install ü§ó diffuser library to start generating your own AI images and key components of the stable diffusion pipeline i.e., CLIP text encoder, VAE, and U-Net. In this post, we will try to put these key components together and do a walk-through of the diffusion process which generates the image."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "1 Overview - The Diffusion Process",
    "text": "1 Overview - The Diffusion Process\nThe stable diffusion model takes the textual input and a seed. The textual input is then passed through the CLIP model to generate textual embedding of size 77x768 and the seed is used to generate Gaussian noise of size 4x64x64 which becomes the first latent image representation.\n\n\n\n\n\n\nNote\n\n\n\nYou will notice that there is an additional dimension mentioned (1x) in the image like 1x77x768 for text embedding, that is because it represents the batch size of 1.\n\n\n\n\n\nFig. 2: The diffusion process.\n\n\nNext, the U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings. The output of the U-Net is predicted noise residual, which is then used to compute conditioned latents via a scheduler algorithm. This process of denoising and text conditioning is repeated N times (We will use 50) to retrieve a better latent image representation. Once this process is complete, the latent image representation (4x64x64) is decoded by the VAE decoder to retrieve the final output image (3x512x512).\n\n\n\n\n\n\nNote\n\n\n\nThis iterative denoising is an important step for getting a good output image. Typical steps are in the range of 30-80. However, there are recent papers that claim to reduce it to 4-5 steps by using distillation techniques."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "2 Understanding the diffusion process through code",
    "text": "2 Understanding the diffusion process through code\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 of the series.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nThe code below is a stripped-down version of what is present in the StableDiffusionPipeline.from_pretrained function to show the important parts of the diffusion process.\n\ndef prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet‚Äôs see if the function works as intended.\n\nimages = prompt_2_img([\"A dog wearing a hat\", \"a photograph of an astronaut riding a horse\"], save_int=False)\nfor img in images:display(img)\n\n\n\n\n\n\n\n\n\n\nLooks like it is working! So let‚Äôs take a deeper dive at the hyper-parameters of the function.  1. prompt - this is the textual prompt we pass through to generate an image. Similar to the pipe(prompt) function we saw in part 1  2. g or guidance scale - It‚Äôs a value that determines how close the image should be to the textual prompt. This is related to a technique called Classifier free guidance which improves the quality of the images generated. The higher the value of the guidance scale, more close it will be to the textual prompt  3. seed - This sets the seed from which the initial Gaussian noisy latents are generated  4. steps - Number of de-noising steps taken for generating the final latents.  5. dim - dimension of the image, for simplicity we are currently generating square images, so only one value is needed  6. save_int - This is optional, a boolean flag, if we want to save intermediate latent images, helps in visualization.\nLet‚Äôs visualize this process of generation from noise to the final image.\n\n\nCode\n## Creating image through prompt_2_img modified function\nimages = prompt_2_img([\"A dog wearing a hat\"], save_int=True)\n\n## Converting intermediate images to video\n!ffmpeg -v 1 -y -f image2 -framerate 20 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n\n## Deleting intermediate images\nshutil.rmtree(f'./steps/')\n\n## Displaying video output\nmp4 = open('out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=600 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n\n\n\n\n\n\n\n      \n\n\n\n\nFig 3: The de-noisation steps visualization."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview and breaks the code to the bare minimum so that we can understand each component. Now that we have the minimum code implemented, in the next post we will see make some tweaks to the mk_img function to add additional functionality i.e., img2img pipeline and negative prompt.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "",
    "text": "An introduction to negative prompting and image to image stable diffusion pipeline using ü§ó hugging face diffusers library.\nThis is my fourth post of the Stable diffusion series, if you haven‚Äôt checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using ü§ó Hugging Face - Putting everything together\nIn previous posts, I went over all the key components of Stable Diffusion and how to get a prompt to image pipeline working. In this post, I will show how to edit the prompt to image function to add additional functionality to our Stable diffusion pipeline i.e., Negative prompting and Image to Image pipeline. Hopefully, this will provide enough motivation to play around with this function and conduct your research."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "1 Variation 1: Negative Prompt",
    "text": "1 Variation 1: Negative Prompt\n\n1.1 What is negative prompting?\nA negative prompt is an additional capability we can add to our model to tell the stable diffusion model what we don‚Äôt want to see in the generated image. This feature is popular to remove anything a user doesn‚Äôt want to see from the original generated image.\n\n\n\nFig. 2: Negative prompt example\n\n\n\n\n1.2 Understanding negative prompting through code\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\n\nNow we are going to change the prompt_2_img function from part 3 by passing an additional function neg_prompts. The way negative prompt works is by using user-specified text instead of an empty string for unconditional embedding(uncond) when doing sampling.\n\n\n\nFig. 3: Negative prompt code change\n\n\nSo let‚Äôs make this change and update our prompt_2_img function.\n\ndef prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"] * bs, text.shape[1])\n    else: uncond =  text_enc(neg_prompts, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet‚Äôs see if the function works as intended.\n\n## Image without neg prompt\nimages = [None, None]\nimages[0] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"\"],steps=50, save_int=False)[0]\nimages[1] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"White hat\"],steps=50, save_int=False)[0]\n    \n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate(images): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"A dog wearing a white hat\")\n    else: axs[c].set_title(f\"Neg prompt - white hat\")\n\n\n\n\n\nFig. 4: Visualization of negative prompting. Left SD generated with prompt ‚ÄúA dog wearing a white hat‚Äù and on right the same caption with negative prompt of ‚ÄúWhite hat‚Äù\n\nAs we can see it can be a really handy feature to fine-tune the image to your liking. You can also use it to generate a pretty realistic face by being really descriptive as this Reddit post. Let‚Äôs try it -\n\nprompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']\nneg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']\nimages = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)\nimages[0]\n\n\n\n\n\n\n\n\nFig. 5: An image generated using negative prompting.\n\n Pretty neat! I hope this gives you some ideas on how to get going with your own variations of stable diffusion. Now let‚Äôs look at another variation of Stable diffusion."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "2 Variation 2: Image to Image pipeline",
    "text": "2 Variation 2: Image to Image pipeline\n\n2.1 What is an image to image pipeline?\nAs seen above, prompt_2_img functions start generating an image from random gaussian noise, but what if we feed an initial seed image to guide the diffusion process? This is exactly how the image to image pipeline works. Instead of purely relying on text conditioning for the output image, we can use an initial seed image mix it with some noise (which can be guided by a strength parameter), and then run the diffusion loop.\n\n\n\nFig. 6: Image to image pipeline example.\n\n\n\n\n2.2 Understanding image to image prompting through code\nNow we are going to change the prompt_2_img function defined above. We will introduce two more parameters to our prompt_2_img_i2i function -  1. init_img: Which is going to be the Image object containing the seed image  2. strength: This parameter will take a value between 0 and 1. The higher the value less the final image is going to look similar to the seed image.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompt) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nInstead of using random noise, you will notice we use the strength parameter to figure out how much noise to add and also the number of steps to run the diffusion loop for. The amount of noise is calculated by multiplying strength(default = 0.8) with steps (default = 50) which is the 10th (50 - 50 * 0.8) step and running the diffusion loop for 40(50*0.8) remaining steps. Let‚Äôs load an initial image and pass it through the prompt_2_img_i2i function.\n\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\nimage = Image.open(p).convert('RGB').resize((512,512))\nprompt = [\"Wolf howling at the moon, photorealistic 4K\"]\nimages = prompt_2_img_i2i(prompts = prompt, init_img = image)\n\n\n\n\n/home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:146: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n\n\n\nCode\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([image, images[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image\")\n    else: axs[c].set_title(f\"Image 2 Image output\")\n\n\n\n\n\n\nFig. 7: Visualization of image to image pipeline. Left is initial image passed in img2img pipeline and right is the output of the img2img pipeline.\n\nWe can see our prompt_2_img_i2i function creates a pretty epic image from the initial sketch provided."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview of how to tweak the prompt_2_img function to add additional capabilities to your stable diffusion loop. The understanding of this lower-level function is useful for trying your own idea to improve stable diffusion or implement new papers which I might cover in my next post.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "title": "Stable diffusion using ü§ó Hugging Face - DiffEdit paper implementation",
    "section": "",
    "text": "An implementation of DIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE using ü§ó hugging face diffusers library.\nIn this post, I am going to implement a recent paper that came from researchers in Meta AI and Sorbonne Universite named DIFFEDIT. This blog will make more sense to people who are either familiar with the stable diffusion process or are reading after four-part series I made on Stable Diffusion -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using ü§ó Hugging Face - Putting everything together  4. Part 4 - Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion\nOriginally, this was the blog post I wanted to write about but realized there is no single place for understanding Stable diffusion with code. Which is the reason I ended up creating the four-part series as a reference or pre-read material to understand this paper."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "title": "Stable diffusion using ü§ó Hugging Face - DiffEdit paper implementation",
    "section": "1 What is DiffEdit?",
    "text": "1 What is DiffEdit?\nIn simple terms, you can think of DiffEdit approach as a more controlled version of the Image to Image pipeline. DiffEdit takes three inputs-  1. An input image  2. Caption - Describing the input image  3. Target Query - Describe the new image you want to generate\nand produce a modified version of the original image based on the query text. This process is particularly good if you want to make a slight tweak to the actual image without completely modifying it.\n\n\n\nFig. 1: Overview of Diff Edit.\n\n\nAs we can see from the image above only the fruits parts of the image were replaced with pears. Pretty amazing results!\nThe way the authors explain they achieve it is by introducing a mask generation module that determines which part of the image should be edited and then only perform text-based diffusion conditioning on the masked part.\n\n\n\nFig. 2: From the paper DiffEdit. An approach to change an input image by providing caption text and new text.\n\n\nAs we can see from the image above taken from the paper, the authors create a mask from the input image which accurately determines the part of the image where fruits are present and generate a mask (shown in Orange) and then perform masked diffusion to replace fruits with pears. Reading further the authors provide a good visual representation of the whole DiffEdit process.\n\n\n\nFig. 3: Three steps of DiffEdit. Credit - Paper\n\n\nAs I was reading this paper, it seems generating the masking is the most important step and the rest is just textual conditioning using the diffusion process. The conditioning of an image using the mask is a similar idea implemented in Hugging face In-Paint Pipeline. As suggested by the authors, ‚Äúthere are three steps to the DiffEdit process -  Step 1: Add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results.  Step2: we encode the input image with DDIM, to estimate the latents corresponding to the input image  Step3: we perform DDIM decoding conditioned on the text query, using the inferred mask to replace the background with pixel values coming from the encoding process at the corresponding timestep‚Äù1\nIn the next sections, we will start implementing these ideas into actual code.\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the stable diffusion series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n\n## Helper functions\n\ndef load_artifacts():\n    '''\n    A function to load all diffusion artifacts\n    '''\n    vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n    unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)    \n    return vae, unet, tokenizer, text_encoder, scheduler\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nvae, unet, tokenizer, text_encoder, scheduler = load_artifacts()\n\n\nLet‚Äôs also download an image which we will use for the code implementation process.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\ninit_img"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#diffedit-purist-implementation",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#diffedit-purist-implementation",
    "title": "Stable diffusion using ü§ó Hugging Face - DiffEdit paper implementation",
    "section": "2 DiffEdit: Purist implementation",
    "text": "2 DiffEdit: Purist implementation\nLet‚Äôs start by implementing the paper as closely as the authors suggested, hence the Purist implementation.\n\n2.1 Mask Creation: First Step of the DiffEdit process\n\n\n\nFig. 4: Step 1 from the DiffEdit paper.\n\n\nThere is a more detailed explanation of Step 1 from the paper, here are the key parts mentioned -  1. Denoise image using different text conditioning, one using reference text and the other using query text, and take differences from the result. The idea is there are more changes in the different parts and not in the background of the image.  2. Repeat this differencing process 10 times  3. Average out these differences and binarize for mask \n\n\n\n\n\n\nNote\n\n\n\nThe third step in mask creation (averaging and binarization) is not explained clearly in the paper and it took me a lot of experiments to get this right.\n\n\nFirst, we will try to implement the paper exactly as it‚Äôs mentioned. We will modify the prompt_2_img_i2i function for this task to return latents instead of rescaled and decoded de-noised images.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n\n        # Conditioning  the latents\n        #latents = scheduler.step(pred, ts, latents).pred_original_sample\n        latents = scheduler.step(pred, ts, latents).prev_sample\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents.detach().cpu()\n\nNext, we will make a create_mask function, which will take an initial image, reference prompt, and query prompt with the number of times we need to repeat the steps. In the paper, the author suggests that n=10 and a strength of 0.5 works well in their experimentation. Hence, the default for the function is adjusted to that. create_mask function performs the following steps -  1. Create two denoised latents, one conditioned on reference text and the second on query text, and take a difference of these latents  2. Repeat this step n times  3. Take an average of these differences and standardize  4. Pick a threshold of 0.5 to binarize and create a mask\n\ndef create_mask(init_img, rp, qp, n=10, s=0.5):\n    ## Initialize a dictionary to save n iterations\n    diff = {}\n    \n    ## Repeating the difference process n times\n    for idx in range(n):\n        ## Creating denoised sample using reference / original text\n        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Creating denoised sample using query / target text\n        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Taking the difference \n        diff[idx] = (np.array(orig_noise)-np.array(query_noise))\n    \n    ## Creating a mask placeholder\n    mask = np.zeros_like(diff[0])\n    \n    ## Taking an average of 10 iterations\n    for idx in range(n):\n        ## Note np.abs is a key step\n        mask += np.abs(diff[idx])  \n        \n    ## Averaging multiple channels \n    mask = mask.mean(0)\n    \n    ## Normalizing \n    mask = (mask - mask.mean()) / np.std(mask)\n    \n    ## Binarizing and returning the mask object\n    return (mask > 0).astype(\"uint8\")\n\nmask = create_mask(init_img=init_img, rp=[\"a horse image\"], qp=[\"a zebra image\"], n=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs visualize the generated mask over the image.\n\n\nCode\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  \n)\n\n\n<matplotlib.image.AxesImage at 0x7ff6be5216d0>\n\n\n\n\n\n\nFig. 5: Masking visualization over our horse image.\n\nAs we can see above, the mask produced covers the horse portion well which is what we want.\n\n\n2.2 Masked Diffusion: Step 2 and 3 of DiffEdit paper.\n\n\n\nFig. 6: Step 2 and 3 from the DiffEdit paper.\n\n\nSteps 2 and 3 need to be implemented in the same loop. Simply put author is saying to condition the latents based on reference text for the non-masked part and on query text for the masked part.  Combine these two parts using this simple formula to create combined latents - \n\\[  \\hat{y}_{t} = My_{t} + (1-M)x_{t} \\]\n\ndef prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.7, steps=70, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    rtext = text_enc(rp) \n    qtext = text_enc(qp)\n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"], rtext.shape[1])\n    emb = torch.cat([uncond, rtext, qtext])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Converting mask to torch tensor\n    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n            \n        # Performing Guidance\n        rpred = u + g*(rt-u)\n        qpred = u + g*(qt-u)\n\n        # Conditioning  the latents\n        rlatents = scheduler.step(rpred, ts, latents).prev_sample\n        qlatents = scheduler.step(qpred, ts, latents).prev_sample\n        latents = mask*qlatents + (1-mask)*rlatents\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents_to_pil(latents)\n\nLet‚Äôs visualize the generated image.\n\noutput = prompt_2_img_diffedit(\n    rp = [\"a horse image\"], \n    qp=[\"a zebra image\"],\n    init_img=init_img, \n    mask = mask, \n    g=7.5, seed=100, strength =0.5, steps=70, dim=512)\n\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([init_img, output[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image \")\n    else: axs[c].set_title(f\"DiffEdit output\")\n\n\n\n\n\n\n\n\nFig. 7: DiffEdit output visualization\n\n Let‚Äôs create a simple function for the masking and diffusion process.\n\ndef diffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=70, dim=512):\n    \n    ## Step 1: Create mask\n    mask = create_mask(init_img=init_img, rp=rp, qp=qp)\n    \n    ## Step 2 and 3: Diffusion process using mask\n    output = prompt_2_img_diffedit(\n        rp = rp, \n        qp=qp, \n        init_img=init_img, \n        mask = mask, \n        g=g, \n        seed=seed,\n        strength =strength, \n        steps=steps, \n        dim=dim)\n    return mask , output\n\nLet‚Äôs also create a visualization function for DiffEdit showing the original input image, masked image, and final output image.\n\ndef plot_diffEdit(init_img, output, mask):\n    ## Plotting side by side\n    fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n    \n    ## Visualizing initial image\n    axs[0].imshow(init_img)\n    axs[0].set_title(f\"Initial image\")\n    \n    ## Visualizing initial image\n    axs[2].imshow(output[0])\n    axs[2].set_title(f\"DiffEdit output\")\n    \n    ## Visualizing the mask \n    axs[1].imshow(np.array(init_img), cmap='gray') \n    axs[1].imshow(\n        Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n        cmap='cividis', \n        alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  \n    )\n    axs[1].set_title(f\"DiffEdit mask\")\n\nLet‚Äôs test this function on a few images.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask, output = diffEdit(init_img, rp = [\"a horse image\"], qp=[\"a zebra image\"])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 8: Purist implementation output example\n\nPerfect, let‚Äôs try another one.\n\np = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')\ninit_img = load_image(p)\nmask, output = diffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 9: Purist implementation output example"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#fastdiffedit-a-faster-diffedit-implementation",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#fastdiffedit-a-faster-diffedit-implementation",
    "title": "Stable diffusion using ü§ó Hugging Face - DiffEdit paper implementation",
    "section": "3 FastDiffEdit: A faster DiffEdit implementation",
    "text": "3 FastDiffEdit: A faster DiffEdit implementation\nNow we have seen the purist implementation, there are some improvements I suggest we can make to the original DiffEdit process in terms of speed and better results. Let‚Äôs call these improvements FastDiffEdit.\n\n3.1 Mask Creation: Fast DiffEdit masking process\nMy biggest issue with the current way of doing masking is that it takes too much time(~50 sec on A4500 GPU). My take is we don‚Äôt need to run a full diffusion loop to denoise the image but just use the U-net prediction of the original sample in one shot and increase the repetition to 20 times. In this case, we can improve the computation from 10*25 = 250 steps to 20 steps (12x less loop). Let‚Äôs see if this works in practice.\n\ndef prompt_2_img_i2i_fast(prompts, init_img, g=7.5, seed=100, strength =0.5, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"], text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # We need to scale the i/p latents to match the variance\n    inp = scheduler.scale_model_input(torch.cat([latents] * 2), timesteps)\n    # Predicting noise residual using U-Net\n    with torch.no_grad(): u,t = unet(inp, timesteps, encoder_hidden_states=emb).sample.chunk(2)\n         \n    # Performing Guidance\n    pred = u + g*(t-u)\n\n    # Zero shot prediction\n    latents = scheduler.step(pred, timesteps, latents).pred_original_sample\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents.detach().cpu()\n\nLet‚Äôs create a new masking function that can take our prompt_2_img_i2i_fast function.\n\ndef create_mask_fast(init_img, rp, qp, n=20, s=0.5):\n    ## Initialize a dictionary to save n iterations\n    diff = {}\n    \n    ## Repeating the difference process n times\n    for idx in range(n):\n        ## Creating denoised sample using reference / original text\n        orig_noise = prompt_2_img_i2i_fast(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Creating denoised sample using query / target text\n        query_noise = prompt_2_img_i2i_fast(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Taking the difference \n        diff[idx] = (np.array(orig_noise)-np.array(query_noise))\n    \n    ## Creating a mask placeholder\n    mask = np.zeros_like(diff[0])\n    \n    ## Taking an average of 10 iterations\n    for idx in range(n):\n        ## Note np.abs is a key step\n        mask += np.abs(diff[idx])  \n        \n    ## Averaging multiple channels \n    mask = mask.mean(0)\n    \n    ## Normalizing \n    mask = (mask - mask.mean()) / np.std(mask)\n    \n    ## Binarizing and returning the mask object\n    return (mask > 0).astype(\"uint8\")\n\nLet‚Äôs see if this new masking function produces a good mask.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask = create_mask_fast(init_img=init_img, rp=[\"a horse image\"], qp=[\"a zebra image\"], n=20)\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  \n)\n\n<matplotlib.image.AxesImage at 0x7ff6847a8b20>\n\n\n\n\n\n\nFig. 10: FastDiffEdit masking visualization over our horse image.\n\n As we can see above the masking is improved and compute time has reduced from ~50 seconds to ~10 secs on my machine(5x improvement!).\nLet‚Äôs improve our masking by adding a cv2 trick. This will just smooth out the masking a little bit more.\n\nimport cv2\ndef improve_mask(mask):\n    mask  = cv2.GaussianBlur(mask*255,(3,3),1) > 0\n    return mask.astype('uint8')\n\n\nmask = improve_mask(mask)\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  \n)\n\n<matplotlib.image.AxesImage at 0x7ff6a425caf0>\n\n\n\n\n\n\nFig. 11: Improved FastDiffEdit masking visualization over our horse image with cv2 Gaussian blur trick.\n\n As we can see above the masking has become a bit more smooth and covers more area.\n\n\n3.2 Masked Diffusion: Replace with ü§ó inpaint pipeline\nSo, instead of using our function to perform the masked diffusion, there is a special pipeline in ü§ó diffusers library called inpaint pipeline. Which takes the query prompt, initial image, and generated mask to generate the output image. Let‚Äôs start by loading in the inpaint pipeline.\n\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n\n\n\nLet‚Äôs use the inpaint pipeline with our generated mask and image.\n\npipe(\n    prompt=[\"a zebra image\"], \n    image=init_img, \n    mask_image=Image.fromarray(mask*255).resize((512,512)), \n    generator=torch.Generator(\"cuda\").manual_seed(100),\n    num_inference_steps = 20\n).images[0]\nimage\n\n\n\n\n\n\n\n\nFig. 12: In-paint pipeline output.\n\n As we can see above, inpaint pipeline creates a more realistic zebra image. Let‚Äôs create a simple function for the masking and diffusion process.\n\ndef fastDiffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=20, dim=512):\n    \n    ## Step 1: Create mask\n    mask = create_mask_fast(init_img=init_img, rp=rp, qp=qp, n=20)\n    \n    ## Improve masking using CV trick\n    mask = improve_mask(mask)\n    \n    ## Step 2 and 3: Diffusion process using mask\n    output = pipe(\n        prompt=qp, \n        image=init_img, \n        mask_image=Image.fromarray(mask*255).resize((512,512)), \n        generator=torch.Generator(\"cuda\").manual_seed(100),\n        num_inference_steps = steps\n    ).images\n    return mask , output\n\nLet‚Äôs test this function on a few images.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask, output = fastDiffEdit(init_img, rp = [\"a horse image\"], qp=[\"a zebra image\"])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\nFig. 13: FastDiffEdit output example\n\nPerfect, let‚Äôs try another one.\n\np = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')\ninit_img = load_image(p)\nmask, output = fastDiffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\nFig. 14: FastDiffEdit output example"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - DiffEdit paper implementation",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we implemented the DiffEdit paper as the author mentioned and then we proposed improvements to the method to create FastDiffEdit which speeds up computation times up to 5 times.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html",
    "title": "Python OOPs fundamentals",
    "section": "",
    "text": "An introduction to Object Oriented programming using Python.\nIncreasingly it‚Äôs becoming important for Data professionals to become better at programming and modern programming is centered around Object Oriented programming paradigm. This article helps in explaining some important programming concepts which are mostly language agnostic but we will be using Python in this article.\nObject-oriented programming (OOPs) is a programming paradigm that relies on the concept of classes and objects. The basic idea of OOP is to divide a sophisticated program into a number of objects that interact with each other to achieve the desired functionality. There are several advantages of using OOP for data science:\nOverall, OOP can help data professionals organize and manage their code more effectively, making it easier to develop and maintain data science projects. Let‚Äôs dive into the OOPs concept."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#what-are-objects-and-classes",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#what-are-objects-and-classes",
    "title": "Python OOPs fundamentals",
    "section": "1 What are Objects and Classes?",
    "text": "1 What are Objects and Classes?\nClasses are the blueprint for defining an Object. While an Object is a collection of data/properties and their behaviors/methods.\nFor example- Think of a class Bulb that will have a state (On/Off) and methods to turnOn and turnoff the bulb.\n\nclass Bulb():\n    def __init__(self, onOff=False): self.onOff = onOff    \n    def turnOn(self): self.onOff = True\n    def turnOff(self): self.onOff = False\n\nNow we can create multiple bulb objects from this Bulb class.\n\nb1 = Bulb(onOff=True)\nb2 = Bulb()\nprint(f\"Bulb 1 state is :{b1.onOff}, Bulb 2 state is :{b2.onOff}\")\n\nBulb 1 state is :True, Bulb 2 state is :False\n\n\nb1 and b2 are objects of the class Bulb. Let‚Äôs use the turnOn and turnOff methods to update the bulb properties.\n\nb1.turnOff(); b2.turnOn()\nprint(f\"Bulb 1 state is :{b1.onOff}, Bulb 2 state is :{b2.onOff}\")\n\nBulb 1 state is :False, Bulb 2 state is :True\n\n\nWe can see from the example above, a Bulb object contains the onOff property. Properties are variables that contain information regarding the object of a class and Methods like turnOn and turnOff in our Bulb class are functions that have access to the properties of a class. Methods can accept additional parameters, modify properties and return values."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-and-instance-variables",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-and-instance-variables",
    "title": "Python OOPs fundamentals",
    "section": "2 Class and Instance variables",
    "text": "2 Class and Instance variables\nIn Python, properties can be defined in two ways -\n\nClass Variables - Class variables are shared by all objects of the class. A change in the class variable will change the value of that property in all the objects of the class.\nInstance Variables - Instance variables are unique to each instance or object of the class. A change in instance variable will change the value of the property in that specific object only.\n\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n    \ne1 = Employee('Aayush')\ne2 = Employee('John')\n\nprint(f'Name :{e1.name}')\nprint(f'Company Name: {e1.companyName}')\nprint(f'Name :{e2.name}')\nprint(f'Company Name: {e2.companyName}')\n\nName :Aayush\nCompany Name: Microsoft\nName :John\nCompany Name: Microsoft\n\n\nWe can see above, the class variable is defined outside of the initializer and the instance variable is defined inside the initializer.\n\nEmployee.companyName = \"Amazon\"\nprint(e1.companyName, e2.companyName)\n\nAmazon Amazon\n\n\nWe can see above changing a class variable in the Employee class changes the class variable in all objects of the class. Most of the time we will be using instance variables but knowledge about class variables can come in handy. Let‚Äôs look at an interesting use of class variable -\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    companyEmployees = []\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n        self.companyEmployees.append(self.name)\n    \ne1 = Employee('Aayush')\ne2 = Employee('John')\n\nprint(f'Name :{e1.name}')\nprint(f'Team Members: {e1.companyEmployees}')\nprint(f'Name :{e2.name}')\nprint(f'Company Name: {e2.companyEmployees}')\n\nName :Aayush\nTeam Members: ['Aayush', 'John']\nName :John\nCompany Name: ['Aayush', 'John']\n\n\nWe can see above, we are saving all objects of the Employee class in companyEmployees which is a list shared by all objects of the class Employee."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-static-and-instance-methods",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-static-and-instance-methods",
    "title": "Python OOPs fundamentals",
    "section": "3 Class, Static and Instance methods",
    "text": "3 Class, Static and Instance methods\nIn Python classes, we have three types of methods -\n\nClass Methods - Class methods work with class variables and are accessible using the class name rather than its object.\nStatic Methods - Static methods are methods that are usually limited to class only and not their objects. They don‚Äôt typically modify or access class and instance variables. They are used as utility functions inside the class and we don‚Äôt want the inherited class to modify them.\nInstance Methods - Instance methods are the most used methods and have access to instance variables within the class. They can also take new parameters to perform desired operations.\n\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    companyEmployees = []\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n        self.companyEmployees.append(self.name)\n    \n    @classmethod\n    def getCompanyName(cls): # This is a class method\n         return cls.companyName\n    \n    @staticmethod\n    def plusTwo(x): # This is a static method\n        return x+2\n    \n    def getName(self): # This is an instance method\n        return self.name\n    \ne1 = Employee('Aayush')\nprint(f\"Calling class method. Company name is {e1.getCompanyName()}\")\nprint(f\"Calling Static method. {e1.plusTwo(2)}\")\nprint(f\"Calling instance method. Employee name is {e1.getName()}\")\n\nCalling class method. Company name is Microsoft\nCalling Static method. 4\nCalling instance method. Employee name is Aayush\n\n\nWe can see above we use the @classmethod decorator to define the class method. cls is used to refer to the class just as self is used to refer to the object of the class. The class method at least takes one argument cls.\n\n\n\n\n\n\nNote\n\n\n\nWe can use any other name instead of cls but cls is used as a convention.\n\n\nWe use @staticmethod decorator to define static class plusTwo. We can see that static methods don‚Äôt take any argument like self and cls.\nThe most commonly used methods are instance methods and they can be defined without a decorator within the class. Just like the class method they take at least one argument which is self by convention.\n\n\n\n\n\n\nNote\n\n\n\nWe can use any other name instead of self but self is used as a convention."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#access-modifiers",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#access-modifiers",
    "title": "Python OOPs fundamentals",
    "section": "4 Access Modifiers",
    "text": "4 Access Modifiers\nAccess modifiers limit access to the variables and functions of a class. There are three types of access modifiers - public, protected, and private.\n\n4.1 Public Attributes\nPublic attributes are those methods and properties which can be accessed anywhere inside and outside of the class. By default, all the member variables and functions are public.\n\nclass Employee:\n    def __init__(self, name):\n        self.name = name ## Public variable\n        \n    def getName(self): ## Public method\n        return self.name\n\ne1 = Employee(\"Aayush\")\nprint(f\"Employee Name: {e1.getName()}\")\n\nEmployee Name: Aayush\n\n\nIn the case above, both property name and method getName are public attributes.\n\n\n4.2 Protected Attributes\nProtected attributes are similar to public attributes which can be accessed within the class and also available to subclasses. The only difference is the convention, which is to define each protected member with a single underscore ‚Äú_‚Äù.\n\nclass Employee:\n    def __init__(self, name, project):\n        self.name = name ## Public variable\n        self._project = project ## Protected variable\n        \n    def getName(self): ## Public method\n        return self.name\n    \n    def _getProject(self): ## Protected method\n        return self._project\n    \ne1 = Employee(\"Aayush\", \"Project Orland\")\nprint(f\"Employee Name: {e1.getName()}\")\nprint(f\"Project Name: {e1._getProject()}\")\n\nEmployee Name: Aayush\nProject Name: Project Orland\n\n\nIn the case above, both property _project and method _getProject are protected attributes.\n\n\n4.3 Private Attributes\nPrivate attributes are accessible within the class but not outside of the class. To define a private attribute, prefix the method or property with the double underscore‚Äù_‚Äú.\n\nclass Employee:\n    def __init__(self, name, project, salary):\n        self.name = name ## Public variable\n        self._project = project ## Protected variable\n        self.__salary = salary\n        \n    def getName(self): ## Public method\n        return self.name\n    \n    def _getProject(self): ## Protected method\n        return self._project\n    \n    def __getSalary(self): ## Protected method\n        return self.__salary\n    \ne1 = Employee(\"Aayush\", \"Project Orland\", \"3500\")\nprint(f\"Employee Name: {e1.getName()}\")\nprint(f\"Project Name: {e1.__getSalary()}\") \n\nEmployee Name: Aayush\n\n\nAttributeError: 'Employee' object has no attribute '__getSalary'\n\n\nWe can see above, __salary property and __getSalary method are both private attributes and when we call them outside of the class they throw an error that the 'Employee' object has no attribute '__getSalary'."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#encapsulation",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#encapsulation",
    "title": "Python OOPs fundamentals",
    "section": "5 Encapsulation",
    "text": "5 Encapsulation\nEncapsulation in OOP refers to binding data and the methods to manipulate that data together in a single unit, that is, class. Encapsulation is usually used to hide the state and representation of the object from the outside. A good use of encapsulation is to make all properties private of a class to prevent direct access from outside and use public methods to let the outside world communicate with the class.\n\nclass Employee:\n    def __init__(self, name, project, salary):\n        self.__name = name ## Public variable\n        self.__project = project ## Protected variable\n        self.__salary = salary\n        \n    def getName(self): ## Public method\n        return self.__name\n    \ne1 = Employee(\"Aayush\", \"Project Orland\", \"3500\")\nprint(f\"Employee Name: {e1.getName()}\")\n\nEmployee Name: Aayush\n\n\nEncapsulation has several advantages -\n\nProperties of the class can be hidden from the outside world\nMore control over what the outside world can access from the class\n\nA good example of encapsulation would be an access control class based on username and password.\n\nclass Auth:\n    def __init__(self, userName=None, password=None):\n        self.__userName = userName\n        self.__password = password\n        \n    def login(self, userName, password):\n        if (self.__userName == userName) and (self.__password == password):\n            print (f\"Access granted to {userName}\")\n        else:\n            print(\"Invalid credentials\")\n            \ne1 = Auth(\"Aayush\", \"whatever\")\ne1.login(\"Aayush\", \"whatever\") ## This will grant access\n\ne1.login(\"Aayush\", \"aasdasd\") ## This will say invalid creds\ne1.__password ## This will raise an error as private properties can't be accessed from outside.\n\nAccess granted to Aayush\nInvalid credentials\n\n\nAttributeError: 'Auth' object has no attribute '__password'\n\n\nAs we can see above __username and __password are protected properties and can only be used by the class to grand or reject access requests."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#inheritance",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#inheritance",
    "title": "Python OOPs fundamentals",
    "section": "6 Inheritance",
    "text": "6 Inheritance\nInheritance provides a way to create new classes from the existing classes. The new class will inherit all the non-private attributes(properties and methods) from the existing class. The new class can be called a child class and the existing class can be called a parent class.\n\nimport math\nclass Shape:\n    def __init__(self, name):\n        self.name = name\n        \n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    ## Overiding the getArea function\n    def getArea(self):\n        return self.edge**2\n    \nclass Circle(Shape):\n    def __init__(self, radius):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Circle\")\n        self.radius = radius\n    ## Overiding the getArea function\n    def getArea(self):\n        return math.pi * (self.radius**2)\n    \nobj1 = Square(4)\nobj1.printDetails()\n\nobj2 = Circle(3)\nobj2.printDetails()\n\nThis shape is called Square and area is 16.\nThis shape is called Circle and area is 28.274333882308138.\n\n\nWe can see above we defined a parent class Shape and then we inherited it to create a Square and Circle child class. While defining the Square and Circle class we overwrote the getArea function pertinent to the class but we used the printDetails function from the parent class to print details about child classes. The more common example in the machine learning world would be to create your own models in Pytorch where we inherit from nn.Module class to create a new model.\n\n6.1 Use of super() Function\nsuper() function comes into play when we implement inheritance. The super() function is used to refer to the parent class without explicitly naming the class. super() function can be used to access parent class properties, calling the parent class, and can be used as initializers. Let‚Äôs look at the example above and see how we can modify the Square class to use super() function.\n\nclass Shape:\n    maxArea = 100\n    def __init__(self, name): self.name = name\n    def getArea(self): pass\n    def printDetails(self): \n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \n\nclass Square(Shape):\n    maxArea = 50\n    def __init__(self, edge):\n        super().__init__(name = \"Square\") ## Initializing parent class\n        self.edge = edge\n    \n    def getName(self):\n        return super().maxArea\n    \n    def getArea(self):\n        return self.edge**2\n    \n    def printDetails(self):\n        super().printDetails() ## Calling a parent class function\n        print(f\"Max area from Shape class: {super().maxArea}\") ## Accessing parent class property\n        print(f\"Max area from Square class: {self.maxArea}\")\n\nobj1 = Square(4)\nobj1.getName()\nobj1.printDetails()\n\nThis shape is called Square and area is 16.\nMax area from Shape class: 100\nMax area from Square class: 50\n\n\nAs we can see in the example above we have used -\n\nsuper().__init__ to initialize the parent Shape class\nsuper().printDetails() function to use a method from parent class\nsuper().maxArea to access a property of a parent class\n\nThere are many advantages of inheritance -\n\nReusability - Inheritance makes the code reusable. Common methods and properties can be stored in a parent class and child classes can inherit these methods.\nModification - Code modification becomes easier if we use inheritance, if we want to make a change in the base class function it will be propagated to the child classes.\nExtensibility - We can derive new classes from the old ones by keeping things we need in the derived class."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#polymorphism",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#polymorphism",
    "title": "Python OOPs fundamentals",
    "section": "7 Polymorphism",
    "text": "7 Polymorphism\nPolymorphism refers to the same object exhibiting different forms and behaviors. For example consider our shape class which could be a square, rectangle, polygon, etc. Instead of writing multiple functions to get the area of these shapes, we can use a common function like getArea() and implement this function in the derived class.\n\nimport math\nclass Shape:\n    def __init__(self, name):\n        self.name = name\n        \n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    ## Overiding the getArea function\n    def getArea(self):\n        return self.edge**2\n    \nclass Circle(Shape):\n    def __init__(self, radius):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Circle\")\n        self.radius = radius\n    ## Overiding the getArea function\n    def getArea(self):\n        return math.pi * (self.radius**2)\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nobj2 = Circle(3)\nprint(f\"Area of this {obj2.name} is {obj2.getArea()}\")\n\nArea of this Square is 16\nArea of this Circle is 28.274333882308138\n\n\nAs we can see above there is a pre-defined dummy method called getArea in the Shape class. We override this method in the Square and Circle class. This technique is called method overriding. The advantage of method overriding is that the derived class can write its own specific implementation based on the requirement while using the same function name.\n\n7.1 Abstract base classes\nAbstract base classes define a set of methods and properties that a class must implement in order to inherit the parent class. This is a useful technique to enforce that certain functions within the derived class must exist. To define an abstract base class, we use the abc module. The abstract base class inherits from the built-in ABC class and we use the decorator @abstractmethod to declare an abstract method.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    def __init__(self, name):\n        self.name = name\n    \n    @abstractmethod\n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nTypeError: Can't instantiate abstract class Square with abstract methods getArea\n\n\nWe can see above that we have created a Shape class from the ABC class which has an abstract method getArea. Since our child class Square didn‚Äôt have getArea implemented we get an error instantiating this class.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    def __init__(self, name):\n        self.name = name\n    \n    @abstractmethod\n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    def getArea(self): return self.edge**2\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nArea of this Square is 16\n\n\nWe can see above, once we implemented the getArea method, the code runs fine.\n\nAbstract base classes serve as a blueprint for derived classes to implement methods that are required to run the function appropriately."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#conclusion",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#conclusion",
    "title": "Python OOPs fundamentals",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nIn this article, we learned about what is object-oriented programming and key concepts using Python. A good understanding of these concepts will lay a solid foundation for any software professional to write and understand python code better.\nI hope you enjoyed reading it. If there is any feedback on the code or just the blog post, feel free to comment below or reach out on LinkedIn."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html",
    "title": "privateGPT Walkthrough",
    "section": "",
    "text": "A code walkthrough of privateGPT repo on how to build your own offline GPT Q&A system.\nLarge Language Models (LLMs) have surged in popularity, pushing the boundaries of natural language processing. OpenAI‚Äôs GPT-3.5 is a prime example, revolutionizing our technology interactions and sparking innovation. Particularly, LLMs excel in building Question Answering applications on knowledge bases. In this blog, we delve into the top trending GitHub repository for this week: the PrivateGPT repository and do a code walkthrough."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#ingestion-pipeline",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#ingestion-pipeline",
    "title": "privateGPT Walkthrough",
    "section": "3.1 Ingestion Pipeline",
    "text": "3.1 Ingestion Pipeline\nLet‚Äôs delve into the ingestion pipeline for a closer examination. The ingestion pipeline encompasses the following steps:\n\nIdentifying files with various extensions and retrieving all the knowledge base from the source directory.\nSplitting the documents into smaller chunks based on the parameters of chunk_size and chunk_overlap.\nInitializing the Huggingfaceembeddings module of langchain. This involves loading a pre-trained language model from the sentence_transformers library.\nInitializing the Chroma database from langchain.vectorstores. This step involves taking the chunked text and the initialized embedding model and saving it in the embedding database on disk.\n\n\n\n\nFig.5: Ingestion Pipeline\n\nLet‚Äôs look at these steps one by one.\n\n3.1.1 Identifying and loading files from the source directory\nFirst, we import the required libraries and various text loaders from langchain.document_loaders.\n\nimport os\nimport glob\nfrom typing import List\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom langchain.document_loaders import (\n    CSVLoader,\n    EverNoteLoader,\n    PDFMinerLoader,\n    TextLoader,\n    UnstructuredEmailLoader,\n    UnstructuredEPubLoader,\n    UnstructuredHTMLLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredODTLoader,\n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader,\n)\nfrom langchain.docstore.document import Document\n\nNext, we define the mapping b/w each extension and their respective langchain document loader. You can read document loader documentation for more available loaders.\n\n# Map file extensions to document loaders and their arguments\nLOADER_MAPPING = {\n    \".csv\": (CSVLoader, {}),\n    \".doc\": (UnstructuredWordDocumentLoader, {}),\n    \".docx\": (UnstructuredWordDocumentLoader, {}),\n    \".enex\": (EverNoteLoader, {}),\n    \".epub\": (UnstructuredEPubLoader, {}),\n    \".html\": (UnstructuredHTMLLoader, {}),\n    \".md\": (UnstructuredMarkdownLoader, {}),\n    \".odt\": (UnstructuredODTLoader, {}),\n    \".pdf\": (PDFMinerLoader, {}),\n    \".ppt\": (UnstructuredPowerPointLoader, {}),\n    \".pptx\": (UnstructuredPowerPointLoader, {}),\n    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n}\n\nNext, we define our single document loader.\n\ndef load_single_document(file_path: str) -> Document:\n    ## Find extension of the file\n    ext = \".\" + file_path.rsplit(\".\", 1)[-1] \n    if ext in LOADER_MAPPING: \n        # Find the appropriate loader class and arguments\n        loader_class, loader_args = LOADER_MAPPING[ext] \n        # Invoke the instance of document loader\n        loader = loader_class(file_path, **loader_args) \n        ## Return the loaded document\n        return loader.load()[0] \n    raise ValueError(f\"Unsupported file extension '{ext}'\")\n    \ngit_dir = \"../../../../privateGPT/\"\nloaded_document = load_single_document(git_dir+'source_documents/state_of_the_union.txt')\nprint(f'Type of loaded document {type(loaded_document)}')\nloaded_document\n\nType of loaded document <class 'langchain.schema.Document'>\n\n\nDocument(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat‚Äôs why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin‚Äôs latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn‚Äôt respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia‚Äôs lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies ‚Äìwe are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia‚Äôs largest banks from the international financial system.  \\n\\nPreventing Russia‚Äôs central bank from defending the Russian Ruble making Putin‚Äôs $630 Billion ‚Äúwar fund‚Äù worthless.   \\n\\nWe are choking off Russia‚Äôs access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights ‚Äì further isolating Russia ‚Äì and adding an additional squeeze ‚Äìon their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia‚Äôs economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies ‚Äì in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we‚Äôve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield ‚Äì he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin‚Äôs war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn‚Äôt have taken something so terrible for people around the world to see what‚Äôs at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It‚Äôs going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat‚Äôs why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people‚Äîand left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs. \\n\\nIn fact‚Äîour economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn‚Äôt worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won‚Äôt be able to compete for the jobs of the 21st Century if we don‚Äôt fix that. \\n\\nThat‚Äôs why it was so important to pass the Bipartisan Infrastructure Law‚Äîthe most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe‚Äôre done talking about infrastructure weeks. \\n\\nWe‚Äôre going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world‚Äîparticularly with China.  \\n\\nAs I‚Äôve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe‚Äôll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we‚Äôll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe‚Äôll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes‚Äîso every child‚Äîand every American‚Äîhas clean water to drink at home and at school, provide affordable high-speed internet for every American‚Äîurban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced. \\n\\nAnd tonight, I‚Äôm announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America ‚Äì we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere‚Äôs been a law on the books for almost a century \\nto make sure taxpayers‚Äô dollars support American jobs and businesses. \\n\\nEvery Administration says they‚Äôll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat‚Äôs why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it‚Äôs so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you‚Äôll find 1,000 empty acres of land. \\n\\nIt won‚Äôt look like much, but if you stop and look closely, you‚Äôll see a ‚ÄúField of dreams,‚Äù the ground on which America‚Äôs future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor ‚Äúmega site‚Äù. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that‚Äôs just the beginning. \\n\\nIntel‚Äôs CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they‚Äôre waiting for is for you to pass this bill. \\n\\nSo let‚Äôs not wait any longer. Send it to my desk. I‚Äôll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere‚Äôs something happening in America. \\n\\nJust look around and you‚Äôll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products ‚ÄúMade In America.‚Äù The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat‚Äôs what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history‚Äî$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I‚Äôve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who‚Äôs here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, ‚ÄúIt‚Äôs time to bury the label ‚ÄúRust Belt.‚Äù \\n\\nIt‚Äôs time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That‚Äôs why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren‚Äôt enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo‚Äîwe have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let‚Äôs make it in America. \\n\\nEconomists call it ‚Äúincreasing the productive capacity of our economy.‚Äù \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here‚Äôs the plan: \\n\\nFirst ‚Äì cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua‚Äôs mom. \\n\\nImagine what it‚Äôs like to look at your child who needs insulin and have no idea how you‚Äôre going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let‚Äôs cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we‚Äôre at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let‚Äôs close the coverage gap and make those savings permanent. \\n\\nSecond ‚Äì cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet‚Äôs provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America‚Äôs clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you‚Äôll never have to pay at the gas pump again. \\n\\nThird ‚Äì cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn‚Äôt have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn‚Äôt afford child care, to be able to get back to work. \\n\\nMy plan doesn‚Äôt stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI‚Äôm not looking to punish anyone. But let‚Äôs make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat‚Äôs simply not fair. That‚Äôs why I‚Äôve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can‚Äôt get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat‚Äôs why I‚Äôve proposed closing loopholes so the very wealthy don‚Äôt pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that‚Äôs my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let‚Äôs get this done. And while you‚Äôre at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe‚Äôre going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I‚Äôm announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI‚Äôm a capitalist, but capitalism without competition isn‚Äôt capitalism. \\n\\nIt‚Äôs exploitation‚Äîand it drives up prices. \\n\\nWhen corporations don‚Äôt have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I‚Äôm announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe‚Äôll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet‚Äôs pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet‚Äôs increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill‚Äîour First Lady who teaches full-time‚Äîcalls America‚Äôs best-kept secret: community colleges. \\n\\nAnd let‚Äôs pass the PRO Act when a majority of workers want to form a union‚Äîthey shouldn‚Äôt be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven‚Äôt done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you‚Äôre tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we‚Äôve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe‚Äôve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention‚Äîthe CDC‚Äîissued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about ‚Äúliving with COVID-19‚Äù. Tonight ‚Äì I say that we will never just accept living with COVID-19. \\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \\n\\nHere are four common sense steps as we move forward safely.  \\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you‚Äôre vaccinated and boosted you have the highest degree of protection. \\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \\n\\nThe scientists are working hard to get that done and we‚Äôll be ready with plenty of vaccines when they do. \\n\\nWe‚Äôre also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \\n\\nWe‚Äôve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \\n\\nAnd we‚Äôre launching the ‚ÄúTest to Treat‚Äù initiative so people can get tested at a pharmacy, and if they‚Äôre positive, receive antiviral pills on the spot at no cost.  \\n\\nIf you‚Äôre immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe‚Äôre leaving no one behind or ignoring anyone‚Äôs needs as we move forward. \\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.   \\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \\n\\nSecond ‚Äì we must prepare for new variants. Over the past year, we‚Äôve gotten much better at detecting new variants. \\n\\nIf necessary, we‚Äôll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we‚Äôll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won‚Äôt come. But I can promise you we‚Äôll do everything within our power to be ready if it does.  \\n\\nThird ‚Äì we can end the shutdown of schools and businesses. We have the tools we need. \\n\\nIt‚Äôs time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \\n\\nWe‚Äôre doing that here in the federal government. The vast majority of federal workers will once again work in person. \\n\\nOur schools are open. Let‚Äôs keep it that way. Our kids need to be in school. \\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks. \\n\\nOf course, continuing this costs money. \\n\\nI will soon send Congress a request. \\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.     \\n\\nWe‚Äôve sent 475 Million vaccine doses to 112 countries, more than any other nation. \\n\\nAnd we won‚Äôt stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who‚Äôd grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI‚Äôve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who‚Äôll walk the beat, who‚Äôll know the neighborhood, and who can restore trust and safety. \\n\\nSo let‚Äôs not abandon our streets. Or choose between safety and equal justice. \\n\\nLet‚Äôs come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat‚Äôs why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat‚Äôs why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption‚Äîtrusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home‚Äîthey have no serial numbers and can‚Äôt be traced. \\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \\n\\nBan assault weapons and high-capacity magazines. \\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can‚Äôt be sued. \\n\\nThese laws don‚Äôt infringe on the Second Amendment. They save lives. \\n\\nThe most fundamental right in America is the right to vote ‚Äì and to have it counted. And it‚Äôs under assault. \\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we‚Äôve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe‚Äôve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe‚Äôre putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe‚Äôre securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land‚Äîmy forefathers and so many of yours. \\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \\n\\nRevise our laws so businesses have the workers they need and families don‚Äôt wait decades to reunite. \\n\\nIt‚Äôs not only the right thing to do‚Äîit‚Äôs the economically smart thing to do. \\n\\nThat‚Äôs why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \\n\\nLet‚Äôs get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade‚Äîstanding precedent for half a century‚Äîis under attack as never before. \\n\\nIf we want to go forward‚Äînot backward‚Äîwe must protect access to health care. Preserve a woman‚Äôs right to choose. And let‚Äôs continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let‚Äôs finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn‚Äôt true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we‚Äôll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I‚Äôm offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you‚Äôre suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let‚Äôs take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part‚Äîsign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they‚Äôre conducting on our children for profit. \\n\\nIt‚Äôs time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let‚Äôs get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI‚Äôve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers. \\n\\nOne was stationed at bases and breathing in toxic smoke from ‚Äúburn pits‚Äù that incinerated wastes of war‚Äîmedical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world‚Äôs fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don‚Äôt know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I‚Äôm committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath‚Äôs widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath‚Äôs lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn‚Äôt know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle‚Äîwe are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I‚Äôm announcing we‚Äôre expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI‚Äôm also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let‚Äôs end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America‚Äìsecond only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt‚Äôs based on DARPA‚Äîthe Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose‚Äîto drive breakthroughs in cancer, Alzheimer‚Äôs, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans‚Äîtonight , we have gathered in a sacred space‚Äîthe citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity. \\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong‚Äîbecause you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})\n\n\nThe load_single_document function accomplishes the following steps:\n\nExtracts the file extension from the given file path.\nRetrieves the corresponding document loader and its arguments from the previously defined LOADER_MAPPING dictionary.\nCreates an instance of the appropriate document loader.\nLoads the document using the instantiated loader.\nReturns the loaded document.\n\nWe can see that load_single_document returns a document of type langchain.schema.Document. Which according to the documentation consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data).\n\ndef load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Loads all documents from the source documents directory, ignoring specified files\n    \"\"\"\n    all_files = []\n    for ext in LOADER_MAPPING:\n        #Find all the files within source documents which matches the extensions in Loader_Mapping file\n        all_files.extend(\n            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n        )\n    \n    ## Filtering files from all_files if its in ignored_files\n    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n    \n    ## Spinning up resource pool\n    with Pool(processes=os.cpu_count()) as pool:\n        results = []\n        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n            # Load each document from filtered files list using load_single_document function\n            for i, doc in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n                results.append(doc)\n                pbar.update()\n    \n    return results\n\nThe load_single_documents function carries out the following steps: 1. Initializes an empty dictionary called all_files.  2. For each extension in the LOADER_MAPPING dictionary, it searches for all the files with that extension in the source directory and adds them to the all_files list.  3. Creates a new list named filtered_files by removing the files listed in the ignored_files list from the all_files list. 4. Executes a parallel loading operation on all the files in the filtered_files list using the load_single_document function, and appends the results to the results list. 5. Returns the list of loaded documents.\n\nloaded_documents = load_documents(git_dir+'source_documents')\nprint(f\"Length of loaded documents: {len(loaded_documents)}\")\nloaded_documents[0]\n\nLoading new documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 246.69it/s]\n\n\nLength of loaded documents: 1\n\n\n\n\n\nDocument(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we‚Äôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat‚Äôs why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin‚Äôs latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn‚Äôt respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia‚Äôs lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies ‚Äìwe are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia‚Äôs largest banks from the international financial system.  \\n\\nPreventing Russia‚Äôs central bank from defending the Russian Ruble making Putin‚Äôs $630 Billion ‚Äúwar fund‚Äù worthless.   \\n\\nWe are choking off Russia‚Äôs access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights ‚Äì further isolating Russia ‚Äì and adding an additional squeeze ‚Äìon their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia‚Äôs economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies ‚Äì in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we‚Äôve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield ‚Äì he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin‚Äôs war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn‚Äôt have taken something so terrible for people around the world to see what‚Äôs at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It‚Äôs going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat‚Äôs why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people‚Äîand left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs. \\n\\nIn fact‚Äîour economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn‚Äôt worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won‚Äôt be able to compete for the jobs of the 21st Century if we don‚Äôt fix that. \\n\\nThat‚Äôs why it was so important to pass the Bipartisan Infrastructure Law‚Äîthe most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe‚Äôre done talking about infrastructure weeks. \\n\\nWe‚Äôre going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world‚Äîparticularly with China.  \\n\\nAs I‚Äôve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe‚Äôll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we‚Äôll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe‚Äôll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes‚Äîso every child‚Äîand every American‚Äîhas clean water to drink at home and at school, provide affordable high-speed internet for every American‚Äîurban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced. \\n\\nAnd tonight, I‚Äôm announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America ‚Äì we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere‚Äôs been a law on the books for almost a century \\nto make sure taxpayers‚Äô dollars support American jobs and businesses. \\n\\nEvery Administration says they‚Äôll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat‚Äôs why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it‚Äôs so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you‚Äôll find 1,000 empty acres of land. \\n\\nIt won‚Äôt look like much, but if you stop and look closely, you‚Äôll see a ‚ÄúField of dreams,‚Äù the ground on which America‚Äôs future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor ‚Äúmega site‚Äù. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that‚Äôs just the beginning. \\n\\nIntel‚Äôs CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they‚Äôre waiting for is for you to pass this bill. \\n\\nSo let‚Äôs not wait any longer. Send it to my desk. I‚Äôll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere‚Äôs something happening in America. \\n\\nJust look around and you‚Äôll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products ‚ÄúMade In America.‚Äù The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat‚Äôs what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history‚Äî$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I‚Äôve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who‚Äôs here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, ‚ÄúIt‚Äôs time to bury the label ‚ÄúRust Belt.‚Äù \\n\\nIt‚Äôs time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That‚Äôs why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren‚Äôt enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo‚Äîwe have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let‚Äôs make it in America. \\n\\nEconomists call it ‚Äúincreasing the productive capacity of our economy.‚Äù \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here‚Äôs the plan: \\n\\nFirst ‚Äì cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua‚Äôs mom. \\n\\nImagine what it‚Äôs like to look at your child who needs insulin and have no idea how you‚Äôre going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let‚Äôs cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we‚Äôre at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let‚Äôs close the coverage gap and make those savings permanent. \\n\\nSecond ‚Äì cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet‚Äôs provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America‚Äôs clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you‚Äôll never have to pay at the gas pump again. \\n\\nThird ‚Äì cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn‚Äôt have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn‚Äôt afford child care, to be able to get back to work. \\n\\nMy plan doesn‚Äôt stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI‚Äôm not looking to punish anyone. But let‚Äôs make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat‚Äôs simply not fair. That‚Äôs why I‚Äôve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can‚Äôt get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat‚Äôs why I‚Äôve proposed closing loopholes so the very wealthy don‚Äôt pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that‚Äôs my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let‚Äôs get this done. And while you‚Äôre at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe‚Äôre going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I‚Äôm announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI‚Äôm a capitalist, but capitalism without competition isn‚Äôt capitalism. \\n\\nIt‚Äôs exploitation‚Äîand it drives up prices. \\n\\nWhen corporations don‚Äôt have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I‚Äôm announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe‚Äôll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet‚Äôs pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet‚Äôs increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill‚Äîour First Lady who teaches full-time‚Äîcalls America‚Äôs best-kept secret: community colleges. \\n\\nAnd let‚Äôs pass the PRO Act when a majority of workers want to form a union‚Äîthey shouldn‚Äôt be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven‚Äôt done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you‚Äôre tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we‚Äôve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe‚Äôve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention‚Äîthe CDC‚Äîissued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about ‚Äúliving with COVID-19‚Äù. Tonight ‚Äì I say that we will never just accept living with COVID-19. \\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \\n\\nHere are four common sense steps as we move forward safely.  \\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you‚Äôre vaccinated and boosted you have the highest degree of protection. \\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \\n\\nThe scientists are working hard to get that done and we‚Äôll be ready with plenty of vaccines when they do. \\n\\nWe‚Äôre also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \\n\\nWe‚Äôve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \\n\\nAnd we‚Äôre launching the ‚ÄúTest to Treat‚Äù initiative so people can get tested at a pharmacy, and if they‚Äôre positive, receive antiviral pills on the spot at no cost.  \\n\\nIf you‚Äôre immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe‚Äôre leaving no one behind or ignoring anyone‚Äôs needs as we move forward. \\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.   \\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \\n\\nSecond ‚Äì we must prepare for new variants. Over the past year, we‚Äôve gotten much better at detecting new variants. \\n\\nIf necessary, we‚Äôll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we‚Äôll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won‚Äôt come. But I can promise you we‚Äôll do everything within our power to be ready if it does.  \\n\\nThird ‚Äì we can end the shutdown of schools and businesses. We have the tools we need. \\n\\nIt‚Äôs time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \\n\\nWe‚Äôre doing that here in the federal government. The vast majority of federal workers will once again work in person. \\n\\nOur schools are open. Let‚Äôs keep it that way. Our kids need to be in school. \\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks. \\n\\nOf course, continuing this costs money. \\n\\nI will soon send Congress a request. \\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.     \\n\\nWe‚Äôve sent 475 Million vaccine doses to 112 countries, more than any other nation. \\n\\nAnd we won‚Äôt stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who‚Äôd grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI‚Äôve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who‚Äôll walk the beat, who‚Äôll know the neighborhood, and who can restore trust and safety. \\n\\nSo let‚Äôs not abandon our streets. Or choose between safety and equal justice. \\n\\nLet‚Äôs come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat‚Äôs why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat‚Äôs why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption‚Äîtrusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home‚Äîthey have no serial numbers and can‚Äôt be traced. \\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \\n\\nBan assault weapons and high-capacity magazines. \\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can‚Äôt be sued. \\n\\nThese laws don‚Äôt infringe on the Second Amendment. They save lives. \\n\\nThe most fundamental right in America is the right to vote ‚Äì and to have it counted. And it‚Äôs under assault. \\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she‚Äôs been nominated, she‚Äôs received a broad range of support‚Äîfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we‚Äôve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe‚Äôve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe‚Äôre putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe‚Äôre securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land‚Äîmy forefathers and so many of yours. \\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \\n\\nRevise our laws so businesses have the workers they need and families don‚Äôt wait decades to reunite. \\n\\nIt‚Äôs not only the right thing to do‚Äîit‚Äôs the economically smart thing to do. \\n\\nThat‚Äôs why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \\n\\nLet‚Äôs get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade‚Äîstanding precedent for half a century‚Äîis under attack as never before. \\n\\nIf we want to go forward‚Äînot backward‚Äîwe must protect access to health care. Preserve a woman‚Äôs right to choose. And let‚Äôs continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let‚Äôs finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn‚Äôt true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we‚Äôll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I‚Äôm offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you‚Äôre suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let‚Äôs take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part‚Äîsign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they‚Äôre conducting on our children for profit. \\n\\nIt‚Äôs time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let‚Äôs get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI‚Äôve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers. \\n\\nOne was stationed at bases and breathing in toxic smoke from ‚Äúburn pits‚Äù that incinerated wastes of war‚Äîmedical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world‚Äôs fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don‚Äôt know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I‚Äôm committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath‚Äôs widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath‚Äôs lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn‚Äôt know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle‚Äîwe are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I‚Äôm announcing we‚Äôre expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI‚Äôm also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let‚Äôs end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America‚Äìsecond only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt‚Äôs based on DARPA‚Äîthe Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose‚Äîto drive breakthroughs in cancer, Alzheimer‚Äôs, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans‚Äîtonight , we have gathered in a sacred space‚Äîthe citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity. \\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong‚Äîbecause you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})\n\n\nYou can see we have loaded the state_of_the_union.txt file from the privateGPT repo. As this is the only file in that directory the length of loaded documents is one.\n\n\n3.1.2 Splitting the documents into smaller chunks\nNow we have seen how we can load multiple documents of different extensions using the load_documents function. The next step is to look at process_document function which loads and splits large documents into smaller chunks.\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nchunk_size = 500\nchunk_overlap = 50\n\ndef process_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Load documents and split in chunks\n    \"\"\"\n    print(f\"Loading documents from {source_dir}\")\n    documents = load_documents(source_dir, ignored_files)\n    if not documents:\n        print(\"No new documents to load\")\n        exit(0)\n    print(f\"Loaded {len(documents)} new documents from {source_dir}\")\n    ## Load text splitter\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    ## Split text\n    texts = text_splitter.split_documents(documents)\n    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n    return texts\n\nprocessed_documents = process_documents(git_dir+'source_documents')\n\nLoading documents from ../../../../privateGPT/source_documents\n\n\nLoading new documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 315.74it/s]\n\n\nLoaded 1 new documents from ../../../../privateGPT/source_documents\nSplit into 90 chunks of text (max. 500 tokens each)\n\n\n\n\n\nThe process_documents function performs the following steps:\n\nLoads all the documents from the source_dir directory using the load_documents function.\nInitializes an instance of RecursiveCharacterTextSplitter from the langchain.text_splitter module, providing the chunk_size and chunk_overlap parameters. This class is responsible for splitting a list of documents into smaller overlapping chunks. [RecursiveCharacterTextSplitter documentation].\nUses the split_documents method of the RecursiveCharacterTextSplitter instance to split the loaded documents into smaller chunks.\nReturns the resulting list of the smaller document chunks.\n\n\n\n3.1.3 Initializing the embedding model\nNext, we load our embedding module which converts the smaller document chunks from previous steps to embeddings.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nEMBEDDINGS_MODEL_NAME = \"all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n\nprint(\"Testing on a single query.\")\nembedded_vector = embeddings.embed_query(\"What is your name?\")\nprint(f\"Size of embedded vector: {len(embedded_vector)}\")\n\nTesting on a single query.\nSize of embedded vector: 384\n\n\nThe given code snippet carries out the following steps:\n\nImports the HuggingFaceEmbeddings function from the langchain.embeddings module. This function is responsible for loading and encapsulating the SentenceTransformers embeddings, which are used for generating dense vector representations of sentences. You can refer to the HuggingFaceEmbeddings documentation for more details.\nLoads the all-MiniLM-L6-v2 model from the sentence_transformers library. This model is specifically designed to map sentences and paragraphs into a 384-dimensional dense vector space. It is commonly utilized for tasks such as semantic search and similarity analysis.\n\nWe can see that our embedded vector on a sample query returns a 384 dimension vector.\n\n\n3.1.4 Embed smaller text and save it in the vector database\nThe next step involves utilizing the document chunks and the embedding model to store the documents and their corresponding embeddings in a vector database.\n\nfrom chromadb.config import Settings\nfrom langchain.vectorstores import Chroma\n\nPERSIST_DIRECTORY= git_dir+\"db\"\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=PERSIST_DIRECTORY,\n        anonymized_telemetry=False\n)\n## Create the embedding database\ndb = Chroma.from_documents(processed_documents, embeddings, persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\ndb.persist()\n\nUsing embedded DuckDB with persistence: data will be stored in: ../../../../privateGPT/db\n\n\nThe given code snippet performs the following operations:\n\nIt imports the Settings class from the chromadb.config module and the Chroma class from the langchain.vectorstores module.\nIt creates an instance of the Settings class named CHROMA_SETTINGS, providing several configuration parameters:\n\nchroma_db_impl is set to 'duckdb+parquet', specifying the implementation to be used for the Chroma vector database.\npersist_directory is set to the PERSIST_DIRECTORY variable defined earlier, indicating the directory where the vector database will be saved.\nanonymized_telemetry is set to False, indicating whether anonymized telemetry data should be collected.\n\nIt creates a vector database by calling the Chroma.from_documents() method. This method takes the following arguments:\n\nprocessed_documents: The list of processed documents obtained from the previous step.\nembeddings: The embeddings object/model used to generate the document embeddings.\npersist_directory: The directory where the vector database will be persisted, specified by the PERSIST_DIRECTORY variable.\nclient_settings: The settings object (CHROMA_SETTINGS) containing configuration parameters for the vector database.\n\nWe use db.persist() to store the index for future retrieval task\n\n\n## Test the semantic retrieval \ndb.similarity_search(query=\"What is the American Rescue Plan?\", k= 4)\n\n[Document(page_content='The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part‚Äîsign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people‚Äîand left no one behind.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let‚Äôs close the coverage gap and make those savings permanent. \\n\\nSecond ‚Äì cut energy costs for families an average of $500 a year by combatting climate change.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='That‚Äôs why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat‚Äôs why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption‚Äîtrusted messengers breaking the cycle of violence and trauma and giving young people hope.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})]\n\n\n\ndb = None\n\nTo test the retrieval of semantic similarity, we can use the similarity_search function. similarity_search function takes a text query as input and returns the top k=4 document chunks from the vector database."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#question-answer-interface",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#question-answer-interface",
    "title": "privateGPT Walkthrough",
    "section": "3.2 Question & Answer Interface",
    "text": "3.2 Question & Answer Interface\nLet‚Äôs explore the Q&A interface in more detail. The Q&A interface consists of the following steps:\n\nLoad the vector database and prepare it for the retrieval task.\nLoad a pre-trained Large language model from LlamaCpp or GPT4ALL.\nPrompt the user with a query and generate a response using the RetrievalQA pipeline from langchain.chains.\n\n\n\n\nFig.6: Question Answering Pipeline\n\nLet‚Äôs look at these steps one by one.\n\n3.2.1 Load the vector database\nFirst, we import the required libraries.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom chromadb.config import Settings\ngit_dir = \"../../../../privateGPT/\"\nPERSIST_DIRECTORY= git_dir+\"db\"\nEMBEDDINGS_MODEL_NAME = \"all-MiniLM-L6-v2\"\n\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=PERSIST_DIRECTORY,\n        anonymized_telemetry=False\n)\n\nembeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\ndb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\nretriever = db.as_retriever()\n\nUsing embedded DuckDB with persistence: data will be stored in: ../../../../privateGPT/db\n\n\nThe given code snippet carries out the following steps:\n\nLoads the embeddings using the HuggingFaceEmbeddings function, which was previously used to create the embedding store.\nInstantiates a Chroma vector database that was created earlier.\nSets the vector database in retrieval mode.\n\n\n## Testing retriever\nretriever.vectorstore.similarity_search(query = \"What is Amercian rescue plan?\")\n\n[Document(page_content='The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part‚Äîsign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people‚Äîand left no one behind.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='That‚Äôs why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat‚Äôs why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption‚Äîtrusted messengers breaking the cycle of violence and trauma and giving young people hope.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let‚Äôs close the coverage gap and make those savings permanent. \\n\\nSecond ‚Äì cut energy costs for families an average of $500 a year by combatting climate change.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})]\n\n\n\n\n3.2.2 Load a pre-trained Large language model.\n\nfrom langchain.llms import GPT4All\n\nMODEL_PATH = git_dir+\"models/ggml-gpt4all-j-v1.3-groovy.bin\" \nMODEL_N_CTX=1000\n\n# Prepare the LLM\nllm = GPT4All(model=MODEL_PATH, n_ctx=MODEL_N_CTX, backend='gptj', callbacks=None, verbose=False)\n\ngptj_model_load: loading model from '../../../../privateGPT/models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load: n_ctx   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head  = 16\ngptj_model_load: n_layer = 28\ngptj_model_load: n_rot   = 64\ngptj_model_load: f16     = 2\ngptj_model_load: ggml ctx size = 4505.45 MB\ngptj_model_load: memory_size =   896.00 MB, n_mem = 57344\ngptj_model_load: ................................... done\ngptj_model_load: model size =  3609.38 MB / num tensors = 285\n\n\nThe code snippet above create an instance of the GPT4All class named llm, which represents the Language Model (LLM) using the GPT-4All model. The constructor of GPT4All takes the following arguments:  - model: The path to the GPT-4All model file specified by the MODEL_PATH variable.  - n_ctx: The context size or maximum length of input sequences specified by the MODEL_N_CTX variable.  - backend: The backend to use for the LLM. In this case, it is set to ‚Äògptj‚Äô.  - callbacks: The callbacks to be used during the LLM execution. In this case, it is set to None.  - verbose: A boolean flag indicating whether to print verbose output during LLM execution. In this case, it is set to False.\n\n\n3.2.3 Prompt the user with a query and generate a response\n\nfrom langchain.chains import RetrievalQA\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\nquery = \"What is American rescue plan?\"\nres = qa(query)\nanswer, docs = res['result'], res['source_documents']\n\n# Get the answer from the chain\n# Print the result\nprint(\"\\n\\n> Question:\")\nprint(query)\nprint(\"\\n> Answer:\")\nprint(answer)\n\n# Print the relevant sources used for the answer\nfor document in docs:\n    print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n\n\n> Question:\nWhat is American rescue plan?\n\n> Answer:\n The American Rescue Plan is a program that provides funding to schools to hire teachers and help students make up for lost learning due to the COVID-19 pandemic. It also provides economic relief for tens of millions of Americans by helping them put food on their table, keep a roof over their heads, and cut the cost of health insurance. The plan also helps working people by providing breathing room and giving them a little breathing room. It is a program that helps millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums and combat climate change by cutting energy costs for families an average of $500 a year.\n\n> ../../../../privateGPT/source_documents/state_of_the_union.txt:\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \n\nI urge every parent to make sure your school does just that. And we can all play a part‚Äîsign up to be a tutor or a mentor. \n\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.\n\n> ../../../../privateGPT/source_documents/state_of_the_union.txt:\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \n\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \n\nAnd as my Dad used to say, it gave people a little breathing room. \n\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people‚Äîand left no one behind.\n\n> ../../../../privateGPT/source_documents/state_of_the_union.txt:\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let‚Äôs close the coverage gap and make those savings permanent. \n\nSecond ‚Äì cut energy costs for families an average of $500 a year by combatting climate change.\n\n> ../../../../privateGPT/source_documents/state_of_the_union.txt:\nThat‚Äôs why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \n\nThat‚Äôs why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption‚Äîtrusted messengers breaking the cycle of violence and trauma and giving young people hope.\n\n\nFirstly, an instance of the RetrievalQA class named qa is created using the from_chain_type method. The RetrievalQA class is a chain specifically designed for question-answering tasks over an index. Please refer to the documentation for further details. The from_chain_type method takes the following arguments:\n\nllm: The Language Model instance (llm) that was created previously.\nchain_type: A string representing the type of chain to be used. In this case, it is set to \"stuff\". There may be other available chain types specific to the question-answering scenario. Please consult the documentation for more information.\nretriever: An instance of a Chroma database used to retrieve relevant documents for the given query.\nreturn_source_documents: A boolean flag indicating whether to return the source documents along with the answer. In this case, it is set to True.\n\nNext, the qa instance is used to process a query. The Language Model (LLM) within the qa instance generates a response that includes the query, the answer, and the source documents used as context for generating the answer.\nFinally, the answer and source documents are printed out for display."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#conclusion",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#conclusion",
    "title": "privateGPT Walkthrough",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nIn this blog post, we explored privateGPT, its implementation, and the code walkthrough for its ingestion pipeline and q&A interface. I hope this blog post has been valuable in understanding privateGPT and its implementation. I recommend my readers to try privateGPT on your own knowledge base.\nI hope you enjoyed reading it. If there is any feedback on the code or just the blog post, feel free to comment below or reach out on LinkedIn."
  },
  {
    "objectID": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html",
    "href": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html",
    "title": "ML Interview Prepration Guide (Draft)",
    "section": "",
    "text": "A collection of resources while preparing for MLE interviews at Meta or other big tech companies.\nNot long ago, I transitioned from a Senior ML Scientist role at Microsoft to a Machine Learning Engineer position at Meta, and the journey was anything but quick. The preparation process was extensive, especially since it was my first experience with LeetCode-style coding interviews and ML system design interviews. While there are many resources available for preparation, I‚Äôll be sharing the ones that helped me navigate and succeed in this challenging process.\nI will try to cover the following -"
  },
  {
    "objectID": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-coding-problems-in-interviews",
    "href": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-coding-problems-in-interviews",
    "title": "ML Interview Prepration Guide (Draft)",
    "section": "2.1 A Structured Approach to Solving Coding Problems in Interviews",
    "text": "2.1 A Structured Approach to Solving Coding Problems in Interviews\nWhen tackling a coding problem, following this structured approach can be very helpful:\n\nAsk Clarifying Questions(~3mins): When the problem is presented, read it aloud to ensure you fully understand the requirements before jumping to a solution. Ask follow-up questions to clarify any ambiguities. This might involve discussing test cases, considering edge cases, and understanding the expected input range or type. For example, think about how the solution should handle null inputs or extreme values. Ideal state is to get an alignment with your interviewer by writing out some test cases and expected output for the same.\nPlan Your Approach(~5 mins): Outline your solution strategy and explain it to your interviewer while typing it out in the shared text window. Break down the problem into smaller parts if possible, and decide on the most appropriate algorithm or data structure and discuss any trade-offs you are making and write down potential time and space complexity of the solution you are proposing. Once your interview agree with your approach and then ask permission to code it out.\nWrite the code(~5 mins): Implement your solution, keeping your code clean and well-organized. As you code, ensure that you handle edge cases. Make sure to name your functions, classes and variables appropriately so anybody reading your code can follow.\nPseudo run your solution(~2 mins): Manually run your code against various test cases while explaining it to your interviewer, including both typical and edge cases, to ensure it behaves as expected. This will help you find potential bugs and an opportunity to correct them before your interviewer points it out.\nClose(~2 mins): Explain time and space complexity of the solution and answer any followup questions your interviewer might have.\n\nBy following these steps, you can effectively navigate coding problems and demonstrate a clear, methodical problem-solving approach.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nKeep your introduction breif (~30 seconds) to have more time for solving the problem. For example - ‚ÄúHey I am [Your Name], I currently work as [Title] at [Employer Name]. I have been working here from past [N] years. I‚Äôm now seeking new opportunities, which brings me here today.‚Äù\nIf you‚Äôre running out of time, it‚Äôs acceptable to manually walk through one or two test cases with your interviewer. You can then suggest moving on to the next question to ensure you cover everything within the allotted time.\nIts okay to ask for help from interviewer if you are stuck on a problem\n\n\n\n\nAs you can see from above, coding rounds are really fast paced and you need to be well prepared to get through it. That brings us to prepration."
  },
  {
    "objectID": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-coding-interviews",
    "href": "posts/2024-08-24-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-coding-interviews",
    "title": "ML Interview Prepration Guide (Draft)",
    "section": "2.2 How to prepare for coding interviews",
    "text": "2.2 How to prepare for coding interviews\nHere is a simple guide on how to prepare -\n\nPurchase a leetcode subscription - This website is the only paid resource you need to prepare for coding interviews.\nGetting started with Leetcode learn - If you are like me who doesn‚Äôt come from a CS degree then going through Leetcode learn cards is a good starting point. Here is the structure I followed-\n\n\nArray\nLinked List\nStack & Queue\nArray & Strings\nBinary Tree\nBinary Search\nBinary Search Tree\nHeap\nGraph\nSorting\nDynamic Programming\n\n\nFollowing Neetcode.io Roadmap- This roadmap contains 75 leetcode questions which will familiarize you with common coding patterns useful in coding interviews\nSolving company tagged questions - On leetcode.com you can filter for the company you are interviewing and see top tagged question for the same. I would recommend solving top 100 tagged questions based on frequency which are asked in last six months.\nPractice - Once you are done with the above then you can practice timed assessment on leetcode.com for your specific employer or generic ones if not listed. If you want more realistic practice then you can buy some mock interviews on interviewing.io where an engineer from top tech company will take your mock and provide feedback on your performance.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nThe more you practice in conditions similar to the actual interview‚Äîsuch as using a text editor and working within timed constraints‚Äîthe better you will perform\nWhen practicing, attempt to solve the problem on your own for 20-30 minutes before consulting the solution.\nIf you find a problem challenging to understand, search for [LeetCode Problem #XYZ] on YouTube; you‚Äôll likely find a video with a clearer explanation.\nKeeping an Excel sheet to track the problems you‚Äôve solved during practice, along with notes such as ‚Äòneeds revision‚Äô, time/space complexity and a brief summary of the solution, can be very helpful for review later. You can use this Conding Tracking Sheet Tab as a template."
  },
  {
    "objectID": "talks/2017-09-13-introduction-to-keras/index.html",
    "href": "talks/2017-09-13-introduction-to-keras/index.html",
    "title": "Introduction to Keras",
    "section": "",
    "text": "Presentation and codes on ‚ÄúIntroduction of Keras‚Äù. This was material presented by me in Analyze This! meetup on 13th September, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting a real time object detection application using deep learning and openCV."
  },
  {
    "objectID": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "href": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "title": "Deep dive in Hierarchical clustering",
    "section": "",
    "text": "Presentation and codes on ‚ÄúDeep dive in Hierarchical Clustering‚Äù. This was material presented by me in Analyze This! meetup on 11th October, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2017-11-08-Transfer-Learning/index.html",
    "href": "talks/2017-11-08-Transfer-Learning/index.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Presentation and codes on ‚ÄúTransfer learning‚Äù. This was material presented by me in Analyze This! meetup on 8th November, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2018-10-18-msba_devops/index.html",
    "href": "talks/2018-10-18-msba_devops/index.html",
    "title": "DevOps for Data scientist",
    "section": "",
    "text": "Conducted a DevOps for data scientist workshop for 2019 batch of Masters in Business Analytics student in Carlson school of management. The workshop was attended by 100+ students.\nLink to workshop content\nWorkshop Content\n\nWhat is DevOps? Why is it needed?\nModel building using cookbooks in Python\nVersion control using GIT\nDeploying Model as an API using Flask and microservices framework\nPackaging applications using Docker\nScaling and Deploying applications in Google cloud using Docker and Kubernetes"
  },
  {
    "objectID": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "href": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "title": "Model building, Explainability, and Deployment",
    "section": "",
    "text": "Presentation and codes on ‚ÄúModel building, Explainability, and Deployment‚Äù. This was material presented by me in Social Data science meetup on 28th July, 2018\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Sharing our insights from deploying OrcaHello - an open source, AI-assisted 24x7 hydrophone monitoring & Southern Resident Killer Whale alert system in Puget Sound.\n\nYoutube Link - (1:10:00)\nAI For Orcas Webiste"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML in the wild\n\n\n0 min\n\n\n\nDeep Learning\n\n\nAudio\n\n\nVision\n\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevOps for Data scientist\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nOct 18, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nCarlson School of Management, University of Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel building, Explainability, and Deployment\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nVeritas Technologies LLC, 2815 Cleveland Ave N Roseville, MN 55113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning\n\n\n0 min\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nNov 8, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep dive in Hierarchical clustering\n\n\n0 min\n\n\n\nClustering\n\n\n\n\n\n\n\nOct 11, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Keras\n\n\n0 min\n\n\n\nKeras\n\n\n\n\n\n\n\nSep 3, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]