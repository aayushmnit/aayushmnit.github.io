[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm an experienced Data Scientist with specialized skills in machine learning-based solutions. I enjoy staying on top of cutting-edge data technologies, including big data platforms, deep learning, optimization methods, and business analytics. My current work involves building data-driven products to enable smarter recommendations for Microsoft Partners, M365 service administrators and end-users to ensure the best usage of M365 services. Before that, I have experience working in various verticals like agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing business model."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion\n\n\n4 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Putting everything together\n\n\n3 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Looking under the hood\n\n\n9 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion using ü§ó Hugging Face - Introduction\n\n\n2 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel calibration for classification task using Python\n\n\n4 min\n\n\n\nModel Calibration\n\n\nMachine Learning\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixing art into the science of model explainability\n\n\n9 min\n\n\n\nExplainability\n\n\nMachine Learning\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal inference with Synthetic Control using Python and SparseSC\n\n\n7 min\n\n\n\nCausal Inference\n\n\nSynthetic Control\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding similar images using Deep learning and Locality Sensitive Hashing\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\nVideo\n\n\n\n\n\n\n\nFeb 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiLayer Perceptron using Fastai and Pytorch\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nPytorch\n\n\nVision\n\n\n\n\n\n\n\nJan 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Disease detection by Tranfer learning using FastAI V1 library\n\n\n0 min\n\n\n\nDeep Learning\n\n\nFastAI\n\n\nVision\n\n\n\n\n\n\n\nOct 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer perceptron using Tensorflow\n\n\n0 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Neural Network from scratch\n\n\n14 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\nJun 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving business usecases by recommender system using lightFM\n\n\n30 min\n\n\n\nRecommender System\n\n\nMachine Learning\n\n\nBusiness\n\n\n\n\n\n\n\nApr 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite launch\n\n\n0 min\n\n\n\nlaunch\n\n\nannouncement\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffEdit implementation using ü§ó Hugging Face\n\n\n5 min\n\n\n\nStable Diffusion\n\n\n\n\n\n\n\nNov 17, 1999\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-02-19-launch/index.html",
    "href": "posts/2018-02-19-launch/index.html",
    "title": "Website launch",
    "section": "",
    "text": "Finally got time to build my own website on Github. This is my first post and I would like to thank Academic Pages which provided with a wonderful repository to help me get started with building this website.\nI am intending to use this website to publish blogs, workshop and any material which would be releveant in Data science field.\nWill see you guys in next post.\n-Aayush"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "",
    "text": "In this post, I am going to write about Recommender systems, how they are used in many e-commerce websites. The post will also cover about building simple recommender system models using Matrix Factorization algorithm using lightFM package and my recommender system cookbook. The post will focus on business use cases and simple implementations. The post only cover basic intuition around algorithms and will provide links to resources if you want to understand the math behind the algorithm."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Motivation",
    "text": "Motivation\nI am an avid reader and a believer in open source education and continuously expand my knowledge around data science & computer science using online courses, blogs, Github repositories and participating in data science competitions. While searching for quality content on the internet, I have come across various learning links which either focus on the implementation of the algorithm using specific data/modeling technique in ABC language or focus on business impact/results using the broad concept of a family of algorithms(like classification, forecasting, recommender systems etc.) but don‚Äôt go into details of how to do it. So the idea is to write some blogs which can combine both business use cases with codes & algorithmic intuition to provide a holistic view of how data science is used in business scenarios. \nAs the world is becoming more digital, we are already getting used to a lot of personalized experience and the algorithm which help us achieve this falls in the family of recommender systems. Almost every web-based platform is using some recommender system to provide customized content. Following are the companies I admire the most."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "What is personalization?",
    "text": "What is personalization?\nPersonalization is a technique of dynamically tailoring your content based on needs of each user. Simple examples of personalization could be movie recommendation on Netflix, personalized email targeting/re-targeting by e-commerce platforms, item recommendation on Amazon, etc. Personalization helps us achieve these four Rs - - Recognize: Know customer‚Äôs and prospects‚Äô profiles, including demographics, geography, and expressed and shared interests. - Remember: Recall customers‚Äô history, primarily how they act as expressed by what they browse and buy - Reach: Deliver the right promotion, content, recommendation for a customer based on actions, preferences, and interests - Relevance: Deliver personalization within the context of the digital experience based on who customers are, where they are located and what time of year it is"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Why personalization?",
    "text": "Why personalization?\nPersonalization has a lot of benefits for both users and companies. For users, it makes their life easy as they only get to see more relevant stuff to them (unless it‚Äôs an advertisement, even they are personalized). For business benefits are countless but here are few which I would like to mention - - Enhance customer experience: Personalization reduces the clutter and enhances the customer experience by showing relevant content - Cross-sell/ Up-sell opportunities: Relevant product offerings based on customer preferences can lead to increasing products visibility and eventually selling more products - Increased basket size: Personalized experience and targeting ultimately leads to increased basket size and frequent purchases - Increased customer loyalty: In the digital world, customer retention/loyalty is the most prominent problem faced by many companies as finding a replacement for a particular service is quite easy. According to a Forbes article, Forty-four percent of consumers say they will likely repeat after a personalized experience"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Introduction to Matrix factorization",
    "text": "Introduction to Matrix factorization\nMatrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc.  \n Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it‚Äôs more female-driven the value is typically low.  \nFor more information on matrix factorization and factorization machines you can read these articles -  Matrix Factorization: A Simple Tutorial and Implementation in Python  Introductory Guide ‚Äì Factorization Machines & their application on huge datasets (with codes in Python)"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "HandOn: Building recommender system using LightFM package in Python",
    "text": "HandOn: Building recommender system using LightFM package in Python\nIn the hands-on section, we will be building recommender system for different scenarios which we typically see in many companies using LightFM package and MovieLens data. We are using small size data which contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users\n\nData\nLet‚Äôs start by importing data, recommender system cookbook and preprocessing cookbook files for this hands-on section. I have written these reusable generic cookbook codes to increase productivity and write clean/modular codes; you will see we can build a recommender system using 10-15 lines of code by using these cookbooks(do more with less!).\n# Importing Libraries and cookbooks\nfrom recsys import * ## recommender system cookbook\nfrom generic_preprocessing import * ## pre-processing code\nfrom IPython.display import HTML ## Setting display options for Ipython Notebook\n# Importing rating data and having a look\nratings = pd.read_csv('./ml-latest-small/ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\n\n\nmovieId\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n0\n\n\n1\n\n\n31\n\n\n2.5\n\n\n1260759144\n\n\n\n\n1\n\n\n1\n\n\n1029\n\n\n3.0\n\n\n1260759179\n\n\n\n\n2\n\n\n1\n\n\n1061\n\n\n3.0\n\n\n1260759182\n\n\n\n\n3\n\n\n1\n\n\n1129\n\n\n2.0\n\n\n1260759185\n\n\n\n\n4\n\n\n1\n\n\n1172\n\n\n4.0\n\n\n1260759205\n\n\n\n\n\n\nAs we can see rating data contain user id, movie id and a rating between 0.5 to 5 with a timestamp representing when the rating was given.\n# Importing movie data and having a look at first five columns\nmovies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\n\n\ntitle\n\n\ngenres\n\n\n\n\n\n\n0\n\n\n1\n\n\nToy Story (1995)\n\n\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n1\n\n\n2\n\n\nJumanji (1995)\n\n\nAdventure|Children|Fantasy\n\n\n\n\n2\n\n\n3\n\n\nGrumpier Old Men (1995)\n\n\nComedy|Romance\n\n\n\n\n3\n\n\n4\n\n\nWaiting to Exhale (1995)\n\n\nComedy|Drama|Romance\n\n\n\n\n4\n\n\n5\n\n\nFather of the Bride Part II (1995)\n\n\nComedy\n\n\n\n\n\n\nMovie data consist of movie id, their title, and genre they belong.\n\n\nPreprocessing\nAs I mentioned before, to create a recommender system we need to start by creating an interaction matrix. For this task, we will use the create_interaction_matrix function from the recsys cookbook. This function requires you to input a pandas dataframe and necessary information like column name for user id, item id, and rating. It also takes an additional parameter threshold if norm=True which means any rating above the mentioned threshold is considered a positive rating. In our case, we don‚Äôt have to normalize our data, but in cases of retail data any purchase of a particular type of item can be considered a positive rating, quantity doesn‚Äôt matter.\n# Creating interaction matrix using rating data\ninteractions = create_interaction_matrix(df = ratings,\n                                         user_col = 'userId',\n                                         item_col = 'movieId',\n                                         rating_col = 'rating')\ninteractions.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n‚Ä¶\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n4\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n5\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n‚Ä¶\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n5 rows √ó 9066 columns\n\n\nAs we can see the data is created in an interaction format where rows represent each user and columns represent each movie id with ratings as values.  We will also create user and item dictionaries to later convert user_id to user_name or movie_id to movie_name by using create_user_dict and create_item dict function.\n# Create User Dict\nuser_dict = create_user_dict(interactions=interactions)\n# Create Item dict\nmovies_dict = create_item_dict(df = movies,\n                               id_col = 'movieId',\n                               name_col = 'title')\n\n\nBuilding Matrix Factorization model\nTo build a matrix factorization model, we will use the runMF function which will take following input -\n- interaction matrix: Interaction matrix created in the previous section - n_components: Number of embedding generated for each user and item - loss: We need to define a loss function, in this case, we are using warp loss because we mostly care about the ranking of data, i.e, which items should we show first - epoch: Number of times to run - n_jobs: Number of cores to use in parallel processing\nmf_model = runMF(interactions = interactions,\n                 n_components = 30,\n                 loss = 'warp',\n                 epoch = 30,\n                 n_jobs = 4)\nNow we have built our matrix factorization model we can now do some interesting things. There are various use cases which can be solved by using this model for a web platform let‚Äôs look into them.\n\n\nUsecase 1: Item recommendation to a user\nIn this use case, we want to show a user, items he might be interested in buying/viewing based on his/her interactions done in the past. Typical industry examples for this are like ‚ÄúDeals recommended for you‚Äù on Amazon or ‚ÄúTop pics for a user‚Äù on Netflix or personalized email campaigns. \nWe can use the sample_recommendation_user function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, user_id and the number of items as input and return the list of item id‚Äôs a user may be interested in interacting.\n## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = mf_model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = movies_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)\nKnown Likes:\n1- The Hunger Games: Catching Fire (2013)\n2- Gravity (2013)\n3- Dark Knight Rises, The (2012)\n4- The Hunger Games (2012)\n5- Town, The (2010)\n6- Exit Through the Gift Shop (2010)\n7- Bank Job, The (2008)\n8- Departed, The (2006)\n9- Bourne Identity, The (1988)\n10- Step Into Liquid (2002)\n11- SLC Punk! (1998)\n12- Last of the Mohicans, The (1992)\n13- Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)\n14- Robin Hood: Prince of Thieves (1991)\n15- Citizen Kane (1941)\n16- Trainspotting (1996)\n17- Pulp Fiction (1994)\n18- Usual Suspects, The (1995)\n\n Recommended Items:\n1- Dark Knight, The (2008)\n2- Inception (2010)\n3- Iron Man (2008)\n4- Shutter Island (2010)\n5- Fight Club (1999)\n6- Avatar (2009)\n7- Forrest Gump (1994)\n8- District 9 (2009)\n9- WALL¬∑E (2008)\n10- Matrix, The (1999)\nprint(rec_list)\n[593L, 260L, 110L, 480L, 47L, 527L, 344L, 858L, 231L, 780L]\nAs we can see in this case user is interested in ‚ÄúDark Knight Rises(2012)‚Äù so the first recommendation is ‚ÄúThe Dark Knight(2008)‚Äù. This user also seems to have a strong liking towards movies in drama, sci-fi and thriller genre and there are many movies recommended in the same genre like Dark Knight(Drama/Crime), Inception(Sci-Fi, Thriller), Iron Man(Sci-FI thriller), Shutter Island(Drame/Thriller), Fight club(drama), Avatar(Sci-fi), Forrest Gump(Drama), District 9(Thriller), Wall-E(Sci-fi), The Matrix(Sci-Fi) \nSimilar models can also be used for building sections like ‚ÄúBased on your recent browsing history‚Äù recommendations by just changing the rating matrix only to contain interaction which is recent and based on browsing history visits on specific items.\n\n\nUsecase 2: User recommendation to a item\nIn this use case, we will discuss how we can recommend a list of users specific to a particular item. Example of such cases is when you are running a promotion on an item and want to run an e-mail campaign around this promotional item to only 10,000 users who might be interested in this item.\n\nWe can use the sample_recommendation_item function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, item_id and the number of users as input and return the list of user id‚Äôs who are more likely be interested in the item.\n## Calling 15 user recommendation for item id 1\nsample_recommendation_item(model = mf_model,\n                           interactions = interactions,\n                           item_id = 1,\n                           user_dict = user_dict,\n                           item_dict = movies_dict,\n                           number_of_user = 15)\n[116, 410, 449, 657, 448, 633, 172, 109, 513, 44, 498, 459, 317, 415, 495]\nAs you can see function return a list of userID who might be interested in item id 1. Another example why you might need such model is when there is an old inventory sitting in your warehouse which needs to clear up otherwise you might have to write it off, and you want to clear it by giving some discount to users who might be interested in buying.\n\n\nUsecase 3: Item recommendation to items\nIn this use case, we will discuss how we can recommend a list of items specific to a particular item. This kind of models will help you to find similar/related items or items which can be bundled together. Typical industry use case for such models are in cross-selling and up-selling opportunities on product page like ‚ÄúProducts related to this item‚Äù, ‚ÄúFrequently bought together‚Äù, ‚ÄúCustomers who bought this also bought this‚Äù and ‚ÄúCustomers who viewed this item also viewed‚Äù.  ‚ÄúCustomers who bought this also bought this‚Äù and ‚ÄúCustomers who viewed this item also viewed‚Äù can also be solved through market basket analysis.\n\nTo achieve this use case, we will create a cosine distance matrix using item embeddings generated by matrix factorization model. This will help us calculate similarity b/w items, and then we can recommend top N similar item to an item of interest. First step is to create a item-item distance matrix using the create_item_emdedding_distance_matrix function. This function takes matrix factorization models and interaction matrix as input and returns an item_embedding_distance_matrix.\n## Creating item-item distance matrix\nitem_item_dist = create_item_emdedding_distance_matrix(model = mf_model,\n                                                       interactions = interactions)\n## Checking item embedding distance matrix\nitem_item_dist.head()\n\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n‚Ä¶\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nmovieId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1.000000\n\n\n0.760719\n\n\n0.491280\n\n\n0.427250\n\n\n0.484597\n\n\n0.740024\n\n\n0.486644\n\n\n0.094009\n\n\n-0.083986\n\n\n0.567389\n\n\n‚Ä¶\n\n\n-0.732112\n\n\n-0.297997\n\n\n-0.451733\n\n\n-0.767141\n\n\n-0.501647\n\n\n-0.270280\n\n\n-0.455277\n\n\n-0.292823\n\n\n-0.337935\n\n\n-0.636147\n\n\n\n\n2\n\n\n0.760719\n\n\n1.000000\n\n\n0.446414\n\n\n0.504502\n\n\n0.525171\n\n\n0.572113\n\n\n0.364393\n\n\n0.290633\n\n\n0.231926\n\n\n0.653033\n\n\n‚Ä¶\n\n\n-0.748452\n\n\n-0.307634\n\n\n-0.165400\n\n\n-0.526614\n\n\n-0.146751\n\n\n-0.156305\n\n\n-0.223818\n\n\n-0.138412\n\n\n-0.209538\n\n\n-0.733489\n\n\n\n\n3\n\n\n0.491280\n\n\n0.446414\n\n\n1.000000\n\n\n0.627473\n\n\n0.769991\n\n\n0.544175\n\n\n0.632008\n\n\n0.336824\n\n\n0.392284\n\n\n0.510592\n\n\n‚Ä¶\n\n\n-0.331028\n\n\n-0.264556\n\n\n-0.308592\n\n\n-0.285085\n\n\n-0.046424\n\n\n-0.165821\n\n\n-0.183842\n\n\n-0.143613\n\n\n-0.156418\n\n\n-0.378811\n\n\n\n\n4\n\n\n0.427250\n\n\n0.504502\n\n\n0.627473\n\n\n1.000000\n\n\n0.582582\n\n\n0.543208\n\n\n0.602390\n\n\n0.655708\n\n\n0.527346\n\n\n0.471166\n\n\n‚Ä¶\n\n\n-0.380431\n\n\n-0.163091\n\n\n-0.232833\n\n\n-0.334746\n\n\n-0.052832\n\n\n-0.266185\n\n\n-0.158415\n\n\n-0.211618\n\n\n-0.232351\n\n\n-0.469629\n\n\n\n\n5\n\n\n0.484597\n\n\n0.525171\n\n\n0.769991\n\n\n0.582582\n\n\n1.000000\n\n\n0.354141\n\n\n0.639958\n\n\n0.396447\n\n\n0.432026\n\n\n0.385051\n\n\n‚Ä¶\n\n\n-0.273074\n\n\n-0.280585\n\n\n-0.306195\n\n\n-0.265243\n\n\n0.012961\n\n\n-0.225142\n\n\n-0.317043\n\n\n-0.136875\n\n\n-0.122382\n\n\n-0.312858\n\n\n\n\n\n\n5 rows √ó 9066 columns\n\n\nAs we can see the matrix have movies as both row and columns and the value represents the cosine distance between them. Next step is to use item_item_recommendation function to get top N items with respect to an item_id. This function takes item embedding distance matrix, item_id, item_dictionary and number of items to be recommended as input and return similar item list as output.\n## Calling 10 recommended items for item id \nrec_list = item_item_recommendation(item_emdedding_distance_matrix = item_item_dist,\n                                    item_id = 5378,\n                                    item_dict = movies_dict,\n                                    n_items = 10)\nItem of interest :Star Wars: Episode II - Attack of the Clones (2002)\nItem similar to the above item:\n1- Star Wars: Episode III - Revenge of the Sith (2005)\n2- Lord of the Rings: The Two Towers, The (2002)\n3- Lord of the Rings: The Fellowship of the Ring, The (2001)\n4- Lord of the Rings: The Return of the King, The (2003)\n5- Matrix Reloaded, The (2003)\n6- Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n7- Gladiator (2000)\n8- Spider-Man (2002)\n9- Minority Report (2002)\n10- Mission: Impossible II (2000)\nAs we can see for ‚ÄúStar Wars: Episode II - Attack of the Clones (2002)‚Äù movie we are getting it‚Äôs next released movies which is ‚ÄúStar Wars: Episode III - Revenge of the Sith (2005)‚Äù as the first recommendation."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Summary",
    "text": "Summary\nLike any other blog, this method isn‚Äôt perfect for every application, but the same ideas can work if we use it effectively. There is a lot of advancements in recommender systems with the advent of Deep learning. While there is room for improvement, I am pleased with how it has been working for me so far. I might write about deep learning based recommender systems later sometime.\nIn the meantime, I hope you enjoyed reading, and feel free to use my code to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "title": "Building Neural Network from scratch",
    "section": "",
    "text": "In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let‚Äôs start with the very basics of a neural network which is Multilayer Perceptron. You can find the same blog in notebook version here."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "title": "Building Neural Network from scratch",
    "section": "What is a neural network?",
    "text": "What is a neural network?\nA neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it‚Äôs the same with a neural network."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "title": "Building Neural Network from scratch",
    "section": "What is a Multi layer perceptron?",
    "text": "What is a Multi layer perceptron?\nMulti-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let‚Äôs start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.   \n\nFig 1: Perceptron image\n\n\n\nImage credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/\n\nNow, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology.  Dense layer,   Note that bias term is now a vector and W is a weight matrix  \n\nFig: Single dense layer perceptron network\n\n\n\nImage credit=http://www.texample.net/tikz/examples/neural-network/\n\nNow we understand dense layer let‚Äôs add a bunch of them, and that network becomes a multi-layer perceptron network.\n\n\nFig: Multi layer perceptron network\n\n\n\nImage credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2s\n\nIf you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero.  ReLU function   Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a feed-forward loop and backpropagation loop. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don‚Äôt understand the concept of gradient weight updates and SGD, I recommend you to watch week 1 of Machine learning by Andrew NG lectures.\nSo, to summarize a neural network needs few building blocks\n\nDense layer - a fully-connected layer, \nReLU layer (or any other activation function to introduce non-linearity)\nLoss function - (crossentropy in case of multi-class classification problem)\nBackprop algorithm - a stochastic gradient descent with backpropageted gradients\n\nLet‚Äôs approach them one at a time."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "title": "Building Neural Network from scratch",
    "section": "Coding Starts here:",
    "text": "Coding Starts here:\nLet‚Äôs start by importing some libraires required for creating our neural network.\nfrom __future__ import print_function\nimport numpy as np ## For numerical python\nnp.random.seed(42)\nEvery layer will have a forward pass and backpass implementation. Let‚Äôs create a main class layer which can do a forward pass .forward() and Backward pass .backward().\nclass Layer:\n    \n    #A building block. Each layer is capable of performing two things:\n\n    #- Process input to get output:           output = layer.forward(input)\n    \n    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n    \n    #Some layers also have learnable parameters which they update during layer.backward.\n    \n    def __init__(self):\n        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n        # A dummy layer does nothing\n        pass\n    \n    def forward(self, input):\n        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        \n        # A dummy layer just returns whatever it gets as input.\n        return input\n\n    def backward(self, input, grad_output):\n        # Performs a backpropagation step through the layer, with respect to the given input.\n        \n        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n        \n        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n        \n        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n        \n        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n        \n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n        num_units = input.shape[1]\n        \n        d_layer_d_input = np.eye(num_units)\n        \n        return np.dot(grad_output, d_layer_d_input) # chain rule\n\nNonlinearity ReLU layer\nThis is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\nclass ReLU(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        relu_grad = input > 0\n        return grad_output*relu_grad \n\n\nDense layer\nNow let‚Äôs build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\nA dense layer applies affine transformation. In a vectorized form, it can be described as: \nWhere * X is an object-feature matrix of shape [batch_size, num_features], * W is a weight matrix [num_features, num_outputs] * and b is a vector of num_outputs biases.\nBoth W and b are initialized during layer creation and updated each time backward is called. Note that we are using Xavier initialization which is a trick to train our model to converge faster read more. Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        # A dense layer is a layer which performs a learned affine transformation:\n        # f(x) = <W*x> + b\n        \n        self.learning_rate = learning_rate\n        self.weights = np.random.normal(loc=0.0, \n                                        scale = np.sqrt(2/(input_units+output_units)), \n                                        size = (input_units,output_units))\n        self.biases = np.zeros(output_units)\n        \n    def forward(self,input):\n        # Perform an affine transformation:\n        # f(x) = <W*x> + b\n        \n        # input shape: [batch, input_units]\n        # output shape: [batch, output units]\n        \n        return np.dot(input,self.weights) + self.biases\n    \n    def backward(self,input,grad_output):\n        # compute d f / d x = d f / d dense * d dense / d x\n        # where d dense/ d x = weights transposed\n        grad_input = np.dot(grad_output, self.weights.T)\n        \n        # compute gradient w.r.t. weights and biases\n        grad_weights = np.dot(input.T, grad_output)\n        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n        \n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        \n        # Here we perform a stochastic gradient descent step. \n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n        \n        return grad_input\n\n\nThe loss function\nSince we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\nIf we write down the expression for crossentropy as a function of softmax logits (a), you‚Äôll see: \n  If we take a closer look, we‚Äôll see that it can be rewritten as: \n  It‚Äôs called Log-softmax and it‚Äôs better than naive log(softmax(a)) in all aspects: * Better numerical stability * Easier to get derivative right * Marginally faster to compute\nSo why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\ndef softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n    \n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n    \n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n    \n    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- ones_for_answers + softmax) / logits.shape[0]\n\n\nFull network\nNow let‚Äôs combine what we‚Äôve just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let‚Äôs import it!.\nimport keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) / 255.\n    X_test = X_test.astype(float) / 255.\n\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n\n## Let's look at some example\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n\nWe‚Äôll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\nnetwork = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\ndef forward(network, X):\n    # Compute activations of all network layers by applying them sequentially.\n    # Return a list of activations for each layer. \n    \n    activations = []\n    input = X\n\n    # Looping through each layer\n    for l in network:\n        activations.append(l.forward(input))\n        # Updating input to last layer output\n        input = activations[-1]\n    \n    assert len(activations) == len(network)\n    return activations\n\ndef predict(network,X):\n    # Compute network predictions. Returning indices of largest Logit probability\n\n    logits = forward(network,X)[-1]\n    return logits.argmax(axis=-1)\n\ndef train(network,X,y):\n    # Train our network on a given batch of X and y.\n    # We first need to run forward to get all layer activations.\n    # Then we can run layer.backward going from last to first layer.\n    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n    \n    \n    # Get the layer activations\n    layer_activations = forward(network,X)\n    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n    logits = layer_activations[-1]\n    \n    # Compute the loss and the initial gradient\n    loss = softmax_crossentropy_with_logits(logits,y)\n    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n    \n    # Propagate gradients through the network\n    # Reverse propogation as this is backprop\n    for layer_index in range(len(network))[::-1]:\n        layer = network[layer_index]\n        \n        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n        \n    return np.mean(loss)\n\n\nTraining loop\nWe split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent.\nfrom tqdm import trange\ndef iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.random.permutation(len(inputs))\n    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\nfrom IPython.display import clear_output\ntrain_log = []\nval_log = []\nfor epoch in range(25):\n\n    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n        train(network,x_batch,y_batch)\n    \n    train_log.append(np.mean(predict(network,X_train)==y_train))\n    val_log.append(np.mean(predict(network,X_val)==y_val))\n    \n    clear_output()\n    print(\"Epoch\",epoch)\n    print(\"Train accuracy:\",train_log[-1])\n    print(\"Val accuracy:\",val_log[-1])\n    plt.plot(train_log,label='train accuracy')\n    plt.plot(val_log,label='val accuracy')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n    \nEpoch 24\nTrain accuracy: 1.0\nVal accuracy: 0.9809\n\nAs we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
  },
  {
    "objectID": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "href": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "title": "Multi-Layer perceptron using Tensorflow",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "href": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "title": "Leaf Disease detection by Tranfer learning using FastAI V1 library",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "href": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "title": "MultiLayer Perceptron using Fastai and Pytorch",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "href": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "title": "Real-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "href": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "title": "Finding similar images using Deep learning and Locality Sensitive Hashing",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "",
    "text": "Understanding Synthetic Control and using Microsoft‚Äôs SparceSC package to run synthetic control on larger datasets."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "What is Synthetic Control Method?",
    "text": "What is Synthetic Control Method?\nI will try to keep this part short and focus more on why Data scientists should care about such methods and how to use them on larger datasets based on practical experience using SparseSC package.\nThe Synthetic Control (SC) method is a statistical method used to estimate causal effects from binary treatments on observational panel (longitudinal) data. The method got quite a coverage by being described as ‚Äúthe most important innovation in the policy evaluation literature in the last few years‚Äù and got an article published in Washington Post - Seriously, here‚Äôs one amazing math trick to learn what can‚Äôt be known. ‚ÄúSC is a technique to create an artificial control group by taking a weighted average of untreated units in such a way that it reproduces the characteristics of the treated units before the intervention(treatment). The SC acts as the counterfactual for a treatment unit, and the estimate of a treatment effect is the difference between the observed outcome in the post-treatment period and the SC‚Äôs outcome.‚Äù1\n‚ÄúOne way to think of SC is as an improvement upon difference-in-difference (DiD) estimation. Typical DiD will compare a treated unit to the average of the control units. But often the treated unit does not look like a typical control (e.g., it might have a different growth rate), in which case the ‚Äòparallel trend‚Äô assumption of DiD is not valid. SC remedies this by choosing a smarter linear combination, rather than the simple average, to weigh more heavily the more similar units. SC‚Äôs assumption is if there are endogenous factors that affect treatment and future outcomes then you should be able to control them by matching past outcomes. The matching that SC provides can therefore deal with some problems in estimation that DiD cannot handle.‚Äù2\nHere is the link to the Causal inference book which I found most useful to understand the math behind SC- Causal Inference for The Brave and True by Matheus Facure - Chapter 15."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Why should any Data scientist care about this method?",
    "text": "Why should any Data scientist care about this method?\nOften as a Data Scientist, you will encounter situations as follows where running A/B testing is not feasible because of -\n\nLack of infrastructure\nLack of similar groups for running A/B testing (in case of evaluation of state policies, as there is no state equivalent of other)\nProviding unwanted advantage to one group over others. Sometimes running an A/B test can give an unfair advantage and lead you into anti-trust territory. For example, what if Amazon tries to charge differential pricing for different customers or apply different margins for their sellers for the same product?\n\nAs a data scientist, stakeholders may still ask you to estimate the impact of certain changes/treatments, and Synthetic controls can come to the rescue in this situation. For this reason, it is a valuable tool to keep in your algorithmic toolkit."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Problem Overview",
    "text": "Problem Overview\nWe will use the Proposition 99 data to explain the use case for this approach and also how to use the SparceSC library and its key features. ‚ÄúIn 1988, California passed a famous Tobacco Tax and Health Protection Act, which became known as Proposition 99. Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements. To evaluate its effect, we can gather data on cigarette sales from multiple states and across a number of years. In our case, we got data from the year 1970 to 2000 from 39 states.‚Äù3\n\n\nCode\n# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport SparseSC\nfrom datetime import datetime\nimport warnings\nimport plotly.express as px\nimport plotly.graph_objects as pgo\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings('ignore')\n\n\nLet‚Äôs look at the data\n\n#Import data\ndata_dir = \"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/\"\ndf = pd.read_csv(data_dir + \"smoking_data\" + \".csv\").drop(columns=[\"lnincome\",\"beer\", \"age15to24\"])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      cigsale\n      retprice\n    \n  \n  \n    \n      0\n      Alabama\n      1970.0\n      89.8\n      39.6\n    \n    \n      1\n      Alabama\n      1971.0\n      95.4\n      42.7\n    \n    \n      2\n      Alabama\n      1972.0\n      101.1\n      42.3\n    \n    \n      3\n      Alabama\n      1973.0\n      102.9\n      42.1\n    \n    \n      4\n      Alabama\n      1974.0\n      108.2\n      43.1\n    \n  \n\n\n\n\nWe have data per state as treatment unit and yearly (year column) per-capita sales of cigarettes in packs (cigsale column) and the cigarette retail price (retprice column). We are going to pivot this data so that each row is one treatment unit(state), and columns represent the yearly cigsale value.\n\ndf = df.pivot(index= 'state', columns = 'year', values = \"cigsale\")\ndf.head()\n\n\n\n\n\n  \n    \n      year\n      1970.0\n      1971.0\n      1972.0\n      1973.0\n      1974.0\n      1975.0\n      1976.0\n      1977.0\n      1978.0\n      1979.0\n      1980.0\n      1981.0\n      1982.0\n      1983.0\n      1984.0\n      1985.0\n      1986.0\n      1987.0\n      1988.0\n      1989.0\n      1990.0\n      1991.0\n      1992.0\n      1993.0\n      1994.0\n      1995.0\n      1996.0\n      1997.0\n      1998.0\n      1999.0\n      2000.0\n    \n    \n      state\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Alabama\n      89.8\n      95.4\n      101.1\n      102.9\n      108.2\n      111.7\n      116.2\n      117.1\n      123.0\n      121.4\n      123.2\n      119.6\n      119.1\n      116.3\n      113.0\n      114.5\n      116.3\n      114.0\n      112.1\n      105.6\n      108.6\n      107.9\n      109.1\n      108.5\n      107.1\n      102.6\n      101.4\n      104.9\n      106.2\n      100.7\n      96.2\n    \n    \n      Arkansas\n      100.3\n      104.1\n      103.9\n      108.0\n      109.7\n      114.8\n      119.1\n      122.6\n      127.3\n      126.5\n      131.8\n      128.7\n      127.4\n      128.0\n      123.1\n      125.8\n      126.0\n      122.3\n      121.5\n      118.3\n      113.1\n      116.8\n      126.0\n      113.8\n      108.8\n      113.0\n      110.7\n      108.7\n      109.5\n      104.8\n      99.4\n    \n    \n      California\n      123.0\n      121.0\n      123.5\n      124.4\n      126.7\n      127.1\n      128.0\n      126.4\n      126.1\n      121.9\n      120.2\n      118.6\n      115.4\n      110.8\n      104.8\n      102.8\n      99.7\n      97.5\n      90.1\n      82.4\n      77.8\n      68.7\n      67.5\n      63.4\n      58.6\n      56.4\n      54.5\n      53.8\n      52.3\n      47.2\n      41.6\n    \n    \n      Colorado\n      124.8\n      125.5\n      134.3\n      137.9\n      132.8\n      131.0\n      134.2\n      132.0\n      129.2\n      131.5\n      131.0\n      133.8\n      130.5\n      125.3\n      119.7\n      112.4\n      109.9\n      102.4\n      94.6\n      88.8\n      87.4\n      90.2\n      88.3\n      88.6\n      89.1\n      85.4\n      83.1\n      81.3\n      81.2\n      79.6\n      73.0\n    \n    \n      Connecticut\n      120.0\n      117.6\n      110.8\n      109.3\n      112.4\n      110.2\n      113.4\n      117.3\n      117.5\n      117.4\n      118.0\n      116.4\n      114.7\n      114.1\n      112.5\n      111.0\n      108.5\n      109.0\n      104.8\n      100.6\n      91.5\n      86.7\n      83.5\n      79.1\n      76.6\n      79.3\n      76.0\n      75.9\n      75.5\n      73.4\n      71.4\n    \n  \n\n\n\n\nLet‚Äôs observe how cigarettes sales per capita is trending over time w.r.t California and other states.\n\n\nCode\nplot_df = df.loc[df.index == \"California\"].T.reset_index(drop=False)\nplot_df[\"OtherStates\"] = df.loc[df.index != \"California\"].mean(axis=0).values\n\n\nfig = px.line(\n        data_frame = plot_df, \n        x = \"year\", \n        y = [\"California\",\"OtherStates\"], \n        template = \"plotly_dark\")\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[plot_df.California.min()*0.98,plot_df.OtherStates.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Gap in per-capita cigarette sales(in packs)\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Cigarette Sales Trend\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 1 - Cigarette sales comparison b/w California and other states\n\n\nAs we can see from the chart above, we can see that there is a general decline in cigarette sales after the 1980s, and with the introduction of Proposition 99, the decreasing trend accelerated for the state of California. We cannot say for sure if this is happening with any statistical significance, it is just something we observed by examining the chart above.\nTo answer the question of whether Proposition 99 influenced cigarette consumption, we will use the pre-intervention period (1970-1988) to build a synthetic control group that mimics California cigarette sales trend. Then, we will see how this synthetic control behaves after the intervention."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Fitting Synthetic Control using SparseSC package",
    "text": "Fitting Synthetic Control using SparseSC package\nOn a high level SparseSC package provide two functions for fitting Synthetic controls i.e., fit() method and fit_fast() method. On a high level -\n\nfit() - This method tries to compute the weight jointly and results in SCs which are ‚Äòoptimal‚Äô. This is the most common method used in most of the code/libraries I have found but it is computationally expensive and takes a long time to run. Hence, does not scale for larger datasets.\nfit_fast()- This method tries to compute the weight separately by performing some non-matching analysis. This solution is much faster and often the only feasible method with larger datasets. The authors of this package recommend the fit_fast method to start with and only move towards the fit method if needed.\n\nThe SparseSC.fit_fast() method required at least three arguments -\n\nfeatures - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from pre-treatment (1970-1988), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntargets - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from post-treatment (1999-2000), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntreatment_units - This is the list of integers containing the row index value of treated units\n\n\n\n\n\n\n\nNote\n\n\n\nNote that treatment units can be a list of multiple treatment indexes. Think of cases where the same treatment is applied to multiple groups, for example, what if proposition 99 was rolled in both California and Minnesota State, in this case, treatment_units will get [2, 15], which are the respective index of these states.\n\n\n\n## creating required features\nfeatures = df.iloc[:,df.columns <= 1988].values\ntargets = df.iloc[:,df.columns > 1988].values\ntreated_units = [idx for idx, val in enumerate(df.index.values) if val == 'California'] # [2]\n\n## Fit fast model for fitting Synthetic controls\nsc_model = SparseSC.fit_fast( \n    features=features,\n    targets=targets,\n    treated_units=treated_units\n)\n\nNow that we have fitted the model, let‚Äôs get the Synthetic Control output by using predict() function.\n\nresult = df.loc[df.index == 'California'].T.reset_index(drop=False)\nresult.columns = [\"year\", \"Observed\"] \nresult['Synthetic'] = sc_model.predict(df.values)[treated_units,:][0]\nresult.head(5)\n\n\n\n\n\n  \n    \n      \n      year\n      Observed\n      Synthetic\n    \n  \n  \n    \n      0\n      1970.0\n      123.0\n      122.394195\n    \n    \n      1\n      1971.0\n      121.0\n      125.114849\n    \n    \n      2\n      1972.0\n      123.5\n      129.704372\n    \n    \n      3\n      1973.0\n      124.4\n      126.753988\n    \n    \n      4\n      1974.0\n      126.7\n      126.276394\n    \n  \n\n\n\n\nNow that we have our synthetic control, we can plot it with the outcome variable of the State of California.\n\n\nCode\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = [\"Observed\",\"Synthetic\"], \n        template = \"plotly_dark\",)\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result.Observed.min()*0.98,result.Observed.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Synthetic Control Assessment\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig - Assessment of Proposition 99 on state of California using Synthetic Control\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the pre-intervention period, the synthetic control does not reproduce the treatment exactly but follows the curve closely. This is a good sign, as it indicates that we are not overfitting. Also, note that we do see divergence after the intervention (introduction of Proposition 99) after 1988.\n\n\nWith the synthetic control groups in hand, we can estimate the treatment effect as the gap between the treated and the synthetic control outcomes.\n\n\nCode\nresult['California Effect'] = result.Observed - result.Synthetic\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = \"California Effect\", \n        template = \"plotly_dark\",)\nfig.add_hline(0)\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result[\"California Effect\"].min()*0.98,result[\"California Effect\"].max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\n\nfig.update_layout(\n        title  = {\n            'text':\"Difference across time\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Gap in Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig - Gap in Per-capita cigarette sales in California w.r.t Synthetic Control\n\n\n\n\nCode\nprint(f\"Effect of Proposition 99 w.r.t Synthetic Control => {np.round(result.loc[result.year==2000,'California Effect'].values[0],1)} packs\")\n\n\nEffect of Proposition 99 w.r.t Synthetic Control => -28.8 packs\n\n\nLooking at the chart above, we can observe that by the year 2000, Proposition 99 has reduced the sales of cigarettes by ~29 packs. Now we will figure out if this is statistically significant."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Making Inference",
    "text": "Making Inference\nIn Synthetic control, to find if the effect we observed is significant or not, we run a placebo test. A placebo test is taking a random untreated unit and pretending all other units are in control and fit the Synthetic control over this randomly selected untreated unit to estimate the effect. Once we have repeated this placebo test multiple times, we can estimate the distribution of this randomly observed effect and see if the effect observed is significantly different from the placebo observed effect. In the SparceSC package, we can use the estimate_effects method to do this automatically for us. The estimate effects method takes a minimum of two arguments -\n\noutcomes - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), and each column is the period of both pre-treatment and post-treatment period and the value in the matrix is the metric of interest (in this case it is the cigsale value)\nunit_treatment_periods - Vector of treatment periods for each unit, (if a unit is never treated then use np.NaN if vector refers to periods by numerical index)\n\n\n## Creating unit treatment_periods\nunit_treatment_periods = np.full((df.values.shape[0]), np.nan)\nunit_treatment_periods[treated_units] = [idx for idx, colname in enumerate(df.columns) if colname == 1988][0]\n\n## fitting estimate effects method\nsc = SparseSC.estimate_effects(\n    outcomes = df.values,  \n    unit_treatment_periods = unit_treatment_periods, \n    max_n_pl=50, # Number of placebos\n    level=0.9 # Level for confidence intervals\n)\nprint(sc)\n\nPre-period fit diagnostic: Were the treated harder to match in the pre-period than the controls were.\nAverage difference in outcome for pre-period between treated and SC unit (concerning if p-value close to 0 ): \n1.8073663793403947 (p-value: 0.9743589743589743)\n\n(Investigate per-period match quality more using self.pl_res_pre.effect_vec)\n\nAverage Effect Estimation: -16.965374734951705 (p-value: 0.07692307692307693)\n\nEffect Path Estimation:\n -2.712194133284143 (p-value: 0.6923076923076923)\n-6.424223898557088 (p-value: 0.20512820512820512)\n-4.18303325159971 (p-value: 0.5128205128205128)\n-10.210104128966762 (p-value: 0.28205128205128205)\n-10.198518971790051 (p-value: 0.2564102564102564)\n-14.921163932323616 (p-value: 0.15384615384615385)\n-18.441946863077938 (p-value: 0.1282051282051282)\n-21.27793738802989 (p-value: 0.1794871794871795)\n-22.47075245885469 (p-value: 0.1794871794871795)\n-23.397467590595554 (p-value: 0.20512820512820512)\n-26.13637789807555 (p-value: 0.05128205128205128)\n-30.504443997992325 (p-value: 0.02564102564102564)\n-29.67170704122487 (p-value: 0.05128205128205128)\n\n \n\n\nThe estimate_effects method returns an object which will print the treatment effect of each post-treatment year and estimate the significance of the observed difference. The information printed can also be found in the pl_res_pre function of the returned object.\n\nprint(f\"Estimated effect of sales in California state in year 2000 because of preposition 99 is {np.round(sc.pl_res_post.effect_vec.effect[-1])}, \\\nwith a p-value of  {np.round(sc.pl_res_post.effect_vec.p[-1],2)}\")\n\nEstimated effect of sales in California state in year 2000 because of preposition 99 is -30.0, with a p-value of  0.05"
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Conclusion",
    "text": "Conclusion\nHere are some key takeaways -\n\nSynthetic control allows us to combine multiple control units to make them resemble the treated unit. With synthetic control, we can estimate what would have happened to our treated unit in the absence of treatment.\nMicrosoft SparseSC library provides a fast and easy-to-use API to run synthetic control groups and allows us to run a placebo test to estimate the significance of the effects observed."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "title": "Mixing art into the science of model explainability",
    "section": "",
    "text": "Overview on Explainable Boosting Machine and an approach for converting ML explanation to more human-friendly explanation."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "title": "Mixing art into the science of model explainability",
    "section": "1.1 The Interpretability vs Accuracy Trade-off",
    "text": "1.1 The Interpretability vs Accuracy Trade-off\nIn traditional tabular machine learning approaches, Data scientists often deal with the trade-off b/w interpretability and accuracy.\n\n\n\nFig.2: Interpretability/Intelligibility and Accuracy Tradeoff  Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs shown in the chart above, we can see that Glass-Box models like Logistic Regression, Naive Bayes, and Decision Trees are simple models to interpret, and predictions from these models are not highly accurate. On the other hand, Black-Box models like Boosted Trees, Random Forest, and Neural Nets are hard to interpret but lead to highly accurate predictions."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "1.2 Introducing EBMs",
    "text": "1.2 Introducing EBMs\nTo solve the problem just mentioned above EBMs(Explainable Boosted Machine) model was developed by Microsoft Research1. ‚ÄúExplainable Boosting Machine (EBM) is a tree-based, cyclic gradient boosting Generalized Additive Model with automatic interaction detection. EBMs are often as accurate as state-of-the-art BlackBox models while remaining completely interpretable. Although EBMs are often slower to train than other modern algorithms, EBMs are extremely compact and fast at prediction time.‚Äù2\n\n\n\nFig.3: EBMs breaking the Interpretability vs Accuracy paradox Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs we can see from the chart above, EBMs help us break out of this trade-off paradox and help us build models which are both highly interpretable and accurate. To further understand the math behind EBMs I highly encourage watching this 12-minute YouTube video -\n\n\n\n\n\nVideo - The Science Behind InterpretML: Explainable Boosting Machine"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "title": "Mixing art into the science of model explainability",
    "section": "1.3 Glass box vs Black box models. What to choose?",
    "text": "1.3 Glass box vs Black box models. What to choose?\n\n\n\n\n\n\nTip\n\n\n\nThe answer to every complex question in life is ‚ÄúIt depends‚Äù.\n\n\nThere are trade-offs b/w using Glassbox models as compared to Blackbox models. There is no clear winner in picking one model over the other but depending on the situation DS can make an educated guess on what model to pick.\n\n\n\nFig.4: Glassbox models vs BlackBox models\n\n\nTwo considerations to think about while picking glass box vs black box models are the following-\n1) Explainability Requirements - In the domain where there is no need for explanation or it is needed for a data scientist or technical audience for intuition/inspection purposes, in these cases, DS are well off using black box models. In the domain where an explanation is needed because of business or regulatory requirements or where these explanations are served to a non-technical audience (humans), glass-box models have an upper hand. This is because explanations coming out of the glass box models are exact and global.\n\n\n\n\n\n\nNote\n\n\n\nExact and global just means that a value of a particular feature will always have the same effect on each prediction explanation. For example, in the case of the prediction of income of a particular individual being above $50k with age as one of the predictors, if the age is 40 and it will impact the target variable with the same proportion let us say 5% in each observation in the data where the age is 40. This is not the case when we build explanations through LIME and Shapely for black box models. In black-box models, age with the value 40 for example can have a 10% lift in an individual probability of their income being above 50k for one observation and -10% lift in the other.\n\n\n2) Compute Requirement - DS needs to pay attention to various compute requirements for testing and training a model depending on its use case. EBMs are particularly slow in the training phase but provide fast predictions with built-in explanations. So, in cases where you need to train your model every hour, EBMs might not suffice your need. But, in cases where the training of the model happens monthly/weekly, and scores are generated on a more frequent basis (hourly/daily) EBMs might fit the use case well. Also, in cases where you might be required to produce an explanation for each prediction EBMs can save a lot of computing and might be the only feasible technique to use for millions of observations. Look below to understand the operational difference b/w EBMs and other tree-based ensemble methods.\n\n\n\nFig. 5: EBMs vs XgBoost/LightGBM"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "title": "Mixing art into the science of model explainability",
    "section": "2.1 Data Overview",
    "text": "2.1 Data Overview\nFor this example, we will use Adult Income Dataset from the UCI machine learning Repository3. The problem in this dataset is set up as a binary classification problem to predict if a certain individual income based on various census information (education level, age, gender, occupation, etc.) exceeds $50K/year. For sake of simplicity, we are only going to use observations of individuals in the United States and the following predictors -\n\nAge: continuous variable, individuals‚Äô age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\n\n\nCode\n## Importing required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nimport warnings\nimport plotly.io as pio\nimport plotly.express as px\nwarnings.filterwarnings('ignore')\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n## Loading the data\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Education\n      Age\n      Occupation\n      HoursPerWeek\n      Income\n    \n  \n  \n    \n      0\n      Bachelors\n      39\n      Adm-clerical\n      40\n      <=50K\n    \n    \n      1\n      Bachelors\n      50\n      Exec-managerial\n      13\n      <=50K\n    \n    \n      2\n      HS-grad\n      38\n      Handlers-cleaners\n      40\n      <=50K\n    \n    \n      3\n      11th\n      53\n      Handlers-cleaners\n      40\n      <=50K\n    \n    \n      5\n      Masters\n      37\n      Exec-managerial\n      40\n      <=50K\n    \n  \n\n\n\n\nLet‚Äôs look at target variable distribution.\n\n\nCode\nplot_df = df.Income.value_counts().reset_index().rename(columns = {\"index\":\"Income\", \"Income\":\"Count\"})\nfig = px.bar(plot_df, x = \"Income\", y = 'Count')\nfig.update_layout(\n        title  = {\n            'text':\"Target variable distribution\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Income\", \n        yaxis_title=\"Count of obersvations\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 1 - Target variable distribution\n\n\n\n\nCode\nprint(df.Income.value_counts(normalize=True))\n\n\n <=50K    0.754165\n >50K     0.245835\nName: Income, dtype: float64\n\n\n~24.6% of people in our dataset have income greater than $50K. The data looks good, we have the columns we need. We will use Education, Age, Occupation, and HoursPerWeek columns and predict Income. Before modeling, let us perform an 80-20 train-test split.\n\n## Train-Test Split\nX = df[df.columns[0:-1]]\ny = df[df.columns[-1]]\nseed = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\nprint(f\"Data in training {len(y_train)}, Data in testing {len(y_test)}\")\n\nData in training 23336, Data in testing 5834"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "title": "Mixing art into the science of model explainability",
    "section": "2.2 Fitting an EBM Model",
    "text": "2.2 Fitting an EBM Model\nEBMs have a scikit-compatible API, so fitting the model and making predictions are the same as any scikit learn model.\n\nebm = ExplainableBoostingClassifier(random_state=seed, interactions=0)\nebm.fit(X_train, y_train)\n\nauc = np.round(metrics.roc_auc_score((y_test != ' <=50K').astype(int).values, ebm.predict_proba(X_test)[:,1], ),3)\nprint(f\"Accuracy: {np.round(np.mean(ebm.predict(X_test) == y_test)*100,2)}%, AUC: {auc}\")\n\nAccuracy: 80.12%, AUC: 0.828\n\n\nI hope the above code block shows how similar the interpret-ml API is to the scikit learn API. Based on AUC on the validation set we can say our model is better than random predictions.\n\n\n\n\n\n\nTip\n\n\n\nIn practice, if you are dealing with millions of observations, Try doing feature selection using LightGBM/XGboost and only train your final models using EBMs. This will save you time in feature exploration."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "2.3 Explaination from EBMs",
    "text": "2.3 Explaination from EBMs\nInterpret package comes with both global and local explanations and has a variety of visualization tools to inspect what the model is learning.\n\n2.3.1 Global explanations\nGlobal explanations provide the following visualization -\n\nSummary - Feature importance plot, this chart provides the importance of each predictor in predicting the target variable\nFeature interaction with Prediction - This chart is the same look-up table EBM uses in making the actual prediction. This can help you in the inspection of how the feature value is contributing to prediction.\n\n\nebm_global = ebm.explain_global()\nshow(ebm_global, renderer='notebook')\n\n\n\n\nFig. 6: EBMs Global Explaination\n\n\n\n\n2.3.2 Local explanations\nThe local explanation is our per-observation level explanation. EBMs have a great in-built visualization for displaying this information.\n\nebm_local = ebm.explain_local(X_test.iloc[0:5,:], y_test)\nshow(ebm_local, renderer='notebook')\n\n\n\n\nFig. 7: EBMs local Explaination\n\n\nLet‚Äôs take one example of this explanation for observation at index 0 and look at it -\n\nexplainDF = pd.DataFrame.from_dict(\n    {\n        'names': ebm_local.data(0)['names'], \n        'data':ebm_local.data(0)['values'], \n        'contribution':ebm_local.data(0)['scores']\n    })\nexplainDF\n\n\n\n\n\n  \n    \n      \n      names\n      data\n      contribution\n    \n  \n  \n    \n      0\n      Education\n      Bachelors\n      0.733420\n    \n    \n      1\n      Age\n      47\n      1.048227\n    \n    \n      2\n      Occupation\n      ?\n      -0.318846\n    \n    \n      3\n      HoursPerWeek\n      18\n      -0.854202\n    \n  \n\n\n\n\nAs we can see from the data, we can see we have the Name of the columns, the actual values, and the contribution of that value to the actual prediction score. For this observation let us see what the model is learning -\n\nEducation as Bachelors is working in favor of >50K income\nAge value 47 is also in favor of >50K\nOccupation being ‚Äú?‚Äù has a negative impact on >50K income\nHoursPerWeek being 18 has a negative impact on >50K income (Average work week hours in the US are around 40, so this makes sense)\n\nYou can also do it for the entire dataset and collect the importance of each feature. Here is a sample code to do the same.\n\nscores = [x['scores'] for x in ebm_local._internal_obj['specific']]\nsummary = pd.DataFrame(scores)\nsummary.columns = ebm_local.data(0)['names']\nsummary.head()\n\n\n\n\n\n  \n    \n      \n      Education\n      Age\n      Occupation\n      HoursPerWeek\n    \n  \n  \n    \n      0\n      0.733420\n      1.048227\n      -0.318846\n      -0.854202\n    \n    \n      1\n      -0.990661\n      0.309251\n      0.171131\n      0.002109\n    \n    \n      2\n      -0.257254\n      0.735232\n      0.171131\n      0.002109\n    \n    \n      3\n      0.193118\n      0.682721\n      -0.417499\n      0.279677\n    \n    \n      4\n      0.733420\n      0.085672\n      0.389171\n      0.002109\n    \n  \n\n\n\n\nNow we can extract the importance of all data rows in our test set."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "title": "Mixing art into the science of model explainability",
    "section": "2.4 Draw back and concerns",
    "text": "2.4 Draw back and concerns\n\n\n\nCredit - XKCD\n\n\nThese kinds of explanations are still very abstract, even at the observational level reasoning is not human(non-technical) friendly. When the feature count grows this becomes even non-human friendly. Typical business consumers of your model might not be well versed in reading such charts and shy away from trying the insights/predictions the model is giving them. After all, if I don‚Äôt understand something, I don‚Äôt trust it. That is where art comes in, let‚Äôs see how we can build on the above-derived observations and make it easier to understand."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "title": "Model calibration for classification task using Python",
    "section": "",
    "text": "A hands-on introduction to model calibration using Python."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "title": "Model calibration for classification task using Python",
    "section": "1 What is Model Calibration?",
    "text": "1 What is Model Calibration?\nWhen working with classification tasks, the machine learning models often produce a probabilistic outcome ranging between 0 to 1. This probabilistic output is then used by organizations/machine learning scientists to make decisions. Unfortunately, many machine learning models‚Äô probabilistic outputs cannot be directly interpreted as the probability of an event happening backed by data. To achieve this outcome, the ML model needs to be calibrated.\n\nFormally, a model is said to be perfectly calibrated if, for any probability value p, a prediction of a class with confidence p is correct 100*p percent of the time.\n\nLet us visualize a perfect calibration curve.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"Fraction of Positives Captured\": np.arange(0,1.1, 0.1),\n    \"Predicted Probability\": np.arange(0,1.1, 0.1)\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"Predicted Probability\", \n        y = \"Fraction of Positives Captured\",\n        template = \"plotly_dark\")\n\nfig.update_layout(\n        title  = {\"text\": \"Perfect calibration\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.update_traces(patch={\"line\": {\"dash\": \"dash\"}}) \nfig.show(renderer=\"notebook\")\n\n\n\n                                                \nFig 1 - A perfectly calibrated classifier\n\n\nOn the x-axis, we have model output p which is between 0 and 1 and on the y-axis, we have fractions of positive captured within the predicted probability bin. We expect a linear relationship with slope 1."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "title": "Model calibration for classification task using Python",
    "section": "2 Why do we need Model calibration?",
    "text": "2 Why do we need Model calibration?\nThere are many cases where model calibration is not required like in the case of ranking or selecting the top 20% for some targeting campaign. Let‚Äôs focus on cases where calibrated models can add a lot of value -\n\n2.1 Case 1: Calibrated models provide flexibility\nConsider the following example -  A data scientist built an ML model to estimate the probability of a customer churning. Let‚Äôs say you want to prioritize b/w the following two customers-\n\n\n\nFig. 2: Churn probability\n\n\nIn this case, Customer 1 has an 80% probability of churn but the revenue generated by them is \\$100 and Customer 2 has a 60% chance of churn, and the revenue generated is about $1000. As a business, you might want to save Customer 2. To make such probability to revenue impact conversion or any cost-benefit analysis you need model calibration.\n\n\n2.2 Case 2: Model modularity\nIn complex machine-learning decision engines, a machine-learning model might be used in conjunction with other machine-learning models. Consider the following example -  A data scientist built two ML models to estimate the probability of a customer upselling two different products. Let‚Äôs say you want to prioritize which recommendation to act on looking at the following case -\n\n\n\nFig. 3: Upsell probability\n\n\nIn this case, Product 1 has an 80% probability of upselling but the revenue generated by this product is $10 per SKU and Product 2 has a 60% chance of upselling and revenue generated is about $100 per SKU. As a business, you might want to recommend Product 2 to the customer. To make such probability to revenue impact conversion b/w models, you need your models to be calibrated."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#various-methods-for-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#various-methods-for-calibration",
    "title": "Model calibration for classification task using Python",
    "section": "3 Various Methods for Calibration",
    "text": "3 Various Methods for Calibration\n\n3.1 Data overview\nFor this blog, we will use Adult Income Dataset from the UCI machine learning Repository1. The problem in this dataset is set up as a binary classification problem to predict if a certain individual income based on various census information (education level, age, gender, occupation, etc.) exceeds $50K/year. For sake of simplicity, we are only going to use observations of individuals in the United States and the following predictors -\n\nAge: continuous variable, individuals‚Äô age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\n\n\nCode\n## Importing required libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('dark_background')\n\ndef calibration_data(y_true, y_pred):\n    df = pd.DataFrame({'y_true':y_true, 'y_pred_bucket': (y_pred//0.05)*0.05 + 0.025})\n    cdf = df.groupby(['y_pred_bucket'], as_index=False).agg({'y_true':[\"mean\",\"count\"]})\n    return cdf.y_true.values[:,0][cdf.y_true.values[:,1]>10], cdf.y_pred_bucket.values[cdf.y_true.values[:,1]>10]\n\ndef label_encoder(df,columns):\n    '''\n    Function to label encode\n    Required Input - \n        - df = Pandas DataFrame\n        - columns = List input of all the columns which needs to be label encoded\n    Expected Output -\n        - df = Pandas DataFrame with lable encoded columns\n        - le_dict = Dictionary of all the column and their label encoders\n    '''\n    le_dict = {}\n    for c in columns:\n        print(\"Label encoding column - {0}\".format(c))\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values.astype('str')))\n        df[c] = lbl.transform(list(df[c].values.astype('str')))\n        le_dict[c] = lbl\n    return df, le_dict\n\n\n\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\ndf, _ = label_encoder(df, columns = [\"Education\", \"Occupation\", \"Income\"])\nX = df.loc[:,[\"Education\",\"Age\", \"Occupation\", \"HoursPerWeek\"]]\ny = df.Income\n\nLabel encoding column - Education\nLabel encoding column - Occupation\nLabel encoding column - Income\n\n\n\nseed = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n\n\n\n3.2 Visualizing the calibration issue\nLet‚Äôs fit a random forest classifier on this data and visualize the calibration plot.\n\n## Fitting the model on training data\nrf_model = RandomForestClassifier(random_state=seed)\nrf_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = rf_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Without calibration\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 4 - An example of non-calibrated classifier\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be over-predicting in lower deciles and under-predicting in higher deciles.\n\n\n\n\n3.3 Isotonic Regression\nWe will be using CalibratedClassifierCVto calibrate our classifier. The isotonic method fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function. 2\n\n## training model using random forest and isotonic regression for calibration\ncalibrated_rf = CalibratedClassifierCV(RandomForestClassifier(random_state=seed), method = 'isotonic')\ncalibrated_rf.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_rf.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Isotonic\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 5 - An example of calibration using Isotonic regression\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be well calibrated once Isotonic regression is used.\n\n\n\n\n3.4 Sigmoid Method\n\n## training model using random forest and sigmoid method for calibration\ncalibrated_sigmoid = CalibratedClassifierCV(\n     RandomForestClassifier(random_state=seed), method = 'sigmoid')\ncalibrated_sigmoid.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_sigmoid.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot : Sigmoid\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 6 - An example of calibration using sigmoid method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be calibrated but not as well as the isotonic method.\n\n\n\n\n3.5 Train with ‚ÄúLogloss‚Äù metric\nMany boosting based ensembling classifier models like GradientBoostingClassifier, LightGBM, and Xgboost use log_loss as their default loss function. Training with log_loss helps the output probabilities to be calibrated.\n\n## Fitting the model on training data\ngb_model = GradientBoostingClassifier()\ngb_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = gb_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Using boosting method\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n\n                                                \nFig 7 - An example of calibration using boosting method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be well calibrated. This is usually the case with most of the boosted methods as they use the log_loss function as their default loss function for binary classification."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "title": "Model calibration for classification task using Python",
    "section": "4 Conclusion and Practical Guidance",
    "text": "4 Conclusion and Practical Guidance\nIn practice, using boosting methods tends to be better calibrated and if you are using some other classifiers try using isotonic regression with cross-validation to calibrate. I will try to add more in a follow-up blog post, as I research further into this topic."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "",
    "text": "A brief introduction to start generating images from text prompts using ü§ó hugging face - Diffusers library.\nThis is my first post of the Stable diffusion series, which I will write on Stable diffusion and other ongoing research happening in this field. Most of my learning can be attributed to knowledge acquired while doing the ‚ÄòFrom Deep learning foundations to Stable Diffusion‚Äô course by FastAI and supplementing this with my research. The first few lessons of the FastAI course are publicly available here, and the rest will become available in early 2023. In this post, I want to give a brief introduction of how to use setup the ü§ó diffusion library and start generating images on your own. Next post, we will do a deep dive into mid-level components of this library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "1 Introduction",
    "text": "1 Introduction\nStable diffusion simply put is a deep learning model which can generate an image given a textual prompt.\n\n\n\nFig. 1: Stable diffusion overview\n\n\nAs we can see from the image above we can pass a textual prompt like ‚ÄúA dog wearing a hat‚Äù and a stable diffusion model can generate an image representative of the text. Pretty amazing!"
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "2 Using Hugging face Diffuser library",
    "text": "2 Using Hugging face Diffuser library\nAs with any python library, we need to follow certain installation steps before we can run it, here is a rundown of these steps.\n\n2.1 Accepting the license\nBefore using the model, you need to go here and log in using your Hugging face account and then accept the model license to download and use the weights.\n\n\n2.2 Token generation\nIf this is your first time using the hugging face library this might sound like a weird step. You need to go here and generate a token (preferably with write access) to download the model.\n\n\n\nFig. 2: Access token page\n\n\nOnce you have generated the token copy it. First, we will download the hugging face hub library using the following code.\n\n!pip install huggingface-hub==0.10.1\n\nRequirement already satisfied: huggingface-hub==0.10.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (0.10.1)\nRequirement already satisfied: filelock in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (3.8.0)\nRequirement already satisfied: tqdm in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.64.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.3.0)\nRequirement already satisfied: packaging>=20.9 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (21.3)\nRequirement already satisfied: requests in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub==0.10.1) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests->huggingface-hub==0.10.1) (3.4)\n\n\nThen use the following code, once you run it a widget will appear, paste your newly generated token and click login.\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nLogin successful\nYour token has been saved to /home/aayush/.huggingface/token\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\n\n\n\n\n2.3 Installing diffuser and transformer library\nOnce this process is done, install the dependencies using the following code. This will download the latest version of the diffusers and transformers library.\n\n!pip install -qq -U diffusers transformers\n\nThat‚Äôs it, now we are ready to use the diffusers library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "3 Running Stable Diffusion",
    "text": "3 Running Stable Diffusion\nThe first step is to import the StableDiffusionPipeline from the diffusers library.\n\nfrom diffusers import StableDiffusionPipeline\n\nThe next step is to initialize a pipeline to generate an image. The first time you run the following command, it will download the model from the hugging face model hub to your local machine. You will require a GPU machine to be able to run this code.\n\npipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let‚Äôs pass a textual prompt and generate an image.\n\n# Initialize a prompt\nprompt = \"a dog wearing hat\"\n# Pass the prompt in the pipeline\npipe(prompt).images[0]\n\n\nFig 3 - An example of image generated by the diffuser pipeline.\n\n\n\n\n\nFor further information on the diffusion pipeline read the documentation here."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Introduction",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we saw how to install diffusers library from hugging face and use the Stable diffusion model to generate images using a textual prompt. Read the part 2 here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "",
    "text": "An introduction into what goes on in the pipe function of ü§ó hugging face diffusers library StableDiffusionPipeline function.\nThis is my second post of the Stable diffusion series, if you haven‚Äôt checked out the first one, you can read it here -  1. Part 1 - Introduction to Stable diffusion using ü§ó Hugging Face.\nIn this post, we will understand the basic components of a stable diffusion pipeline and their purpose. Later we will reconstruct StableDiffusionPipeline.from_pretrained function using these components. Let‚Äôs get started -"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "1 Introduction",
    "text": "1 Introduction\nDiffusion models as seen in the previous post can generate high-quality images. Stable diffusion models are a special kind of diffusion model called the Latent Diffusion model. They have first proposed in this paper High-Resolution Image Synthesis with Latent Diffusion Models. The original Diffusion model tends to consume a lot more memory, so latent diffusion models were created which can do the diffusion process in lower dimension space called Latent Space. On a high level, diffusion models are machine learning models that are trained to denoise random Gaussian noise step by step, to get the result i.e., image. In latent diffusion, the model is trained to do this same process in a lower dimension. \nThere are three main components in latent diffusion - \n\nA text encoder, in this case, a CLIP Text encoder\nAn autoencoder, in this case, a Variational Auto Encoder also referred to as VAE\nA U-Net\n\nLet‚Äôs dive into each of these components and understand their use in the diffusion process. The way I will be attempting to explain these components is by talking about them in the following three stages - \n\nThe Basics: What goes in the component and what comes out of the component - This is an important, and key part of the top-down learning approach of understanding ‚Äúthe whole game‚Äù\nDeeper explanation using ü§ó code. - This part will provide more understanding of what the model produces using the code\nWhat‚Äôs their role in the Stable diffusion pipeline - This will build your intuition around how this component fits in the Stable diffusion process. This will help your intuition on the diffusion process"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "2 CLIP Text Encoder",
    "text": "2 CLIP Text Encoder\n\n2.1 Basics - What goes in and out of the component?\nCLIP(Contrastive Language‚ÄìImage Pre-training) text encoder takes the text as an input and generates text embeddings that are close in latent space as it may be if you would have encoded an image through a CLIP model.\n\n\n\nFig. 2: CLIP text encoder\n\n\n\n\n2.2 Deeper explanation using ü§ó code\nAny machine learning model doesn‚Äôt understand text data. For any model to understand text data, we need to convert this text into numbers that hold the meaning of the text, referred to as embeddings. The process of converting a text to a number can be broken down into two parts -  1. Tokenizer - Breaking down each word into sub-words and then using a lookup table to convert them into a number  2. Token_To_Embedding Encoder - Converting those numerical sub-words into a representation that contains the representation of that text \nLet‚Äôs look at it through code. We will start by importing the relevant artifacts.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nLet‚Äôs initialize a prompt and tokenize it.\n\nprompt = [\"a dog wearing hat\"]\ntok =tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \nprint(tok.input_ids.shape)\ntok\n\ntorch.Size([1, 77])\n\n\n{'input_ids': tensor([[49406,   320,  1929,  3309,  3801, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\nA tokenizer returns two objects in the form of a dictionary -  1. input_ids - A tensor of size 1x77 as one prompt was passed and padded to 77 max length. 49406 is a start token, 320 is a token given to the word ‚Äúa‚Äù, 1929 to the word dog, 3309 to the word wearing, 3801 to the word hat, and 49407 is the end of text token repeated till the pad length of 77.  2. attention_mask - 1 representing an embedded value and 0 representing padding.\n\nfor token in list(tok.input_ids[0,:7]): print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token))}\")\n\n49406:<|startoftext|>\n320:a</w>\n1929:dog</w>\n3309:wearing</w>\n3801:hat</w>\n49407:<|endoftext|>\n49407:<|endoftext|>\n\n\nSo, let‚Äôs look at the Token_To_Embedding Encoder which takes the input_ids generated by the tokenizer and converts them into embeddings -\n\nemb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\nprint(f\"Shape of embedding : {emb.shape}\")\nemb\n\nShape of embedding : torch.Size([1, 77, 768])\n\n\ntensor([[[-0.3887,  0.0229, -0.0522,  ..., -0.4902, -0.3066,  0.0673],\n         [ 0.0292, -1.3242,  0.3074,  ..., -0.5264,  0.9766,  0.6655],\n         [-1.5928,  0.5063,  1.0791,  ..., -1.5283, -0.8438,  0.1597],\n         ...,\n         [-1.4688,  0.3113,  1.1670,  ...,  0.3755,  0.5366, -1.5049],\n         [-1.4697,  0.3000,  1.1777,  ...,  0.3774,  0.5420, -1.5000],\n         [-1.4395,  0.3137,  1.1982,  ...,  0.3535,  0.5400, -1.5488]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n\n\nAs we can see above, each tokenized input of size 1x77 has now been translated to 1x77x768 shape embedding. So, each word got represented in a 768-dimensional space.\n\n\n2.3 What‚Äôs their role in the Stable diffusion pipeline\nStable diffusion only uses a CLIP trained encoder for the conversion of text to embeddings. This becomes one of the inputs to the U-net. On a high level, CLIP uses an image encoder and text encoder to create embeddings that are similar in latent space. This similarity is more precisely defined as a Contrastive objective. For more information on how CLIP is trained, please refer to this Open AI blog.\n\n\n\nFig. 3: CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. Credit - OpenAI"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "3 VAE - Variational Auto Encoder",
    "text": "3 VAE - Variational Auto Encoder\n\n3.1 Basics - What goes in and out of the component?\nAn autoencoder contains two parts -  1. Encoder takes an image as input and converts it into a low dimensional latent representation  2. Decoder takes the latent representation and converts it back into an image\n\n\n\nFig. 4: A Variational autoencoder. Original bird pic credit.\n\n\nAs we can see above, the Encoder acts like a compressor that squishes the image into lower dimensions and the decoder recreates the original image back from the compressed version.\n\n\n\n\n\n\nNote\n\n\n\nEncoder-Decoder compression-decompression is not lossless.\n\n\n\n\n3.2 Deeper explanation using ü§ó code\nLet‚Äôs start looking at VAE through code. We will start by importing the required libraries and defining some helper functions.\n\n\nCode\n## To import an image from a URL\nfrom fastdownload import FastDownload\n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## Loading a VAE model\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nLet‚Äôs download an image from the internet.\n\np = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')\nimg = load_image(p)\nprint(f\"Dimension of this image: {np.array(img).shape}\")\nimg\n\nDimension of this image: (512, 512, 3)\n\n\n\n\n\n\nFig. 5: Original bird pic credit.\n\nNow let‚Äôs compress this image by using the VAE encoder, we will be using the pil_to_latents helper function.\n\nlatent_img = pil_to_latents(img)\nprint(f\"Dimension of this latent representation: {latent_img.shape}\")\n\nDimension of this latent representation: torch.Size([1, 4, 64, 64])\n\n\nAs we can see how the VAE compressed a 3 x 512 x 512 dimension image into a 4 x 64 x 64 image. That‚Äôs a compression ratio of 48x! Let‚Äôs visualize these four channels of latent representations.\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')\n\n\n\n\n\nFig. 6: Visualization of latent representation from VAE encoder. \n\nThis latent representation in theory should capture a lot of information about the original image. Let‚Äôs use the decoder on this representation to see what we get back. For this, we will use the latents_to_pil helper function.\n\ndecoded_img = latents_to_pil(latent_img)\ndecoded_img[0]\n\n\n\n\n\nFig. 7: Visualization of decoded latent representation from VAE decoder. \n\nAs we can see from the figure above VAE decoder was able to recover the original image from a 48x compressed latent representation. That‚Äôs impressive!\n\n\n\n\n\n\nNote\n\n\n\nIf you look closely at the decoded image, it‚Äôs not the same as the original image, notice the difference around the eyes. That‚Äôs why VAE encoder/decoder is not a lossless compression.\n\n\n\n\n3.3 What‚Äôs their role in the Stable diffusion pipeline\nStable diffusion can be done without the VAE component but the reason we use VAE is to reduce the computational time to generate High-resolution images. The latent diffusion models can perform diffusion in this latent space produced by the VAE encoder and once we have our desired latent outputs produced by the diffusion process, we can convert them back to the high-resolution image by using the VAE decoder. To get a better intuitive understanding of Variation Autoencoders and how they are trained, read this blog by Irhum Shafkat."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "4 U-Net",
    "text": "4 U-Net\n\n4.1 Basics - What goes in and out of the component?\nThe U-Net model takes two inputs -  1. Noisy latent or Noise- Noisy latents are latents produced by a VAE encoder (in case an initial image is provided) with added noise or it can take pure noise input in case we want to create a random new image based solely on a textual description  2. Text embeddings - CLIP-based embedding generated by input textual prompts \nThe output of the U-Net model is the predicted noise residual which the input noisy latent contains. In other words, it predicts the noise which is subtracted from the noisy latents to return the original de-noised latents.\n\n\n\nFig. 8: A U-Net representation.\n\n\n\n\n4.2 Deeper explanation using ü§ó code\nLet‚Äôs start looking at U-Net through code. We will start by importing the required libraries and initiating our U-Net model.\n\nfrom diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initializing a scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n## Setting number of sampling steps\nscheduler.set_timesteps(51)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nAs you may have noticed from code above, we not only imported unet but also a scheduler. The purpose of a schedular is to determine how much noise to add to the latent at a given step in the diffusion process. Let‚Äôs visualize the schedular function -\n\n\nCode\nplt.plot(scheduler.sigmas)\nplt.xlabel(\"Sampling step\")\nplt.ylabel(\"sigma\")\nplt.title(\"Schedular routine\")\nplt.show()\n\n\n\n\n\n\nFig. 9: Sampling schedule visualization.\n\nThe diffusion process follows this sampling schedule where we start with high noise and gradually denoise the image. Let‚Äôs visualize this process -\n\n\nCode\nnoise = torch.randn_like(latent_img) # Random noise\nfig, axs = plt.subplots(2, 3, figsize=(16, 12))\nfor c, sampling_step in enumerate(range(0,51,10)):\n    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])\n    axs[c//3][c%3].set_title(f\"Step - {sampling_step}\")\n\n\n\n\n\n\nFig. 10: Noise progression through steps.\n\nLet‚Äôs see how a U-Net removes the noise from the image. Let‚Äôs start by adding some noise to the image.\n\n\nCode\nencoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\nlatents_to_pil(encoded_and_noised)[0]\n\n\n\n\n\n\nFig. 11: Noised Input fed to the U-Net.\n\nLet‚Äôs run through U-Net and try to de-noise this image.\n\n## Unconditional textual prompt\nprompt = [\"\"]\n\n## Using clip model to get embeddings\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad(): \n    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n    \n## Using U-Net to predict noise    \nlatent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()]).half()\nwith torch.no_grad():\n    noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n## Visualize after subtracting noise \nlatents_to_pil(encoded_and_noised- noise_pred)[0]\n\n\n\n\n\nFig. 12: De-Noised Output from U-Net\n\nAs we can see above the U-Net output is clearer than the original noisy input passed.\n\n\n4.3 What‚Äôs their role in the Stable diffusion pipeline\nLatent diffusion uses the U-Net to gradually subtract noise in the latent space over several steps to reach the desired output. With each step, the amount of noise added to the latents is reduced till we reach the final de-noised output. U-Nets were first introduced by this paper for Biomedical image segmentation. The U-Net has an encoder and a decoder which are comprised of ResNet blocks. The stable diffusion U-Net also has cross-attention layers to provide them with the ability to condition the output based on the text description provided. The Cross-attention layers are added to both the encoder and the decoder part of the U-Net usually between ResNet blocks. You can learn more about this U-Net architecture here."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn this post, we saw the key components of a Stable diffusion pipeline i.e., CLIP Text encoder, VAE, and U-Net. In the next post, we will look at the diffusion process using these components. Read the next part here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Looking under the hood",
    "section": "6 References",
    "text": "6 References\n\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "",
    "text": "An introduction to the diffusion process using ü§ó hugging face diffusers library.\nThis is my third post of the Stable diffusion series, if you haven‚Äôt checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.\nIn previous posts, I went over showing how to install ü§ó diffuser library to start generating your own AI images and key components of the stable diffusion pipeline i.e., CLIP text encoder, VAE, and U-Net. In this post, we will try to put these key components together and do a walk-through of the diffusion process which generates the image."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "1 Overview - The Diffusion Process",
    "text": "1 Overview - The Diffusion Process\nThe stable diffusion model takes the textual input and a seed. The textual input is then passed through the CLIP model to generate textual embedding of size 77x768 and the seed is used to generate Gaussian noise of size 4x64x64 which becomes the first latent image representation.\n\n\n\n\n\n\nNote\n\n\n\nYou will notice that there is an additional dimension mentioned (1x) in the image like 1x77x768 for text embedding, that is because it represents the batch size of 1.\n\n\n\n\n\nFig. 2: The diffusion process.\n\n\nNext, the U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings. The output of the U-Net is predicted noise residual, which is then used to compute conditioned latents via a scheduler algorithm. This process of denoising and text conditioning is repeated N times (We will use 50) to retrieve a better latent image representation. Once this process is complete, the latent image representation (4x64x64) is decoded by the VAE decoder to retrieve the final output image (3x512x512).\n\n\n\n\n\n\nNote\n\n\n\nThis iterative denoising is an important step for getting a good output image. Typical steps are in the range of 30-80. However, there are recent papers that claim to reduce it to 4-5 steps by using distillation techniques."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "2 Understanding the diffusion process through code",
    "text": "2 Understanding the diffusion process through code\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 of the series.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nThe code below is a stripped-down version of what is present in the StableDiffusionPipeline.from_pretrained function to show the important parts of the diffusion process.\n\ndef prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet‚Äôs see if the function works as intended.\n\nimages = prompt_2_img([\"A dog wearing a hat\", \"a photograph of an astronaut riding a horse\"], save_int=False)\nfor img in images:display(img)\n\n\n\n\n\n\n\n\n\n\nLooks like it is working! So let‚Äôs take a deeper dive at the hyper-parameters of the function.  1. prompt - this is the textual prompt we pass through to generate an image. Similar to the pipe(prompt) function we saw in part 1  2. g or guidance scale - It‚Äôs a value that determines how close the image should be to the textual prompt. This is related to a technique called Classifier free guidance which improves the quality of the images generated. The higher the value of the guidance scale, more close it will be to the textual prompt  3. seed - This sets the seed from which the initial Gaussian noisy latents are generated  4. steps - Number of de-noising steps taken for generating the final latents.  5. dim - dimension of the image, for simplicity we are currently generating square images, so only one value is needed  6. save_int - This is optional, a boolean flag, if we want to save intermediate latent images, helps in visualization.\nLet‚Äôs visualize this process of generation from noise to the final image.\n\n\nCode\n## Creating image through prompt_2_img modified function\nimages = prompt_2_img([\"A dog wearing a hat\"], save_int=True)\n\n## Converting intermediate images to video\n!ffmpeg -v 1 -y -f image2 -framerate 20 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n\n## Deleting intermediate images\nshutil.rmtree(f'./steps/')\n\n## Displaying video output\nmp4 = open('out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=600 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n\n\n\n\n\n\n\n      \n\n\n\n\nFig 3: The de-noisation steps visualization."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview and breaks the code to the bare minimum so that we can understand each component. Now that we have the minimum code implemented, in the next post we will see make some tweaks to the mk_img function to add additional functionality i.e., img2img pipeline and negative prompt.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Putting everything together",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "",
    "text": "An introduction to negative prompting and image to image stable diffusion pipeline using ü§ó hugging face diffusers library.\nThis is my fourth post of the Stable diffusion series, if you haven‚Äôt checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using ü§ó Hugging Face - Putting everything together\nIn previous posts, I went over all the key components of Stable Diffusion and how to get a prompt to image pipeline working. In this post, I will show how to edit the prompt to image function to add additional functionality to our Stable diffusion pipeline i.e., Negative prompting and Image to Image pipeline. Hopefully, this will provide enough motivation to play around with this function and conduct your research."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "1 Variation 1: Negative Prompt",
    "text": "1 Variation 1: Negative Prompt\n\n1.1 What is negative prompting?\nA negative prompt is an additional capability we can add to our model to tell the stable diffusion model what we don‚Äôt want to see in the generated image. This feature is popular to remove anything a user doesn‚Äôt want to see from the original generated image.\n\n\n\nFig. 2: Negative prompt example\n\n\n\n\n1.2 Understanding negative prompting through code\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\n\nNow we are going to change the prompt_2_img function from part 3 by passing an additional function neg_prompts. The way negative prompt works is by using user-specified text instead of an empty string for unconditional embedding(uncond) when doing sampling.\n\n\n\nFig. 3: Negative prompt code change\n\n\nSo let‚Äôs make this change and update our prompt_2_img function.\n\ndef prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"] * bs, text.shape[1])\n    else: uncond =  text_enc(neg_prompts, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet‚Äôs see if the function works as intended.\n\n## Image without neg prompt\nimages = [None, None]\nimages[0] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"\"],steps=50, save_int=False)[0]\nimages[1] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"White hat\"],steps=50, save_int=False)[0]\n    \n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate(images): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"A dog wearing a white hat\")\n    else: axs[c].set_title(f\"Neg prompt - white hat\")\n\n\n\n\n\nFig. 4: Visualization of negative prompting. Left SD generated with prompt ‚ÄúA dog wearing a white hat‚Äù and on right the same caption with negative prompt of ‚ÄúWhite hat‚Äù\n\nAs we can see it can be a really handy feature to fine-tune the image to your liking. You can also use it to generate a pretty realistic face by being really descriptive as this Reddit post. Let‚Äôs try it -\n\nprompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']\nneg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']\nimages = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)\nimages[0]\n\n\n\n\n\n\n\n\nFig. 5: An image generated using negative prompting.\n\n Pretty neat! I hope this gives you some ideas on how to get going with your own variations of stable diffusion. Now let‚Äôs look at another variation of Stable diffusion."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "2 Variation 2: Image to Image pipeline",
    "text": "2 Variation 2: Image to Image pipeline\n\n2.1 What is an image to image pipeline?\nAs seen above, prompt_2_img functions start generating an image from random gaussian noise, but what if we feed an initial seed image to guide the diffusion process? This is exactly how the image to image pipeline works. Instead of purely relying on text conditioning for the output image, we can use an initial seed image mix it with some noise (which can be guided by a strength parameter), and then run the diffusion loop.\n\n\n\nFig. 6: Image to image pipeline example.\n\n\n\n\n2.2 Understanding image to image prompting through code\nNow we are going to change the prompt_2_img function defined above. We will introduce two more parameters to our prompt_2_img_i2i function -  1. init_img: Which is going to be the Image object containing the seed image  2. strength: This parameter will take a value between 0 and 1. The higher the value less the final image is going to look similar to the seed image.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompt) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nInstead of using random noise, you will notice we use the strength parameter to figure out how much noise to add and also the number of steps to run the diffusion loop for. The amount of noise is calculated by multiplying strength(default = 0.8) with steps (default = 50) which is the 10th (50 - 50 * 0.8) step and running the diffusion loop for 40(50*0.8) remaining steps. Let‚Äôs load an initial image and pass it through the prompt_2_img_i2i function.\n\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\nimage = Image.open(p).convert('RGB').resize((512,512))\nprompt = [\"Wolf howling at the moon, photorealistic 4K\"]\nimages = prompt_2_img_i2i(prompts = prompt, init_img = image)\n\n\n\n\n/home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:146: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n\n\n\nCode\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([image, images[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image\")\n    else: axs[c].set_title(f\"Image 2 Image output\")\n\n\n\n\n\n\nFig. 7: Visualization of image to image pipeline. Left is initial image passed in img2img pipeline and right is the output of the img2img pipeline.\n\nWe can see our prompt_2_img_i2i function creates a pretty epic image from the initial sketch provided."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview of how to tweak the prompt_2_img function to add additional capabilities to your stable diffusion loop. The understanding of this lower-level function is useful for trying your own idea to improve stable diffusion or implement new papers which I might cover in my next post.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "title": "Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with üß® Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "title": "DiffEdit implementation using ü§ó Hugging Face",
    "section": "",
    "text": "An implementation of DIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE using ü§ó hugging face diffusers library.\nIn this post, I am going to implement a recent paper which came from researchers in Meta AI and Sorbonne Universite named DIFFEDIT. This blog will make more sense to people who are either familiar with stable diffusion process or are reading after four part series I made on Stable Diffusion -  1. Part 1 - Stable diffusion using ü§ó Hugging Face - Introduction.  2. Part 2 - Stable diffusion using ü§ó Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using ü§ó Hugging Face - Putting everything together  4. Part 4 - Stable diffusion using ü§ó Hugging Face - Variations of Stable Diffusion\nOriginally, this was the blog post I wanted to write about, but realized there is no single place for understanding Stable diffusion with code. Which is the reason I ended up creating the four part series as reference or pre-read material to understand this paper."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "title": "DiffEdit implementation using ü§ó Hugging Face",
    "section": "1 What is DiffEdit?",
    "text": "1 What is DiffEdit?\nIn simple terms, you can think of DiffEdit approach as a more controlled version of Image to Image pipeline. DiffEdit takes three inputs-  1. An input image  2. Caption - Describing the input image  3. Target Query - Describing the new image you want to generate\nand produces a modified version of the original image based on the query text. This process is particularly good if you want to make a slight tweak to the actual image without completely modifying it.\n\n\n\nFig. 1: Overview of Diff Edit.\n\n\nAs we can see from the image above only the fruits parts of the image was replaced with pears. Pretty amazing results!\nThe way authors explains they achieve it by introducing a mask generation module which determines which part of the image should be edited and then only perform text based diffusion conditioning on the masked part.\n\n\n\nFig. 2: From the paper DiffEdit[https://arxiv.org/pdf/2210.11427.pdf]. An approach to change an input image by providing caption text and new text.\n\n\nAs we can see from the image above taken from the paper, the authors create a mask from the input image which accurately determines the part of the image where fruits are present and generate a mask (shown in Orange) and then perform masked diffusion to replace fruits with pears. Reading further the authors provide a good visual representation of the whole DiffEdit process.\n\n\n\nFig. 3: Three steps of DiffEdit. Credit - Paper\n\n\nAs I was reading this paper, it seems generating the masking is the most important step and rest is just textual conditioning using diffusion process. The conditioning of an image using mask is a similar idea implemented in Hugging face In-Paint Pipeline. As suggested by the authors, ‚Äúthere are three steps to DiffEdit process -  Step 1: Add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results.  Step2: we encode the input image with DDIM, to estimate the latents corresponding to the input image  Step3: we perform DDIM decoding conditioned on the text query, using the inferred mask to replace the background with pixel values coming from the encoding process at the corresponding timestep‚Äù1\nIn next sections we will start implementing these ideas into actual code.\nLet‚Äôs start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the stable diffusion series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n\n## Helper functions\n\ndef load_artifacts():\n    '''\n    A function to load all diffusion artifacts\n    '''\n    vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n    unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)    \n    return vae, unet, tokenizer, text_encoder, scheduler\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nvae, unet, tokenizer, text_encoder, scheduler = load_artifacts()\n\n\nLet‚Äôs also download an image which we will use for the code implementation process.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\ninit_img"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#mask-creation-first-step-of-diffedit-process",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#mask-creation-first-step-of-diffedit-process",
    "title": "DiffEdit implementation using ü§ó Hugging Face",
    "section": "2 Mask Creation: First Step of DiffEdit process",
    "text": "2 Mask Creation: First Step of DiffEdit process\n\n\n\nFig. 4: Step 1 from the DiffEdit paper.\n\n\nThere is the more detailed explaination of Step 1 from the paper, here are the key parts mentioned -  1. Denoise image using different text conditioning, one using reference text and other using query text and take differences from the result. The idea is there is more changes in the differed part and not in the background of the image.  2. Repeat this differencing process 10 times and binarize the output by setting a 0.5 default to get the resultant mask. \nFirst, we will try to implement the paper exactly how it‚Äôs mentioned. We will modify the prompt_2_img_i2i function for this task to return latents instead of actually de-noised image.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n\n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).pred_original_sample\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents.detach().cpu()\n\nNext, we will make a create_mask function, which will take initial image, reference prompt and query prompt with the number of times we need to repeat the steps. In paper, author suggests that n=10 and strength of 0.5 works well in their experimentation. Hence, default for the function is adjusted to that. create_mask function performs the following steps -  1. Create two denoised version conditioned on reference text and query text and take a difference  2. Repeat this step n times  3. Take an average of these difference and standardize  4. Pick a threshold of 0.5 to binarize and create a mask\n\ndef create_mask(init_img, rp, qp, n=10, s=0.5):\n    ## Initialize a dictionary to save n iterations\n    diff = {}\n    for idx in range(n):\n        ## Creating denoised sample using reference / original text\n        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Creating denoised sample using query / target text\n        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Taking the difference \n        diff[idx] = (np.array(orig_noise)-np.array(query_noise))\n    \n    ## Creating a mask placeholder\n    mask = np.zeros_like(diff[0])\n    \n    ## Taking an average of 10 iterations\n    for idx in range(n):\n        mask += diff[idx] \n    mask = (mask / n)\n    ## Normalizing and binarizing\n    mask = (mask - mask.min()) / (mask.max() - mask.min()) > 0.5\n    \n    ## Returning the mask object\n    return (mask.mean(0) > 0).astype(\"uint8\")\n\nmask = create_mask(init_img=init_img, rp=[\"a horse image\"], qp=[\"a zebra image\"], n=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs visualize the generated mask over the image.\n\n\nCode\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  \n)\n\n\n<matplotlib.image.AxesImage at 0x7f037194c0d0>\n\n\n\n\n\n\nFig. 5: Masking visualization over our horse image.\n\nAs we can see it decently covers the horse and generate a good enough mask."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#masked-diffusion-step-2-and-3-of-diffedit-paper.",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#masked-diffusion-step-2-and-3-of-diffedit-paper.",
    "title": "DiffEdit implementation using ü§ó Hugging Face",
    "section": "3 Masked Diffusion: Step 2 and 3 of DiffEdit paper.",
    "text": "3 Masked Diffusion: Step 2 and 3 of DiffEdit paper.\n\n\n\nFig. 6: Step 2 and 3 from the DiffEdit paper.\n\n\n\ndef prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.5, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    rtext = text_enc(rp) \n    qtext = text_enc(qp)\n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"], rtext.shape[1])\n    emb = torch.cat([uncond, rtext, qtext])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Converting mask to torch tensor\n    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n            \n        # Performing Guidance\n        rpred = u + g*(rt-u)\n        qpred = u + g*(qt-u)\n\n        # Conditioning  the latents\n        rlatents = scheduler.step(rpred, ts, latents).prev_sample\n        qlatents = scheduler.step(qpred, ts, latents).prev_sample\n        latents = mask*qlatents + (1-mask)*rlatents\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents_to_pil(latents)\n\n\noutput = prompt_2_img_diffedit(\n    rp = [\"a horse image\"], \n    qp=[\"a zebra image\"],\n    init_img=init_img, \n    mask = mask, \n    g=7.5, seed=100, strength =0.5, steps=50, dim=512)\n\n\n\n\n\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([init_img, output[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image \")\n    else: axs[c].set_title(f\"DiffEdit output \")"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "title": "DiffEdit implementation using ü§ó Hugging Face",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nI hope this gives a good overview of how to tweak the prompt_2_img function to add additional capabilities to your stable diffusion loop. The understanding of this lower-level function is useful for trying your own idea to improve stable diffusion or implement new papers which I might cover in my next post.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "talks/2017-09-13-introduction-to-keras/index.html",
    "href": "talks/2017-09-13-introduction-to-keras/index.html",
    "title": "Introduction to Keras",
    "section": "",
    "text": "Presentation and codes on ‚ÄúIntroduction of Keras‚Äù. This was material presented by me in Analyze This! meetup on 13th September, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting a real time object detection application using deep learning and openCV."
  },
  {
    "objectID": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "href": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "title": "Deep dive in Hierarchical clustering",
    "section": "",
    "text": "Presentation and codes on ‚ÄúDeep dive in Hierarchical Clustering‚Äù. This was material presented by me in Analyze This! meetup on 11th October, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2017-11-08-Transfer-Learning/index.html",
    "href": "talks/2017-11-08-Transfer-Learning/index.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Presentation and codes on ‚ÄúTransfer learning‚Äù. This was material presented by me in Analyze This! meetup on 8th November, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2018-10-18-msba_devops/index.html",
    "href": "talks/2018-10-18-msba_devops/index.html",
    "title": "DevOps for Data scientist",
    "section": "",
    "text": "Conducted a DevOps for data scientist workshop for 2019 batch of Masters in Business Analytics student in Carlson school of management. The workshop was attended by 100+ students.\nLink to workshop content\nWorkshop Content\n\nWhat is DevOps? Why is it needed?\nModel building using cookbooks in Python\nVersion control using GIT\nDeploying Model as an API using Flask and microservices framework\nPackaging applications using Docker\nScaling and Deploying applications in Google cloud using Docker and Kubernetes"
  },
  {
    "objectID": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "href": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "title": "Model building, Explainability, and Deployment",
    "section": "",
    "text": "Presentation and codes on ‚ÄúModel building, Explainability, and Deployment‚Äù. This was material presented by me in Social Data science meetup on 28th July, 2018\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Sharing our insights from deploying OrcaHello - an open source, AI-assisted 24x7 hydrophone monitoring & Southern Resident Killer Whale alert system in Puget Sound.\n\nYoutube Link - (1:10:00)\nAI For Orcas Webiste"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML in the wild\n\n\n0 min\n\n\n\nDeep Learning\n\n\nAudio\n\n\nVision\n\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevOps for Data scientist\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nOct 18, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nCarlson School of Management, University of Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel building, Explainability, and Deployment\n\n\n0 min\n\n\n\nMLOps\n\n\nExplainability\n\n\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nVeritas Technologies LLC, 2815 Cleveland Ave N Roseville, MN 55113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning\n\n\n0 min\n\n\n\nDeep Learning\n\n\n\n\n\n\n\nNov 8, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep dive in Hierarchical clustering\n\n\n0 min\n\n\n\nClustering\n\n\n\n\n\n\n\nOct 11, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Keras\n\n\n0 min\n\n\n\nKeras\n\n\n\n\n\n\n\nSep 3, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]