[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML in the wild\n\n1 min\n\n\nDeep Learning\n\nAudio\n\nVision\n\nFastAI\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\nLocation\n\n\nUS\n\n\n\n\n\nVenue\n\n\nMeridian Winter Webinar Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevOps for Data scientist\n\n1 min\n\n\nMLOps\n\nExplainability\n\n\n\n\n\n\n\nOct 18, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\n\nVenue\n\n\nCarlson School of Management, University of Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel building, Explainability, and Deployment\n\n1 min\n\n\nMLOps\n\nExplainability\n\n\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\n\nVenue\n\n\nVeritas Technologies LLC, 2815 Cleveland Ave N Roseville, MN 55113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning\n\n1 min\n\n\nDeep Learning\n\n\n\n\n\n\n\nNov 8, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep dive in Hierarchical clustering\n\n1 min\n\n\nClustering\n\n\n\n\n\n\n\nOct 11, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Keras\n\n1 min\n\n\nKeras\n\n\n\n\n\n\n\nSep 3, 2017\n\n\n\n\n\n\nLocation\n\n\nMinneapolis, Minnesota, US\n\n\n\n\n\nVenue\n\n\nKeller Hall, 200 Union Street SE Minneapolis, MN, Room 3-180\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "href": "talks/2018-7-28-Model-Building-Explainability-Deployment/index.html",
    "title": "Model building, Explainability, and Deployment",
    "section": "",
    "text": "Presentation and codes on “Model building, Explainability, and Deployment”. This was material presented by me in Social Data science meetup on 28th July, 2018\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2017-11-08-Transfer-Learning/index.html",
    "href": "talks/2017-11-08-Transfer-Learning/index.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Presentation and codes on “Transfer learning”. This was material presented by me in Analyze This! meetup on 8th November, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2017-09-13-introduction-to-keras/index.html",
    "href": "talks/2017-09-13-introduction-to-keras/index.html",
    "title": "Introduction to Keras",
    "section": "",
    "text": "Presentation and codes on “Introduction of Keras”. This was material presented by me in Analyze This! meetup on 13th September, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting a real time object detection application using deep learning and openCV."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "",
    "text": "A guide on why technical blogging is one of the best investments you can make in your career, along with practical tips to get started.\nOne of my favorite hobbies outside of work is sharing technical knowledge through blog posts. Over the years, writing has opened doors I never expected: speaking at AI meetups, opportunities to author books with famous publishers, and countless LinkedIn conversations that turned into opportunities. In this post, I want to share why I think every technical person should consider blogging, and give you some practical tips to get started."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#increase-your-visibility-and-reach",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#increase-your-visibility-and-reach",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "1. Increase your visibility and reach",
    "text": "1. Increase your visibility and reach\nMany people have reached out to me through LinkedIn or email asking about topics I’ve written about. Sometimes these turn into job opportunities. Blogs are like resumes, but better. They show what you actually know, not just where you’ve worked."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-helps-you-learn",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-helps-you-learn",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "2. It helps you learn",
    "text": "2. It helps you learn\nOrganizing knowledge forces you to synthesize your own ideas. One reliable test of whether you understand something is whether you can explain it to someone else. Writing a blog post is a great way to do that. I often discover gaps in my own understanding while trying to explain a concept clearly."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-improves-your-writing-skills",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-improves-your-writing-skills",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "3. It improves your writing skills",
    "text": "3. It improves your writing skills\nThe more you write, the more comfortable you become expressing your ideas. This carries over to work emails, design docs, and presentations."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#establish-yourself-as-an-expert",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#establish-yourself-as-an-expert",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "4. Establish yourself as an expert",
    "text": "4. Establish yourself as an expert\nThrough my writing, I’ve landed opportunities to present at local AI meetups, write books with well-known publishers, and review technical books before their release. None of this would have happened without a public body of work people could find."
  },
  {
    "objectID": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-saves-time-dry-principle",
    "href": "posts/2025-12-13-WhyYouShouldBlog/WhyYouShouldBlog.html#it-saves-time-dry-principle",
    "title": "Why You Should Write Technical Blogs (and How to Start)",
    "section": "5. It saves time (DRY principle)",
    "text": "5. It saves time (DRY principle)\nDon’t Repeat Yourself applies to explanations too. Instead of spending two hours giving someone an overview of a topic, I can point them to an article. I also refer back to my own posts as notes, which saves me time when I need to refresh my memory on something I worked on years ago."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html",
    "title": "privateGPT Walkthrough",
    "section": "",
    "text": "A code walkthrough of privateGPT repo on how to build your own offline GPT Q&A system.\nLarge Language Models (LLMs) have surged in popularity, pushing the boundaries of natural language processing. OpenAI’s GPT-3.5 is a prime example, revolutionizing our technology interactions and sparking innovation. Particularly, LLMs excel in building Question Answering applications on knowledge bases. In this blog, we delve into the top trending GitHub repository for this week: the PrivateGPT repository and do a code walkthrough."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#ingestion-pipeline",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#ingestion-pipeline",
    "title": "privateGPT Walkthrough",
    "section": "3.1 Ingestion Pipeline",
    "text": "3.1 Ingestion Pipeline\nLet’s delve into the ingestion pipeline for a closer examination. The ingestion pipeline encompasses the following steps:\n\nIdentifying files with various extensions and retrieving all the knowledge base from the source directory.\nSplitting the documents into smaller chunks based on the parameters of chunk_size and chunk_overlap.\nInitializing the Huggingfaceembeddings module of langchain. This involves loading a pre-trained language model from the sentence_transformers library.\nInitializing the Chroma database from langchain.vectorstores. This step involves taking the chunked text and the initialized embedding model and saving it in the embedding database on disk.\n\n\n\n\nFig.5: Ingestion Pipeline\n\nLet’s look at these steps one by one.\n\n3.1.1 Identifying and loading files from the source directory\nFirst, we import the required libraries and various text loaders from langchain.document_loaders.\n\nimport os\nimport glob\nfrom typing import List\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom langchain.document_loaders import (\n    CSVLoader,\n    EverNoteLoader,\n    PDFMinerLoader,\n    TextLoader,\n    UnstructuredEmailLoader,\n    UnstructuredEPubLoader,\n    UnstructuredHTMLLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredODTLoader,\n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader,\n)\nfrom langchain.docstore.document import Document\n\nNext, we define the mapping b/w each extension and their respective langchain document loader. You can read document loader documentation for more available loaders.\n\n# Map file extensions to document loaders and their arguments\nLOADER_MAPPING = {\n    \".csv\": (CSVLoader, {}),\n    \".doc\": (UnstructuredWordDocumentLoader, {}),\n    \".docx\": (UnstructuredWordDocumentLoader, {}),\n    \".enex\": (EverNoteLoader, {}),\n    \".epub\": (UnstructuredEPubLoader, {}),\n    \".html\": (UnstructuredHTMLLoader, {}),\n    \".md\": (UnstructuredMarkdownLoader, {}),\n    \".odt\": (UnstructuredODTLoader, {}),\n    \".pdf\": (PDFMinerLoader, {}),\n    \".ppt\": (UnstructuredPowerPointLoader, {}),\n    \".pptx\": (UnstructuredPowerPointLoader, {}),\n    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n}\n\nNext, we define our single document loader.\n\ndef load_single_document(file_path: str) -&gt; Document:\n    ## Find extension of the file\n    ext = \".\" + file_path.rsplit(\".\", 1)[-1] \n    if ext in LOADER_MAPPING: \n        # Find the appropriate loader class and arguments\n        loader_class, loader_args = LOADER_MAPPING[ext] \n        # Invoke the instance of document loader\n        loader = loader_class(file_path, **loader_args) \n        ## Return the loaded document\n        return loader.load()[0] \n    raise ValueError(f\"Unsupported file extension '{ext}'\")\n    \ngit_dir = \"../../../../privateGPT/\"\nloaded_document = load_single_document(git_dir+'source_documents/state_of_the_union.txt')\nprint(f'Type of loaded document {type(loaded_document)}')\nloaded_document\n\nType of loaded document &lt;class 'langchain.schema.Document'&gt;\n\n\nDocument(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield – he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs. \\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.  \\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced. \\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere’s been a law on the books for almost a century \\nto make sure taxpayers’ dollars support American jobs and businesses. \\n\\nEvery Administration says they’ll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it’s so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. \\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that’s just the beginning. \\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they’re waiting for is for you to pass this bill. \\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere’s something happening in America. \\n\\nJust look around and you’ll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” \\n\\nIt’s time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That’s why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo—we have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let’s make it in America. \\n\\nEconomists call it “increasing the productive capacity of our economy.” \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: \\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again. \\n\\nThird – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work. \\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that’s my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19. \\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \\n\\nHere are four common sense steps as we move forward safely.  \\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection. \\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \\n\\nThe scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do. \\n\\nWe’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \\n\\nWe’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \\n\\nAnd we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  \\n\\nIf you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward. \\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.   \\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \\n\\nSecond – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. \\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.  \\n\\nThird – we can end the shutdown of schools and businesses. We have the tools we need. \\n\\nIt’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \\n\\nWe’re doing that here in the federal government. The vast majority of federal workers will once again work in person. \\n\\nOur schools are open. Let’s keep it that way. Our kids need to be in school. \\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks. \\n\\nOf course, continuing this costs money. \\n\\nI will soon send Congress a request. \\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.     \\n\\nWe’ve sent 475 Million vaccine doses to 112 countries, more than any other nation. \\n\\nAnd we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. \\n\\nSo let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced. \\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \\n\\nBan assault weapons and high-capacity magazines. \\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued. \\n\\nThese laws don’t infringe on the Second Amendment. They save lives. \\n\\nThe most fundamental right in America is the right to vote – and to have it counted. And it’s under assault. \\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours. \\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \\n\\nRevise our laws so businesses have the workers they need and families don’t wait decades to reunite. \\n\\nIt’s not only the right thing to do—it’s the economically smart thing to do. \\n\\nThat’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \\n\\nLet’s get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. \\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. \\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers. \\n\\nOne was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I’m committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn’t know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle—we are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let’s end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America–second only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity. \\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})\n\n\nThe load_single_document function accomplishes the following steps:\n\nExtracts the file extension from the given file path.\nRetrieves the corresponding document loader and its arguments from the previously defined LOADER_MAPPING dictionary.\nCreates an instance of the appropriate document loader.\nLoads the document using the instantiated loader.\nReturns the loaded document.\n\nWe can see that load_single_document returns a document of type langchain.schema.Document. Which according to the documentation consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data).\n\ndef load_documents(source_dir: str, ignored_files: List[str] = []) -&gt; List[Document]:\n    \"\"\"\n    Loads all documents from the source documents directory, ignoring specified files\n    \"\"\"\n    all_files = []\n    for ext in LOADER_MAPPING:\n        #Find all the files within source documents which matches the extensions in Loader_Mapping file\n        all_files.extend(\n            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n        )\n    \n    ## Filtering files from all_files if its in ignored_files\n    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n    \n    ## Spinning up resource pool\n    with Pool(processes=os.cpu_count()) as pool:\n        results = []\n        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n            # Load each document from filtered files list using load_single_document function\n            for i, doc in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n                results.append(doc)\n                pbar.update()\n    \n    return results\n\nThe load_single_documents function carries out the following steps: 1. Initializes an empty dictionary called all_files.  2. For each extension in the LOADER_MAPPING dictionary, it searches for all the files with that extension in the source directory and adds them to the all_files list.  3. Creates a new list named filtered_files by removing the files listed in the ignored_files list from the all_files list. 4. Executes a parallel loading operation on all the files in the filtered_files list using the load_single_document function, and appends the results to the results list. 5. Returns the list of loaded documents.\n\nloaded_documents = load_documents(git_dir+'source_documents')\nprint(f\"Length of loaded documents: {len(loaded_documents)}\")\nloaded_documents[0]\n\nLoading new documents: 100%|█████████████████████| 1/1 [00:00&lt;00:00, 246.69it/s]\n\n\nLength of loaded documents: 1\n\n\n\n\n\nDocument(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield – he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs. \\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.  \\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced. \\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere’s been a law on the books for almost a century \\nto make sure taxpayers’ dollars support American jobs and businesses. \\n\\nEvery Administration says they’ll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it’s so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. \\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that’s just the beginning. \\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they’re waiting for is for you to pass this bill. \\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere’s something happening in America. \\n\\nJust look around and you’ll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” \\n\\nIt’s time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That’s why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo—we have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let’s make it in America. \\n\\nEconomists call it “increasing the productive capacity of our economy.” \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: \\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again. \\n\\nThird – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work. \\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that’s my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19. \\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \\n\\nHere are four common sense steps as we move forward safely.  \\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection. \\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \\n\\nThe scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do. \\n\\nWe’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \\n\\nWe’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \\n\\nAnd we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  \\n\\nIf you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward. \\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.   \\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \\n\\nSecond – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. \\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.  \\n\\nThird – we can end the shutdown of schools and businesses. We have the tools we need. \\n\\nIt’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \\n\\nWe’re doing that here in the federal government. The vast majority of federal workers will once again work in person. \\n\\nOur schools are open. Let’s keep it that way. Our kids need to be in school. \\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks. \\n\\nOf course, continuing this costs money. \\n\\nI will soon send Congress a request. \\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.     \\n\\nWe’ve sent 475 Million vaccine doses to 112 countries, more than any other nation. \\n\\nAnd we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. \\n\\nSo let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced. \\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \\n\\nBan assault weapons and high-capacity magazines. \\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued. \\n\\nThese laws don’t infringe on the Second Amendment. They save lives. \\n\\nThe most fundamental right in America is the right to vote – and to have it counted. And it’s under assault. \\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours. \\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \\n\\nRevise our laws so businesses have the workers they need and families don’t wait decades to reunite. \\n\\nIt’s not only the right thing to do—it’s the economically smart thing to do. \\n\\nThat’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \\n\\nLet’s get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. \\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. \\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers. \\n\\nOne was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I’m committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn’t know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle—we are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let’s end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America–second only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity. \\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})\n\n\nYou can see we have loaded the state_of_the_union.txt file from the privateGPT repo. As this is the only file in that directory the length of loaded documents is one.\n\n\n3.1.2 Splitting the documents into smaller chunks\nNow we have seen how we can load multiple documents of different extensions using the load_documents function. The next step is to look at process_document function which loads and splits large documents into smaller chunks.\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nchunk_size = 500\nchunk_overlap = 50\n\ndef process_documents(source_dir: str, ignored_files: List[str] = []) -&gt; List[Document]:\n    \"\"\"\n    Load documents and split in chunks\n    \"\"\"\n    print(f\"Loading documents from {source_dir}\")\n    documents = load_documents(source_dir, ignored_files)\n    if not documents:\n        print(\"No new documents to load\")\n        exit(0)\n    print(f\"Loaded {len(documents)} new documents from {source_dir}\")\n    ## Load text splitter\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    ## Split text\n    texts = text_splitter.split_documents(documents)\n    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n    return texts\n\nprocessed_documents = process_documents(git_dir+'source_documents')\n\nLoading documents from ../../../../privateGPT/source_documents\n\n\nLoading new documents: 100%|█████████████████████| 1/1 [00:00&lt;00:00, 315.74it/s]\n\n\nLoaded 1 new documents from ../../../../privateGPT/source_documents\nSplit into 90 chunks of text (max. 500 tokens each)\n\n\n\n\n\nThe process_documents function performs the following steps:\n\nLoads all the documents from the source_dir directory using the load_documents function.\nInitializes an instance of RecursiveCharacterTextSplitter from the langchain.text_splitter module, providing the chunk_size and chunk_overlap parameters. This class is responsible for splitting a list of documents into smaller overlapping chunks. [RecursiveCharacterTextSplitter documentation].\nUses the split_documents method of the RecursiveCharacterTextSplitter instance to split the loaded documents into smaller chunks.\nReturns the resulting list of the smaller document chunks.\n\n\n\n3.1.3 Initializing the embedding model\nNext, we load our embedding module which converts the smaller document chunks from previous steps to embeddings.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nEMBEDDINGS_MODEL_NAME = \"all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n\nprint(\"Testing on a single query.\")\nembedded_vector = embeddings.embed_query(\"What is your name?\")\nprint(f\"Size of embedded vector: {len(embedded_vector)}\")\n\nTesting on a single query.\nSize of embedded vector: 384\n\n\nThe given code snippet carries out the following steps:\n\nImports the HuggingFaceEmbeddings function from the langchain.embeddings module. This function is responsible for loading and encapsulating the SentenceTransformers embeddings, which are used for generating dense vector representations of sentences. You can refer to the HuggingFaceEmbeddings documentation for more details.\nLoads the all-MiniLM-L6-v2 model from the sentence_transformers library. This model is specifically designed to map sentences and paragraphs into a 384-dimensional dense vector space. It is commonly utilized for tasks such as semantic search and similarity analysis.\n\nWe can see that our embedded vector on a sample query returns a 384 dimension vector.\n\n\n3.1.4 Embed smaller text and save it in the vector database\nThe next step involves utilizing the document chunks and the embedding model to store the documents and their corresponding embeddings in a vector database.\n\nfrom chromadb.config import Settings\nfrom langchain.vectorstores import Chroma\n\nPERSIST_DIRECTORY= git_dir+\"db\"\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=PERSIST_DIRECTORY,\n        anonymized_telemetry=False\n)\n## Create the embedding database\ndb = Chroma.from_documents(processed_documents, embeddings, persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\ndb.persist()\n\nUsing embedded DuckDB with persistence: data will be stored in: ../../../../privateGPT/db\n\n\nThe given code snippet performs the following operations:\n\nIt imports the Settings class from the chromadb.config module and the Chroma class from the langchain.vectorstores module.\nIt creates an instance of the Settings class named CHROMA_SETTINGS, providing several configuration parameters:\n\nchroma_db_impl is set to 'duckdb+parquet', specifying the implementation to be used for the Chroma vector database.\npersist_directory is set to the PERSIST_DIRECTORY variable defined earlier, indicating the directory where the vector database will be saved.\nanonymized_telemetry is set to False, indicating whether anonymized telemetry data should be collected.\n\nIt creates a vector database by calling the Chroma.from_documents() method. This method takes the following arguments:\n\nprocessed_documents: The list of processed documents obtained from the previous step.\nembeddings: The embeddings object/model used to generate the document embeddings.\npersist_directory: The directory where the vector database will be persisted, specified by the PERSIST_DIRECTORY variable.\nclient_settings: The settings object (CHROMA_SETTINGS) containing configuration parameters for the vector database.\n\nWe use db.persist() to store the index for future retrieval task\n\n\n## Test the semantic retrieval \ndb.similarity_search(query=\"What is the American Rescue Plan?\", k= 4)\n\n[Document(page_content='The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})]\n\n\n\ndb = None\n\nTo test the retrieval of semantic similarity, we can use the similarity_search function. similarity_search function takes a text query as input and returns the top k=4 document chunks from the vector database."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#question-answer-interface",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#question-answer-interface",
    "title": "privateGPT Walkthrough",
    "section": "3.2 Question & Answer Interface",
    "text": "3.2 Question & Answer Interface\nLet’s explore the Q&A interface in more detail. The Q&A interface consists of the following steps:\n\nLoad the vector database and prepare it for the retrieval task.\nLoad a pre-trained Large language model from LlamaCpp or GPT4ALL.\nPrompt the user with a query and generate a response using the RetrievalQA pipeline from langchain.chains.\n\n\n\n\nFig.6: Question Answering Pipeline\n\nLet’s look at these steps one by one.\n\n3.2.1 Load the vector database\nFirst, we import the required libraries.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom chromadb.config import Settings\ngit_dir = \"../../../../privateGPT/\"\nPERSIST_DIRECTORY= git_dir+\"db\"\nEMBEDDINGS_MODEL_NAME = \"all-MiniLM-L6-v2\"\n\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=PERSIST_DIRECTORY,\n        anonymized_telemetry=False\n)\n\nembeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\ndb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\nretriever = db.as_retriever()\n\nUsing embedded DuckDB with persistence: data will be stored in: ../../../../privateGPT/db\n\n\nThe given code snippet carries out the following steps:\n\nLoads the embeddings using the HuggingFaceEmbeddings function, which was previously used to create the embedding store.\nInstantiates a Chroma vector database that was created earlier.\nSets the vector database in retrieval mode.\n\n\n## Testing retriever\nretriever.vectorstore.similarity_search(query = \"What is Amercian rescue plan?\")\n\n[Document(page_content='The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'}),\n Document(page_content='Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.', metadata={'source': '../../../../privateGPT/source_documents/state_of_the_union.txt'})]\n\n\n\n\n3.2.2 Load a pre-trained Large language model.\n\nfrom langchain.llms import GPT4All\n\nMODEL_PATH = git_dir+\"models/ggml-gpt4all-j-v1.3-groovy.bin\" \nMODEL_N_CTX=1000\n\n# Prepare the LLM\nllm = GPT4All(model=MODEL_PATH, n_ctx=MODEL_N_CTX, backend='gptj', callbacks=None, verbose=False)\n\ngptj_model_load: loading model from '../../../../privateGPT/models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\ngptj_model_load: n_vocab = 50400\ngptj_model_load: n_ctx   = 2048\ngptj_model_load: n_embd  = 4096\ngptj_model_load: n_head  = 16\ngptj_model_load: n_layer = 28\ngptj_model_load: n_rot   = 64\ngptj_model_load: f16     = 2\ngptj_model_load: ggml ctx size = 4505.45 MB\ngptj_model_load: memory_size =   896.00 MB, n_mem = 57344\ngptj_model_load: ................................... done\ngptj_model_load: model size =  3609.38 MB / num tensors = 285\n\n\nThe code snippet above create an instance of the GPT4All class named llm, which represents the Language Model (LLM) using the GPT-4All model. The constructor of GPT4All takes the following arguments:  - model: The path to the GPT-4All model file specified by the MODEL_PATH variable.  - n_ctx: The context size or maximum length of input sequences specified by the MODEL_N_CTX variable.  - backend: The backend to use for the LLM. In this case, it is set to ‘gptj’.  - callbacks: The callbacks to be used during the LLM execution. In this case, it is set to None.  - verbose: A boolean flag indicating whether to print verbose output during LLM execution. In this case, it is set to False.\n\n\n3.2.3 Prompt the user with a query and generate a response\n\nfrom langchain.chains import RetrievalQA\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\nquery = \"What is American rescue plan?\"\nres = qa(query)\nanswer, docs = res['result'], res['source_documents']\n\n# Get the answer from the chain\n# Print the result\nprint(\"\\n\\n&gt; Question:\")\nprint(query)\nprint(\"\\n&gt; Answer:\")\nprint(answer)\n\n# Print the relevant sources used for the answer\nfor document in docs:\n    print(\"\\n&gt; \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n\n\n&gt; Question:\nWhat is American rescue plan?\n\n&gt; Answer:\n The American Rescue Plan is a program that provides funding to schools to hire teachers and help students make up for lost learning due to the COVID-19 pandemic. It also provides economic relief for tens of millions of Americans by helping them put food on their table, keep a roof over their heads, and cut the cost of health insurance. The plan also helps working people by providing breathing room and giving them a little breathing room. It is a program that helps millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums and combat climate change by cutting energy costs for families an average of $500 a year.\n\n&gt; ../../../../privateGPT/source_documents/state_of_the_union.txt:\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \n\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \n\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.\n\n&gt; ../../../../privateGPT/source_documents/state_of_the_union.txt:\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \n\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \n\nAnd as my Dad used to say, it gave people a little breathing room. \n\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.\n\n&gt; ../../../../privateGPT/source_documents/state_of_the_union.txt:\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \n\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.\n\n&gt; ../../../../privateGPT/source_documents/state_of_the_union.txt:\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \n\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.\n\n\nFirstly, an instance of the RetrievalQA class named qa is created using the from_chain_type method. The RetrievalQA class is a chain specifically designed for question-answering tasks over an index. Please refer to the documentation for further details. The from_chain_type method takes the following arguments:\n\nllm: The Language Model instance (llm) that was created previously.\nchain_type: A string representing the type of chain to be used. In this case, it is set to \"stuff\". There may be other available chain types specific to the question-answering scenario. Please consult the documentation for more information.\nretriever: An instance of a Chroma database used to retrieve relevant documents for the given query.\nreturn_source_documents: A boolean flag indicating whether to return the source documents along with the answer. In this case, it is set to True.\n\nNext, the qa instance is used to process a query. The Language Model (LLM) within the qa instance generates a response that includes the query, the answer, and the source documents used as context for generating the answer.\nFinally, the answer and source documents are printed out for display."
  },
  {
    "objectID": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#conclusion",
    "href": "posts/2023-05-22-PrivateGPTWalkthrough/privateGPTWalkthrough.html#conclusion",
    "title": "privateGPT Walkthrough",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nIn this blog post, we explored privateGPT, its implementation, and the code walkthrough for its ingestion pipeline and q&A interface. I hope this blog post has been valuable in understanding privateGPT and its implementation. I recommend my readers to try privateGPT on your own knowledge base.\nI hope you enjoyed reading it. If there is any feedback on the code or just the blog post, feel free to comment below or reach out on LinkedIn."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "",
    "text": "An implementation of DIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE using 🤗 hugging face diffusers library.\nIn this post, I am going to implement a recent paper that came from researchers in Meta AI and Sorbonne Universite named DIFFEDIT. This blog will make more sense to people who are either familiar with the stable diffusion process or are reading after four-part series I made on Stable Diffusion -  1. Part 1 - Stable diffusion using 🤗 Hugging Face - Introduction.  2. Part 2 - Stable diffusion using 🤗 Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using 🤗 Hugging Face - Putting everything together  4. Part 4 - Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion\nOriginally, this was the blog post I wanted to write about but realized there is no single place for understanding Stable diffusion with code. Which is the reason I ended up creating the four-part series as a reference or pre-read material to understand this paper."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#what-is-diffedit",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "1 What is DiffEdit?",
    "text": "1 What is DiffEdit?\nIn simple terms, you can think of DiffEdit approach as a more controlled version of the Image to Image pipeline. DiffEdit takes three inputs-  1. An input image  2. Caption - Describing the input image  3. Target Query - Describe the new image you want to generate\nand produce a modified version of the original image based on the query text. This process is particularly good if you want to make a slight tweak to the actual image without completely modifying it.\n\n\n\nFig. 1: Overview of Diff Edit.\n\n\nAs we can see from the image above only the fruits parts of the image were replaced with pears. Pretty amazing results!\nThe way the authors explain they achieve it is by introducing a mask generation module that determines which part of the image should be edited and then only perform text-based diffusion conditioning on the masked part.\n\n\n\nFig. 2: From the paper DiffEdit. An approach to change an input image by providing caption text and new text.\n\n\nAs we can see from the image above taken from the paper, the authors create a mask from the input image which accurately determines the part of the image where fruits are present and generate a mask (shown in Orange) and then perform masked diffusion to replace fruits with pears. Reading further the authors provide a good visual representation of the whole DiffEdit process.\n\n\n\nFig. 3: Three steps of DiffEdit. Credit - Paper\n\n\nAs I was reading this paper, it seems generating the masking is the most important step and the rest is just textual conditioning using the diffusion process. The conditioning of an image using the mask is a similar idea implemented in Hugging face In-Paint Pipeline. As suggested by the authors, “there are three steps to the DiffEdit process -  Step 1: Add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results.  Step2: we encode the input image with DDIM, to estimate the latents corresponding to the input image  Step3: we perform DDIM decoding conditioned on the text query, using the inferred mask to replace the background with pixel values coming from the encoding process at the corresponding timestep”1\nIn the next sections, we will start implementing these ideas into actual code.\nLet’s start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the stable diffusion series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n\n## Helper functions\n\ndef load_artifacts():\n    '''\n    A function to load all diffusion artifacts\n    '''\n    vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n    unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)    \n    return vae, unet, tokenizer, text_encoder, scheduler\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nvae, unet, tokenizer, text_encoder, scheduler = load_artifacts()\n\n\nLet’s also download an image which we will use for the code implementation process.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\ninit_img"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#diffedit-purist-implementation",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#diffedit-purist-implementation",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "2 DiffEdit: Purist implementation",
    "text": "2 DiffEdit: Purist implementation\nLet’s start by implementing the paper as closely as the authors suggested, hence the Purist implementation.\n\n2.1 Mask Creation: First Step of the DiffEdit process\n\n\n\nFig. 4: Step 1 from the DiffEdit paper.\n\n\nThere is a more detailed explanation of Step 1 from the paper, here are the key parts mentioned -  1. Denoise image using different text conditioning, one using reference text and the other using query text, and take differences from the result. The idea is there are more changes in the different parts and not in the background of the image.  2. Repeat this differencing process 10 times  3. Average out these differences and binarize for mask \n\n\n\n\n\n\nNote\n\n\n\nThe third step in mask creation (averaging and binarization) is not explained clearly in the paper and it took me a lot of experiments to get this right.\n\n\nFirst, we will try to implement the paper exactly as it’s mentioned. We will modify the prompt_2_img_i2i function for this task to return latents instead of rescaled and decoded de-noised images.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n\n        # Conditioning  the latents\n        #latents = scheduler.step(pred, ts, latents).pred_original_sample\n        latents = scheduler.step(pred, ts, latents).prev_sample\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents.detach().cpu()\n\nNext, we will make a create_mask function, which will take an initial image, reference prompt, and query prompt with the number of times we need to repeat the steps. In the paper, the author suggests that n=10 and a strength of 0.5 works well in their experimentation. Hence, the default for the function is adjusted to that. create_mask function performs the following steps -  1. Create two denoised latents, one conditioned on reference text and the second on query text, and take a difference of these latents  2. Repeat this step n times  3. Take an average of these differences and standardize  4. Pick a threshold of 0.5 to binarize and create a mask\n\ndef create_mask(init_img, rp, qp, n=10, s=0.5):\n    ## Initialize a dictionary to save n iterations\n    diff = {}\n    \n    ## Repeating the difference process n times\n    for idx in range(n):\n        ## Creating denoised sample using reference / original text\n        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Creating denoised sample using query / target text\n        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Taking the difference \n        diff[idx] = (np.array(orig_noise)-np.array(query_noise))\n    \n    ## Creating a mask placeholder\n    mask = np.zeros_like(diff[0])\n    \n    ## Taking an average of 10 iterations\n    for idx in range(n):\n        ## Note np.abs is a key step\n        mask += np.abs(diff[idx])  \n        \n    ## Averaging multiple channels \n    mask = mask.mean(0)\n    \n    ## Normalizing \n    mask = (mask - mask.mean()) / np.std(mask)\n    \n    ## Binarizing and returning the mask object\n    return (mask &gt; 0).astype(\"uint8\")\n\nmask = create_mask(init_img=init_img, rp=[\"a horse image\"], qp=[\"a zebra image\"], n=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s visualize the generated mask over the image.\n\n\nCode\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  \n)\n\n\n\n\n\n\n\n\n\n\nFig. 5: Masking visualization over our horse image.\n\nAs we can see above, the mask produced covers the horse portion well which is what we want.\n\n\n2.2 Masked Diffusion: Step 2 and 3 of DiffEdit paper.\n\n\n\nFig. 6: Step 2 and 3 from the DiffEdit paper.\n\n\nSteps 2 and 3 need to be implemented in the same loop. Simply put author is saying to condition the latents based on reference text for the non-masked part and on query text for the masked part.  Combine these two parts using this simple formula to create combined latents - \n\\[  \\hat{y}_{t} = My_{t} + (1-M)x_{t} \\]\n\ndef prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.7, steps=70, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    rtext = text_enc(rp) \n    qtext = text_enc(qp)\n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"], rtext.shape[1])\n    emb = torch.cat([uncond, rtext, qtext])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Converting mask to torch tensor\n    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n            \n        # Performing Guidance\n        rpred = u + g*(rt-u)\n        qpred = u + g*(qt-u)\n\n        # Conditioning  the latents\n        rlatents = scheduler.step(rpred, ts, latents).prev_sample\n        qlatents = scheduler.step(qpred, ts, latents).prev_sample\n        latents = mask*qlatents + (1-mask)*rlatents\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents_to_pil(latents)\n\nLet’s visualize the generated image.\n\noutput = prompt_2_img_diffedit(\n    rp = [\"a horse image\"], \n    qp=[\"a zebra image\"],\n    init_img=init_img, \n    mask = mask, \n    g=7.5, seed=100, strength =0.5, steps=70, dim=512)\n\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([init_img, output[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image \")\n    else: axs[c].set_title(f\"DiffEdit output\")\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 7: DiffEdit output visualization\n\n Let’s create a simple function for the masking and diffusion process.\n\ndef diffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=70, dim=512):\n    \n    ## Step 1: Create mask\n    mask = create_mask(init_img=init_img, rp=rp, qp=qp)\n    \n    ## Step 2 and 3: Diffusion process using mask\n    output = prompt_2_img_diffedit(\n        rp = rp, \n        qp=qp, \n        init_img=init_img, \n        mask = mask, \n        g=g, \n        seed=seed,\n        strength =strength, \n        steps=steps, \n        dim=dim)\n    return mask , output\n\nLet’s also create a visualization function for DiffEdit showing the original input image, masked image, and final output image.\n\ndef plot_diffEdit(init_img, output, mask):\n    ## Plotting side by side\n    fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n    \n    ## Visualizing initial image\n    axs[0].imshow(init_img)\n    axs[0].set_title(f\"Initial image\")\n    \n    ## Visualizing initial image\n    axs[2].imshow(output[0])\n    axs[2].set_title(f\"DiffEdit output\")\n    \n    ## Visualizing the mask \n    axs[1].imshow(np.array(init_img), cmap='gray') \n    axs[1].imshow(\n        Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n        cmap='cividis', \n        alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  \n    )\n    axs[1].set_title(f\"DiffEdit mask\")\n\nLet’s test this function on a few images.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask, output = diffEdit(init_img, rp = [\"a horse image\"], qp=[\"a zebra image\"])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 8: Purist implementation output example\n\nPerfect, let’s try another one.\n\np = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')\ninit_img = load_image(p)\nmask, output = diffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 9: Purist implementation output example"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#fastdiffedit-a-faster-diffedit-implementation",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#fastdiffedit-a-faster-diffedit-implementation",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "3 FastDiffEdit: A faster DiffEdit implementation",
    "text": "3 FastDiffEdit: A faster DiffEdit implementation\nNow we have seen the purist implementation, there are some improvements I suggest we can make to the original DiffEdit process in terms of speed and better results. Let’s call these improvements FastDiffEdit.\n\n3.1 Mask Creation: Fast DiffEdit masking process\nMy biggest issue with the current way of doing masking is that it takes too much time(~50 sec on A4500 GPU). My take is we don’t need to run a full diffusion loop to denoise the image but just use the U-net prediction of the original sample in one shot and increase the repetition to 20 times. In this case, we can improve the computation from 10*25 = 250 steps to 20 steps (12x less loop). Let’s see if this works in practice.\n\ndef prompt_2_img_i2i_fast(prompts, init_img, g=7.5, seed=100, strength =0.5, steps=50, dim=512):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"], text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # We need to scale the i/p latents to match the variance\n    inp = scheduler.scale_model_input(torch.cat([latents] * 2), timesteps)\n    # Predicting noise residual using U-Net\n    with torch.no_grad(): u,t = unet(inp, timesteps, encoder_hidden_states=emb).sample.chunk(2)\n         \n    # Performing Guidance\n    pred = u + g*(t-u)\n\n    # Zero shot prediction\n    latents = scheduler.step(pred, timesteps, latents).pred_original_sample\n    \n    # Returning the latent representation to output an array of 4x64x64\n    return latents.detach().cpu()\n\nLet’s create a new masking function that can take our prompt_2_img_i2i_fast function.\n\ndef create_mask_fast(init_img, rp, qp, n=20, s=0.5):\n    ## Initialize a dictionary to save n iterations\n    diff = {}\n    \n    ## Repeating the difference process n times\n    for idx in range(n):\n        ## Creating denoised sample using reference / original text\n        orig_noise = prompt_2_img_i2i_fast(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Creating denoised sample using query / target text\n        query_noise = prompt_2_img_i2i_fast(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]\n        ## Taking the difference \n        diff[idx] = (np.array(orig_noise)-np.array(query_noise))\n    \n    ## Creating a mask placeholder\n    mask = np.zeros_like(diff[0])\n    \n    ## Taking an average of 10 iterations\n    for idx in range(n):\n        ## Note np.abs is a key step\n        mask += np.abs(diff[idx])  \n        \n    ## Averaging multiple channels \n    mask = mask.mean(0)\n    \n    ## Normalizing \n    mask = (mask - mask.mean()) / np.std(mask)\n    \n    ## Binarizing and returning the mask object\n    return (mask &gt; 0).astype(\"uint8\")\n\nLet’s see if this new masking function produces a good mask.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask = create_mask_fast(init_img=init_img, rp=[\"a horse image\"], qp=[\"a zebra image\"], n=20)\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  \n)\n\n\n\n\n\n\n\n\n\nFig. 10: FastDiffEdit masking visualization over our horse image.\n\n As we can see above the masking is improved and compute time has reduced from ~50 seconds to ~10 secs on my machine(5x improvement!).\nLet’s improve our masking by adding a cv2 trick. This will just smooth out the masking a little bit more.\n\nimport cv2\ndef improve_mask(mask):\n    mask  = cv2.GaussianBlur(mask*255,(3,3),1) &gt; 0\n    return mask.astype('uint8')\n\n\nmask = improve_mask(mask)\nplt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'\nplt.imshow(\n    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size\n    cmap='cividis', \n    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  \n)\n\n\n\n\n\n\n\n\n\nFig. 11: Improved FastDiffEdit masking visualization over our horse image with cv2 Gaussian blur trick.\n\n As we can see above the masking has become a bit more smooth and covers more area.\n\n\n3.2 Masked Diffusion: Replace with 🤗 inpaint pipeline\nSo, instead of using our function to perform the masked diffusion, there is a special pipeline in 🤗 diffusers library called inpaint pipeline. Which takes the query prompt, initial image, and generated mask to generate the output image. Let’s start by loading in the inpaint pipeline.\n\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n\n\n\nLet’s use the inpaint pipeline with our generated mask and image.\n\npipe(\n    prompt=[\"a zebra image\"], \n    image=init_img, \n    mask_image=Image.fromarray(mask*255).resize((512,512)), \n    generator=torch.Generator(\"cuda\").manual_seed(100),\n    num_inference_steps = 20\n).images[0]\nimage\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 12: In-paint pipeline output.\n\n As we can see above, inpaint pipeline creates a more realistic zebra image. Let’s create a simple function for the masking and diffusion process.\n\ndef fastDiffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=20, dim=512):\n    \n    ## Step 1: Create mask\n    mask = create_mask_fast(init_img=init_img, rp=rp, qp=qp, n=20)\n    \n    ## Improve masking using CV trick\n    mask = improve_mask(mask)\n    \n    ## Step 2 and 3: Diffusion process using mask\n    output = pipe(\n        prompt=qp, \n        image=init_img, \n        mask_image=Image.fromarray(mask*255).resize((512,512)), \n        generator=torch.Generator(\"cuda\").manual_seed(100),\n        num_inference_steps = steps\n    ).images\n    return mask , output\n\nLet’s test this function on a few images.\n\np = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')\ninit_img = load_image(p)\nmask, output = fastDiffEdit(init_img, rp = [\"a horse image\"], qp=[\"a zebra image\"])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 13: FastDiffEdit output example\n\nPerfect, let’s try another one.\n\np = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')\ninit_img = load_image(p)\nmask, output = fastDiffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])\nplot_diffEdit(init_img, output, mask)\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 14: FastDiffEdit output example"
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#conclusion",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we implemented the DiffEdit paper as the author mentioned and then we proposed improvements to the method to create FastDiffEdit which speeds up computation times up to 5 times.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#footnotes",
    "href": "posts/2022-11-17-DiffEdit/2022-11-17-DiffEdit.html#footnotes",
    "title": "Stable diffusion using 🤗 Hugging Face - DiffEdit paper implementation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE↩︎"
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html",
    "title": "Stable diffusion using 🤗 Hugging Face - Putting everything together",
    "section": "",
    "text": "An introduction to the diffusion process using 🤗 hugging face diffusers library.\nThis is my third post of the Stable diffusion series, if you haven’t checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using 🤗 Hugging Face - Introduction.  2. Part 2 - Stable diffusion using 🤗 Hugging Face - Looking under the hood.\nIn previous posts, I went over showing how to install 🤗 diffuser library to start generating your own AI images and key components of the stable diffusion pipeline i.e., CLIP text encoder, VAE, and U-Net. In this post, we will try to put these key components together and do a walk-through of the diffusion process which generates the image."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#overview---the-diffusion-process",
    "title": "Stable diffusion using 🤗 Hugging Face - Putting everything together",
    "section": "1 Overview - The Diffusion Process",
    "text": "1 Overview - The Diffusion Process\nThe stable diffusion model takes the textual input and a seed. The textual input is then passed through the CLIP model to generate textual embedding of size 77x768 and the seed is used to generate Gaussian noise of size 4x64x64 which becomes the first latent image representation.\n\n\n\n\n\n\nNote\n\n\n\nYou will notice that there is an additional dimension mentioned (1x) in the image like 1x77x768 for text embedding, that is because it represents the batch size of 1.\n\n\n\n\n\nFig. 2: The diffusion process.\n\n\nNext, the U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings. The output of the U-Net is predicted noise residual, which is then used to compute conditioned latents via a scheduler algorithm. This process of denoising and text conditioning is repeated N times (We will use 50) to retrieve a better latent image representation. Once this process is complete, the latent image representation (4x64x64) is decoded by the VAE decoder to retrieve the final output image (3x512x512).\n\n\n\n\n\n\nNote\n\n\n\nThis iterative denoising is an important step for getting a good output image. Typical steps are in the range of 30-80. However, there are recent papers that claim to reduce it to 4-5 steps by using distillation techniques."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#understanding-the-diffusion-process-through-code",
    "title": "Stable diffusion using 🤗 Hugging Face - Putting everything together",
    "section": "2 Understanding the diffusion process through code",
    "text": "2 Understanding the diffusion process through code\nLet’s start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 of the series.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\nThe code below is a stripped-down version of what is present in the StableDiffusionPipeline.from_pretrained function to show the important parts of the diffusion process.\n\ndef prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    uncond =  text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet’s see if the function works as intended.\n\nimages = prompt_2_img([\"A dog wearing a hat\", \"a photograph of an astronaut riding a horse\"], save_int=False)\nfor img in images:display(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks like it is working! So let’s take a deeper dive at the hyper-parameters of the function.  1. prompt - this is the textual prompt we pass through to generate an image. Similar to the pipe(prompt) function we saw in part 1  2. g or guidance scale - It’s a value that determines how close the image should be to the textual prompt. This is related to a technique called Classifier free guidance which improves the quality of the images generated. The higher the value of the guidance scale, more close it will be to the textual prompt  3. seed - This sets the seed from which the initial Gaussian noisy latents are generated  4. steps - Number of de-noising steps taken for generating the final latents.  5. dim - dimension of the image, for simplicity we are currently generating square images, so only one value is needed  6. save_int - This is optional, a boolean flag, if we want to save intermediate latent images, helps in visualization.\nLet’s visualize this process of generation from noise to the final image.\n\n\nCode\n## Creating image through prompt_2_img modified function\nimages = prompt_2_img([\"A dog wearing a hat\"], save_int=True)\n\n## Converting intermediate images to video\n!ffmpeg -v 1 -y -f image2 -framerate 20 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n\n## Deleting intermediate images\nshutil.rmtree(f'./steps/')\n\n## Displaying video output\nmp4 = open('out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n&lt;video width=600 controls&gt;\n      &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n&lt;/video&gt;\n\"\"\" % data_url)\n\n\n\n\n\n\n\n      \n\n\n\n\nFig 3: The de-noisation steps visualization."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#conclusion",
    "title": "Stable diffusion using 🤗 Hugging Face - Putting everything together",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview and breaks the code to the bare minimum so that we can understand each component. Now that we have the minimum code implemented, in the next post we will see make some tweaks to the mk_img function to add additional functionality i.e., img2img pipeline and negative prompt.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "href": "posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html#references",
    "title": "Stable diffusion using 🤗 Hugging Face - Putting everything together",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with 🧨 Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html",
    "title": "Stable diffusion using 🤗 Hugging Face - Introduction",
    "section": "",
    "text": "A brief introduction to start generating images from text prompts using 🤗 hugging face - Diffusers library.\nThis is my first post of the Stable diffusion series, which I will write on Stable diffusion and other ongoing research happening in this field. Most of my learning can be attributed to knowledge acquired while doing the ‘From Deep learning foundations to Stable Diffusion’ course by FastAI and supplementing this with my research. The first few lessons of the FastAI course are publicly available here, and the rest will become available in early 2023. In this post, I want to give a brief introduction of how to use setup the 🤗 diffusion library and start generating images on your own. Next post, we will do a deep dive into mid-level components of this library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#introduction",
    "title": "Stable diffusion using 🤗 Hugging Face - Introduction",
    "section": "1 Introduction",
    "text": "1 Introduction\nStable diffusion simply put is a deep learning model which can generate an image given a textual prompt.\n\n\n\nFig. 1: Stable diffusion overview\n\n\nAs we can see from the image above we can pass a textual prompt like “A dog wearing a hat” and a stable diffusion model can generate an image representative of the text. Pretty amazing!"
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#using-hugging-face-diffuser-library",
    "title": "Stable diffusion using 🤗 Hugging Face - Introduction",
    "section": "2 Using Hugging face Diffuser library",
    "text": "2 Using Hugging face Diffuser library\nAs with any python library, we need to follow certain installation steps before we can run it, here is a rundown of these steps.\n\n2.1 Accepting the license\nBefore using the model, you need to go here and log in using your Hugging face account and then accept the model license to download and use the weights.\n\n\n2.2 Token generation\nIf this is your first time using the hugging face library this might sound like a weird step. You need to go here and generate a token (preferably with write access) to download the model.\n\n\n\nFig. 2: Access token page\n\n\nOnce you have generated the token copy it. First, we will download the hugging face hub library using the following code.\n\n!pip install huggingface-hub==0.10.1\n\nRequirement already satisfied: huggingface-hub==0.10.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (0.10.1)\nRequirement already satisfied: filelock in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (3.8.0)\nRequirement already satisfied: tqdm in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.64.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (4.3.0)\nRequirement already satisfied: packaging&gt;=20.9 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (21.3)\nRequirement already satisfied: requests in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (2.28.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from huggingface-hub==0.10.1) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from packaging&gt;=20.9-&gt;huggingface-hub==0.10.1) (3.0.9)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;huggingface-hub==0.10.1) (1.26.12)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;huggingface-hub==0.10.1) (2022.9.24)\nRequirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;huggingface-hub==0.10.1) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;huggingface-hub==0.10.1) (3.4)\n\n\nThen use the following code, once you run it a widget will appear, paste your newly generated token and click login.\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\nLogin successful\n\nYour token has been saved to /home/aayush/.huggingface/token\n\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\n\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\n\n\ngit config --global credential.helper store\n\n\n\n\n\n\n2.3 Installing diffuser and transformer library\nOnce this process is done, install the dependencies using the following code. This will download the latest version of the diffusers and transformers library.\n\n!pip install -qq -U diffusers transformers\n\nThat’s it, now we are ready to use the diffusers library."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#running-stable-diffusion",
    "title": "Stable diffusion using 🤗 Hugging Face - Introduction",
    "section": "3 Running Stable Diffusion",
    "text": "3 Running Stable Diffusion\nThe first step is to import the StableDiffusionPipeline from the diffusers library.\n\nfrom diffusers import StableDiffusionPipeline\n\nThe next step is to initialize a pipeline to generate an image. The first time you run the following command, it will download the model from the hugging face model hub to your local machine. You will require a GPU machine to be able to run this code.\n\npipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s pass a textual prompt and generate an image.\n\n# Initialize a prompt\nprompt = \"a dog wearing hat\"\n# Pass the prompt in the pipeline\npipe(prompt).images[0]\n\n\nFig 3 - An example of image generated by the diffuser pipeline.\n\n\n\n\n\n\n\n\n\nFor further information on the diffusion pipeline read the documentation here."
  },
  {
    "objectID": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "href": "posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html#conclusion",
    "title": "Stable diffusion using 🤗 Hugging Face - Introduction",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we saw how to install diffusers library from hugging face and use the Stable diffusion model to generate images using a textual prompt. Read the part 2 here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html",
    "title": "Mixing art into the science of model explainability",
    "section": "",
    "text": "Overview on Explainable Boosting Machine and an approach for converting ML explanation to more human-friendly explanation."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-interpretability-vs-accuracy-trade-off",
    "title": "Mixing art into the science of model explainability",
    "section": "1.1 The Interpretability vs Accuracy Trade-off",
    "text": "1.1 The Interpretability vs Accuracy Trade-off\nIn traditional tabular machine learning approaches, Data scientists often deal with the trade-off b/w interpretability and accuracy.\n\n\n\nFig.2: Interpretability/Intelligibility and Accuracy Tradeoff  Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs shown in the chart above, we can see that Glass-Box models like Logistic Regression, Naive Bayes, and Decision Trees are simple models to interpret, and predictions from these models are not highly accurate. On the other hand, Black-Box models like Boosted Trees, Random Forest, and Neural Nets are hard to interpret but lead to highly accurate predictions."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#introducing-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "1.2 Introducing EBMs",
    "text": "1.2 Introducing EBMs\nTo solve the problem just mentioned above EBMs(Explainable Boosted Machine) model was developed by Microsoft Research1. “Explainable Boosting Machine (EBM) is a tree-based, cyclic gradient boosting Generalized Additive Model with automatic interaction detection. EBMs are often as accurate as state-of-the-art BlackBox models while remaining completely interpretable. Although EBMs are often slower to train than other modern algorithms, EBMs are extremely compact and fast at prediction time.”2\n\n\n\nFig.3: EBMs breaking the Interpretability vs Accuracy paradox Image Credit - The Science Behind InterpretML: Explainable Boosting Machine\n\n\nAs we can see from the chart above, EBMs help us break out of this trade-off paradox and help us build models which are both highly interpretable and accurate. To further understand the math behind EBMs I highly encourage watching this 12-minute YouTube video -\n\n\n\n\n\nVideo - The Science Behind InterpretML: Explainable Boosting Machine"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#glass-box-vs-black-box-models.-what-to-choose",
    "title": "Mixing art into the science of model explainability",
    "section": "1.3 Glass box vs Black box models. What to choose?",
    "text": "1.3 Glass box vs Black box models. What to choose?\n\n\n\n\n\n\nTip\n\n\n\nThe answer to every complex question in life is “It depends”.\n\n\nThere are trade-offs b/w using Glassbox models as compared to Blackbox models. There is no clear winner in picking one model over the other but depending on the situation DS can make an educated guess on what model to pick.\n\n\n\nFig.4: Glassbox models vs BlackBox models\n\n\nTwo considerations to think about while picking glass box vs black box models are the following-\n1) Explainability Requirements - In the domain where there is no need for explanation or it is needed for a data scientist or technical audience for intuition/inspection purposes, in these cases, DS are well off using black box models. In the domain where an explanation is needed because of business or regulatory requirements or where these explanations are served to a non-technical audience (humans), glass-box models have an upper hand. This is because explanations coming out of the glass box models are exact and global.\n\n\n\n\n\n\nNote\n\n\n\nExact and global just means that a value of a particular feature will always have the same effect on each prediction explanation. For example, in the case of the prediction of income of a particular individual being above $50k with age as one of the predictors, if the age is 40 and it will impact the target variable with the same proportion let us say 5% in each observation in the data where the age is 40. This is not the case when we build explanations through LIME and Shapely for black box models. In black-box models, age with the value 40 for example can have a 10% lift in an individual probability of their income being above 50k for one observation and -10% lift in the other.\n\n\n2) Compute Requirement - DS needs to pay attention to various compute requirements for testing and training a model depending on its use case. EBMs are particularly slow in the training phase but provide fast predictions with built-in explanations. So, in cases where you need to train your model every hour, EBMs might not suffice your need. But, in cases where the training of the model happens monthly/weekly, and scores are generated on a more frequent basis (hourly/daily) EBMs might fit the use case well. Also, in cases where you might be required to produce an explanation for each prediction EBMs can save a lot of computing and might be the only feasible technique to use for millions of observations. Look below to understand the operational difference b/w EBMs and other tree-based ensemble methods.\n\n\n\nFig. 5: EBMs vs XgBoost/LightGBM"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#data-overview",
    "title": "Mixing art into the science of model explainability",
    "section": "2.1 Data Overview",
    "text": "2.1 Data Overview\nFor this example, we will use Adult Income Dataset from the UCI machine learning Repository3. The problem in this dataset is set up as a binary classification problem to predict if a certain individual income based on various census information (education level, age, gender, occupation, etc.) exceeds $50K/year. For sake of simplicity, we are only going to use observations of individuals in the United States and the following predictors -\n\nAge: continuous variable, individuals’ age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\n\n\nCode\n## Importing required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom interpret.glassbox import ExplainableBoostingClassifier\nfrom interpret import show\nimport warnings\nimport plotly.io as pio\nimport plotly.express as px\nwarnings.filterwarnings('ignore')\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n## Loading the data\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\n\ndf.head()\n\n\n\n\n\n\n\n\nEducation\nAge\nOccupation\nHoursPerWeek\nIncome\n\n\n\n\n0\nBachelors\n39\nAdm-clerical\n40\n&lt;=50K\n\n\n1\nBachelors\n50\nExec-managerial\n13\n&lt;=50K\n\n\n2\nHS-grad\n38\nHandlers-cleaners\n40\n&lt;=50K\n\n\n3\n11th\n53\nHandlers-cleaners\n40\n&lt;=50K\n\n\n5\nMasters\n37\nExec-managerial\n40\n&lt;=50K\n\n\n\n\n\n\n\nLet’s look at target variable distribution.\n\n\nCode\nplot_df = df.Income.value_counts().reset_index().rename(columns = {\"index\":\"Income\", \"Income\":\"Count\"})\nfig = px.bar(plot_df, x = \"Income\", y = 'Count')\nfig.update_layout(\n        title  = {\n            'text':\"Target variable distribution\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Income\", \n        yaxis_title=\"Count of obersvations\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 1 - Target variable distribution\n\n\n\n\nCode\nprint(df.Income.value_counts(normalize=True))\n\n\n &lt;=50K    0.754165\n &gt;50K     0.245835\nName: Income, dtype: float64\n\n\n~24.6% of people in our dataset have income greater than $50K. The data looks good, we have the columns we need. We will use Education, Age, Occupation, and HoursPerWeek columns and predict Income. Before modeling, let us perform an 80-20 train-test split.\n\n## Train-Test Split\nX = df[df.columns[0:-1]]\ny = df[df.columns[-1]]\nseed = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\nprint(f\"Data in training {len(y_train)}, Data in testing {len(y_test)}\")\n\nData in training 23336, Data in testing 5834"
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#fitting-an-ebm-model",
    "title": "Mixing art into the science of model explainability",
    "section": "2.2 Fitting an EBM Model",
    "text": "2.2 Fitting an EBM Model\nEBMs have a scikit-compatible API, so fitting the model and making predictions are the same as any scikit learn model.\n\nebm = ExplainableBoostingClassifier(random_state=seed, interactions=0)\nebm.fit(X_train, y_train)\n\nauc = np.round(metrics.roc_auc_score((y_test != ' &lt;=50K').astype(int).values, ebm.predict_proba(X_test)[:,1], ),3)\nprint(f\"Accuracy: {np.round(np.mean(ebm.predict(X_test) == y_test)*100,2)}%, AUC: {auc}\")\n\nAccuracy: 80.12%, AUC: 0.828\n\n\nI hope the above code block shows how similar the interpret-ml API is to the scikit learn API. Based on AUC on the validation set we can say our model is better than random predictions.\n\n\n\n\n\n\nTip\n\n\n\nIn practice, if you are dealing with millions of observations, Try doing feature selection using LightGBM/XGboost and only train your final models using EBMs. This will save you time in feature exploration."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#explaination-from-ebms",
    "title": "Mixing art into the science of model explainability",
    "section": "2.3 Explaination from EBMs",
    "text": "2.3 Explaination from EBMs\nInterpret package comes with both global and local explanations and has a variety of visualization tools to inspect what the model is learning.\n\n2.3.1 Global explanations\nGlobal explanations provide the following visualization -\n\nSummary - Feature importance plot, this chart provides the importance of each predictor in predicting the target variable\nFeature interaction with Prediction - This chart is the same look-up table EBM uses in making the actual prediction. This can help you in the inspection of how the feature value is contributing to prediction.\n\n\nebm_global = ebm.explain_global()\nshow(ebm_global, renderer='notebook')\n\n\n\n\nFig. 6: EBMs Global Explaination\n\n\n\n\n2.3.2 Local explanations\nThe local explanation is our per-observation level explanation. EBMs have a great in-built visualization for displaying this information.\n\nebm_local = ebm.explain_local(X_test.iloc[0:5,:], y_test)\nshow(ebm_local, renderer='notebook')\n\n\n\n\nFig. 7: EBMs local Explaination\n\n\nLet’s take one example of this explanation for observation at index 0 and look at it -\n\nexplainDF = pd.DataFrame.from_dict(\n    {\n        'names': ebm_local.data(0)['names'], \n        'data':ebm_local.data(0)['values'], \n        'contribution':ebm_local.data(0)['scores']\n    })\nexplainDF\n\n\n\n\n\n\n\n\nnames\ndata\ncontribution\n\n\n\n\n0\nEducation\nBachelors\n0.733420\n\n\n1\nAge\n47\n1.048227\n\n\n2\nOccupation\n?\n-0.318846\n\n\n3\nHoursPerWeek\n18\n-0.854202\n\n\n\n\n\n\n\nAs we can see from the data, we can see we have the Name of the columns, the actual values, and the contribution of that value to the actual prediction score. For this observation let us see what the model is learning -\n\nEducation as Bachelors is working in favor of &gt;50K income\nAge value 47 is also in favor of &gt;50K\nOccupation being “?” has a negative impact on &gt;50K income\nHoursPerWeek being 18 has a negative impact on &gt;50K income (Average work week hours in the US are around 40, so this makes sense)\n\nYou can also do it for the entire dataset and collect the importance of each feature. Here is a sample code to do the same.\n\nscores = [x['scores'] for x in ebm_local._internal_obj['specific']]\nsummary = pd.DataFrame(scores)\nsummary.columns = ebm_local.data(0)['names']\nsummary.head()\n\n\n\n\n\n\n\n\nEducation\nAge\nOccupation\nHoursPerWeek\n\n\n\n\n0\n0.733420\n1.048227\n-0.318846\n-0.854202\n\n\n1\n-0.990661\n0.309251\n0.171131\n0.002109\n\n\n2\n-0.257254\n0.735232\n0.171131\n0.002109\n\n\n3\n0.193118\n0.682721\n-0.417499\n0.279677\n\n\n4\n0.733420\n0.085672\n0.389171\n0.002109\n\n\n\n\n\n\n\nNow we can extract the importance of all data rows in our test set."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#draw-back-and-concerns",
    "title": "Mixing art into the science of model explainability",
    "section": "2.4 Draw back and concerns",
    "text": "2.4 Draw back and concerns\n\n\n\nCredit - XKCD\n\n\nThese kinds of explanations are still very abstract, even at the observational level reasoning is not human(non-technical) friendly. When the feature count grows this becomes even non-human friendly. Typical business consumers of your model might not be well versed in reading such charts and shy away from trying the insights/predictions the model is giving them. After all, if I don’t understand something, I don’t trust it. That is where art comes in, let’s see how we can build on the above-derived observations and make it easier to understand."
  },
  {
    "objectID": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#footnotes",
    "href": "posts/2022-09-23-Explainability/2022-09-23-Explainability.html#footnotes",
    "title": "Mixing art into the science of model explainability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“InterpretML: A Unified Framework for Machine Learning Interpretability” (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)↩︎\n“Interpret ML - EBM documentation”↩︎\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.↩︎"
  },
  {
    "objectID": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "href": "posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html",
    "title": "Finding similar images using Deep learning and Locality Sensitive Hashing",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "href": "posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html",
    "title": "MultiLayer Perceptron using Fastai and Pytorch",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "href": "posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html",
    "title": "Multi-Layer perceptron using Tensorflow",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "",
    "text": "In this post, I am going to write about Recommender systems, how they are used in many e-commerce websites. The post will also cover about building simple recommender system models using Matrix Factorization algorithm using lightFM package and my recommender system cookbook. The post will focus on business use cases and simple implementations. The post only cover basic intuition around algorithms and will provide links to resources if you want to understand the math behind the algorithm."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#motivation",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Motivation",
    "text": "Motivation\nI am an avid reader and a believer in open source education and continuously expand my knowledge around data science & computer science using online courses, blogs, Github repositories and participating in data science competitions. While searching for quality content on the internet, I have come across various learning links which either focus on the implementation of the algorithm using specific data/modeling technique in ABC language or focus on business impact/results using the broad concept of a family of algorithms(like classification, forecasting, recommender systems etc.) but don’t go into details of how to do it. So the idea is to write some blogs which can combine both business use cases with codes & algorithmic intuition to provide a holistic view of how data science is used in business scenarios. \nAs the world is becoming more digital, we are already getting used to a lot of personalized experience and the algorithm which help us achieve this falls in the family of recommender systems. Almost every web-based platform is using some recommender system to provide customized content. Following are the companies I admire the most."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#what-is-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "What is personalization?",
    "text": "What is personalization?\nPersonalization is a technique of dynamically tailoring your content based on needs of each user. Simple examples of personalization could be movie recommendation on Netflix, personalized email targeting/re-targeting by e-commerce platforms, item recommendation on Amazon, etc. Personalization helps us achieve these four Rs - - Recognize: Know customer’s and prospects’ profiles, including demographics, geography, and expressed and shared interests. - Remember: Recall customers’ history, primarily how they act as expressed by what they browse and buy - Reach: Deliver the right promotion, content, recommendation for a customer based on actions, preferences, and interests - Relevance: Deliver personalization within the context of the digital experience based on who customers are, where they are located and what time of year it is"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#why-personalization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Why personalization?",
    "text": "Why personalization?\nPersonalization has a lot of benefits for both users and companies. For users, it makes their life easy as they only get to see more relevant stuff to them (unless it’s an advertisement, even they are personalized). For business benefits are countless but here are few which I would like to mention - - Enhance customer experience: Personalization reduces the clutter and enhances the customer experience by showing relevant content - Cross-sell/ Up-sell opportunities: Relevant product offerings based on customer preferences can lead to increasing products visibility and eventually selling more products - Increased basket size: Personalized experience and targeting ultimately leads to increased basket size and frequent purchases - Increased customer loyalty: In the digital world, customer retention/loyalty is the most prominent problem faced by many companies as finding a replacement for a particular service is quite easy. According to a Forbes article, Forty-four percent of consumers say they will likely repeat after a personalized experience"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#introduction-to-matrix-factorization",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Introduction to Matrix factorization",
    "text": "Introduction to Matrix factorization\nMatrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc.  \n Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it’s more female-driven the value is typically low.  \nFor more information on matrix factorization and factorization machines you can read these articles -  Matrix Factorization: A Simple Tutorial and Implementation in Python  Introductory Guide – Factorization Machines & their application on huge datasets (with codes in Python)"
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#handon-building-recommender-system-using-lightfm-package-in-python",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "HandOn: Building recommender system using LightFM package in Python",
    "text": "HandOn: Building recommender system using LightFM package in Python\nIn the hands-on section, we will be building recommender system for different scenarios which we typically see in many companies using LightFM package and MovieLens data. We are using small size data which contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users\n\nData\nLet’s start by importing data, recommender system cookbook and preprocessing cookbook files for this hands-on section. I have written these reusable generic cookbook codes to increase productivity and write clean/modular codes; you will see we can build a recommender system using 10-15 lines of code by using these cookbooks(do more with less!).\n# Importing Libraries and cookbooks\nfrom recsys import * ## recommender system cookbook\nfrom generic_preprocessing import * ## pre-processing code\nfrom IPython.display import HTML ## Setting display options for Ipython Notebook\n# Importing rating data and having a look\nratings = pd.read_csv('./ml-latest-small/ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\nuserId\n\n\nmovieId\n\n\nrating\n\n\ntimestamp\n\n\n\n\n\n\n0\n\n\n1\n\n\n31\n\n\n2.5\n\n\n1260759144\n\n\n\n\n1\n\n\n1\n\n\n1029\n\n\n3.0\n\n\n1260759179\n\n\n\n\n2\n\n\n1\n\n\n1061\n\n\n3.0\n\n\n1260759182\n\n\n\n\n3\n\n\n1\n\n\n1129\n\n\n2.0\n\n\n1260759185\n\n\n\n\n4\n\n\n1\n\n\n1172\n\n\n4.0\n\n\n1260759205\n\n\n\n\n\nAs we can see rating data contain user id, movie id and a rating between 0.5 to 5 with a timestamp representing when the rating was given.\n# Importing movie data and having a look at first five columns\nmovies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies.head()\n\n\n\n\n\n\n\n\nmovieId\n\n\ntitle\n\n\ngenres\n\n\n\n\n\n\n0\n\n\n1\n\n\nToy Story (1995)\n\n\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n1\n\n\n2\n\n\nJumanji (1995)\n\n\nAdventure|Children|Fantasy\n\n\n\n\n2\n\n\n3\n\n\nGrumpier Old Men (1995)\n\n\nComedy|Romance\n\n\n\n\n3\n\n\n4\n\n\nWaiting to Exhale (1995)\n\n\nComedy|Drama|Romance\n\n\n\n\n4\n\n\n5\n\n\nFather of the Bride Part II (1995)\n\n\nComedy\n\n\n\n\n\nMovie data consist of movie id, their title, and genre they belong.\n\n\nPreprocessing\nAs I mentioned before, to create a recommender system we need to start by creating an interaction matrix. For this task, we will use the create_interaction_matrix function from the recsys cookbook. This function requires you to input a pandas dataframe and necessary information like column name for user id, item id, and rating. It also takes an additional parameter threshold if norm=True which means any rating above the mentioned threshold is considered a positive rating. In our case, we don’t have to normalize our data, but in cases of retail data any purchase of a particular type of item can be considered a positive rating, quantity doesn’t matter.\n# Creating interaction matrix using rating data\ninteractions = create_interaction_matrix(df = ratings,\n                                         user_col = 'userId',\n                                         item_col = 'movieId',\n                                         rating_col = 'rating')\ninteractions.head()\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n…\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n2\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n3\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n4\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n5\n\n\n0.0\n\n\n0.0\n\n\n4.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n5 rows × 9066 columns\n\n\nAs we can see the data is created in an interaction format where rows represent each user and columns represent each movie id with ratings as values.  We will also create user and item dictionaries to later convert user_id to user_name or movie_id to movie_name by using create_user_dict and create_item dict function.\n# Create User Dict\nuser_dict = create_user_dict(interactions=interactions)\n# Create Item dict\nmovies_dict = create_item_dict(df = movies,\n                               id_col = 'movieId',\n                               name_col = 'title')\n\n\nBuilding Matrix Factorization model\nTo build a matrix factorization model, we will use the runMF function which will take following input -\n- interaction matrix: Interaction matrix created in the previous section - n_components: Number of embedding generated for each user and item - loss: We need to define a loss function, in this case, we are using warp loss because we mostly care about the ranking of data, i.e, which items should we show first - epoch: Number of times to run - n_jobs: Number of cores to use in parallel processing\nmf_model = runMF(interactions = interactions,\n                 n_components = 30,\n                 loss = 'warp',\n                 epoch = 30,\n                 n_jobs = 4)\nNow we have built our matrix factorization model we can now do some interesting things. There are various use cases which can be solved by using this model for a web platform let’s look into them.\n\n\nUsecase 1: Item recommendation to a user\nIn this use case, we want to show a user, items he might be interested in buying/viewing based on his/her interactions done in the past. Typical industry examples for this are like “Deals recommended for you” on Amazon or “Top pics for a user” on Netflix or personalized email campaigns. \nWe can use the sample_recommendation_user function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, user_id and the number of items as input and return the list of item id’s a user may be interested in interacting.\n## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = mf_model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = movies_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)\nKnown Likes:\n1- The Hunger Games: Catching Fire (2013)\n2- Gravity (2013)\n3- Dark Knight Rises, The (2012)\n4- The Hunger Games (2012)\n5- Town, The (2010)\n6- Exit Through the Gift Shop (2010)\n7- Bank Job, The (2008)\n8- Departed, The (2006)\n9- Bourne Identity, The (1988)\n10- Step Into Liquid (2002)\n11- SLC Punk! (1998)\n12- Last of the Mohicans, The (1992)\n13- Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)\n14- Robin Hood: Prince of Thieves (1991)\n15- Citizen Kane (1941)\n16- Trainspotting (1996)\n17- Pulp Fiction (1994)\n18- Usual Suspects, The (1995)\n\n Recommended Items:\n1- Dark Knight, The (2008)\n2- Inception (2010)\n3- Iron Man (2008)\n4- Shutter Island (2010)\n5- Fight Club (1999)\n6- Avatar (2009)\n7- Forrest Gump (1994)\n8- District 9 (2009)\n9- WALL·E (2008)\n10- Matrix, The (1999)\nprint(rec_list)\n[593L, 260L, 110L, 480L, 47L, 527L, 344L, 858L, 231L, 780L]\nAs we can see in this case user is interested in “Dark Knight Rises(2012)” so the first recommendation is “The Dark Knight(2008)”. This user also seems to have a strong liking towards movies in drama, sci-fi and thriller genre and there are many movies recommended in the same genre like Dark Knight(Drama/Crime), Inception(Sci-Fi, Thriller), Iron Man(Sci-FI thriller), Shutter Island(Drame/Thriller), Fight club(drama), Avatar(Sci-fi), Forrest Gump(Drama), District 9(Thriller), Wall-E(Sci-fi), The Matrix(Sci-Fi) \nSimilar models can also be used for building sections like “Based on your recent browsing history” recommendations by just changing the rating matrix only to contain interaction which is recent and based on browsing history visits on specific items.\n\n\nUsecase 2: User recommendation to a item\nIn this use case, we will discuss how we can recommend a list of users specific to a particular item. Example of such cases is when you are running a promotion on an item and want to run an e-mail campaign around this promotional item to only 10,000 users who might be interested in this item.\n\nWe can use the sample_recommendation_item function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, item_id and the number of users as input and return the list of user id’s who are more likely be interested in the item.\n## Calling 15 user recommendation for item id 1\nsample_recommendation_item(model = mf_model,\n                           interactions = interactions,\n                           item_id = 1,\n                           user_dict = user_dict,\n                           item_dict = movies_dict,\n                           number_of_user = 15)\n[116, 410, 449, 657, 448, 633, 172, 109, 513, 44, 498, 459, 317, 415, 495]\nAs you can see function return a list of userID who might be interested in item id 1. Another example why you might need such model is when there is an old inventory sitting in your warehouse which needs to clear up otherwise you might have to write it off, and you want to clear it by giving some discount to users who might be interested in buying.\n\n\nUsecase 3: Item recommendation to items\nIn this use case, we will discuss how we can recommend a list of items specific to a particular item. This kind of models will help you to find similar/related items or items which can be bundled together. Typical industry use case for such models are in cross-selling and up-selling opportunities on product page like “Products related to this item”, “Frequently bought together”, “Customers who bought this also bought this” and “Customers who viewed this item also viewed”.  “Customers who bought this also bought this” and “Customers who viewed this item also viewed” can also be solved through market basket analysis.\n\nTo achieve this use case, we will create a cosine distance matrix using item embeddings generated by matrix factorization model. This will help us calculate similarity b/w items, and then we can recommend top N similar item to an item of interest. First step is to create a item-item distance matrix using the create_item_emdedding_distance_matrix function. This function takes matrix factorization models and interaction matrix as input and returns an item_embedding_distance_matrix.\n## Creating item-item distance matrix\nitem_item_dist = create_item_emdedding_distance_matrix(model = mf_model,\n                                                       interactions = interactions)\n## Checking item embedding distance matrix\nitem_item_dist.head()\n\n\n\n\n\n\nmovieId\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n…\n\n\n161084\n\n\n161155\n\n\n161594\n\n\n161830\n\n\n161918\n\n\n161944\n\n\n162376\n\n\n162542\n\n\n162672\n\n\n163949\n\n\n\n\nmovieId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1.000000\n\n\n0.760719\n\n\n0.491280\n\n\n0.427250\n\n\n0.484597\n\n\n0.740024\n\n\n0.486644\n\n\n0.094009\n\n\n-0.083986\n\n\n0.567389\n\n\n…\n\n\n-0.732112\n\n\n-0.297997\n\n\n-0.451733\n\n\n-0.767141\n\n\n-0.501647\n\n\n-0.270280\n\n\n-0.455277\n\n\n-0.292823\n\n\n-0.337935\n\n\n-0.636147\n\n\n\n\n2\n\n\n0.760719\n\n\n1.000000\n\n\n0.446414\n\n\n0.504502\n\n\n0.525171\n\n\n0.572113\n\n\n0.364393\n\n\n0.290633\n\n\n0.231926\n\n\n0.653033\n\n\n…\n\n\n-0.748452\n\n\n-0.307634\n\n\n-0.165400\n\n\n-0.526614\n\n\n-0.146751\n\n\n-0.156305\n\n\n-0.223818\n\n\n-0.138412\n\n\n-0.209538\n\n\n-0.733489\n\n\n\n\n3\n\n\n0.491280\n\n\n0.446414\n\n\n1.000000\n\n\n0.627473\n\n\n0.769991\n\n\n0.544175\n\n\n0.632008\n\n\n0.336824\n\n\n0.392284\n\n\n0.510592\n\n\n…\n\n\n-0.331028\n\n\n-0.264556\n\n\n-0.308592\n\n\n-0.285085\n\n\n-0.046424\n\n\n-0.165821\n\n\n-0.183842\n\n\n-0.143613\n\n\n-0.156418\n\n\n-0.378811\n\n\n\n\n4\n\n\n0.427250\n\n\n0.504502\n\n\n0.627473\n\n\n1.000000\n\n\n0.582582\n\n\n0.543208\n\n\n0.602390\n\n\n0.655708\n\n\n0.527346\n\n\n0.471166\n\n\n…\n\n\n-0.380431\n\n\n-0.163091\n\n\n-0.232833\n\n\n-0.334746\n\n\n-0.052832\n\n\n-0.266185\n\n\n-0.158415\n\n\n-0.211618\n\n\n-0.232351\n\n\n-0.469629\n\n\n\n\n5\n\n\n0.484597\n\n\n0.525171\n\n\n0.769991\n\n\n0.582582\n\n\n1.000000\n\n\n0.354141\n\n\n0.639958\n\n\n0.396447\n\n\n0.432026\n\n\n0.385051\n\n\n…\n\n\n-0.273074\n\n\n-0.280585\n\n\n-0.306195\n\n\n-0.265243\n\n\n0.012961\n\n\n-0.225142\n\n\n-0.317043\n\n\n-0.136875\n\n\n-0.122382\n\n\n-0.312858\n\n\n\n\n\n5 rows × 9066 columns\n\n\nAs we can see the matrix have movies as both row and columns and the value represents the cosine distance between them. Next step is to use item_item_recommendation function to get top N items with respect to an item_id. This function takes item embedding distance matrix, item_id, item_dictionary and number of items to be recommended as input and return similar item list as output.\n## Calling 10 recommended items for item id \nrec_list = item_item_recommendation(item_emdedding_distance_matrix = item_item_dist,\n                                    item_id = 5378,\n                                    item_dict = movies_dict,\n                                    n_items = 10)\nItem of interest :Star Wars: Episode II - Attack of the Clones (2002)\nItem similar to the above item:\n1- Star Wars: Episode III - Revenge of the Sith (2005)\n2- Lord of the Rings: The Two Towers, The (2002)\n3- Lord of the Rings: The Fellowship of the Ring, The (2001)\n4- Lord of the Rings: The Return of the King, The (2003)\n5- Matrix Reloaded, The (2003)\n6- Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n7- Gladiator (2000)\n8- Spider-Man (2002)\n9- Minority Report (2002)\n10- Mission: Impossible II (2000)\nAs we can see for “Star Wars: Episode II - Attack of the Clones (2002)” movie we are getting it’s next released movies which is “Star Wars: Episode III - Revenge of the Sith (2005)” as the first recommendation."
  },
  {
    "objectID": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "href": "posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html#summary",
    "title": "Solving business usecases by recommender system using lightFM",
    "section": "Summary",
    "text": "Summary\nLike any other blog, this method isn’t perfect for every application, but the same ideas can work if we use it effectively. There is a lot of advancements in recommender systems with the advent of Deep learning. While there is room for improvement, I am pleased with how it has been working for me so far. I might write about deep learning based recommender systems later sometime.\nIn the meantime, I hope you enjoyed reading, and feel free to use my code to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "other/carlson.html",
    "href": "other/carlson.html",
    "title": "Carlson Students Q&A",
    "section": "",
    "text": "NoteHow I apply my learning from courses while my duration at the schoool?\n\n\n\n\n\nHere are a few options to apply your learning from the course -\nUniversity Curriculum\n\nLive project - MSBA program usually collaborates with companies around the Minneapolis area to run live projects every semester to allow students to work on real problems. Typically, these programs are part of some courses like Explorative Data analysis and Predictive modeling.\nExperiential learning, Carlson Analytics Lab - A six-credit course where students work in a group of four or five on a live project with a corporate partner to solve their real problem using real data. You can think of it as an internship.\n\nOther initiatives\n\nMeetup group - There are meetup groups like Social Data science,and Analyze This! etc., where you can actively participate and present.\nLocal Competitions - There are local analytics competitions organized by Minneanalytics, STATCOM, UMN StatClub, Social Data Science, etc. Keep a look out for those and participate.\nBlogging - Blogging is also an effective way to share your learning and establish your brand. Medium is a good place to start and publish in high-visibility publications like Towards Data Science, Analytics Vidhya, etc\n\n\n\n\n\n\n\n\n\n\nNoteWhat kind of analytics/DS/ML roles are available in tech companies?\n\n\n\n\n\nThere are three typical DS/ML roles in tech companies -\n\nData scientist - Which is heavier on data analytics and experimentation.\nMachine Learning Scientist / Applied Scientist - This role is more focused on Machine learning model building, research, and software engineering.\nMachine Learning Engineers - This role is more focused on ML Ops, building data pipelines, and requires a lot more software engineering skills."
  },
  {
    "objectID": "other/carlson.html#admission",
    "href": "other/carlson.html#admission",
    "title": "Carlson Students Q&A",
    "section": "2.1 Admission",
    "text": "2.1 Admission\n\n\n\n\n\n\nNoteI am applying to Carlson School for MSBA program. Could you help me out with interviews?\n\n\n\n\n\nVideo interview usually is a formality to ensure that you can communicate in English. Interviewer will ask you following questions like -\n\nTell me about yourself\nWhy MSBA program and Why Carlson?\nWhat are you hoping to achieve after the course?\nWalking through your work experience\n\nIt’s quite straightforward and usually just ensuring whatever you have written on your resume is actually you.\n\n\n\n\n\n\n\n\n\nNoteI want to apply to Carlson School for MSBA program. What would be good GRE score?\n\n\n\n\n\nScore as high as you can, 320+ on GRE and 105 + on TOEFL would be good scores to aim for. You can find more about academic scores and prior background of current students on the university website."
  },
  {
    "objectID": "other/carlson.html#financial",
    "href": "other/carlson.html#financial",
    "title": "Carlson Students Q&A",
    "section": "2.2 Financial",
    "text": "2.2 Financial\n\n\n\n\n\n\nNoteHow are the job prospects for a candidate after the course? Is the course worth it?\n\n\n\n\n\nJob placements are usually close to 100% so everybody gets a job. Read the placement reports on the website. I think the starting average salary is around $90k-$110k annually plus a 10% annual bonus, with that salary you should be able to recover your cost within 2-4 years. There are some companies that come on campus (Target, Cargill, Capital One, Ameriprise, etc.) but most of the students get jobs through their own search.\n\n\n\n\n\n\n\n\n\nNoteIs TA/RA possible in this course? Do you think I should be approaching professors (aligned with my interests) beforehand?\n\n\n\n\n\nYes, but not in the summer semester. You can do TA/RA in Fall & Spring semesters, but I would not recommend that as the course is hectic and you should be focusing on creating a portfolio and learning. You can reach out to professors during the summer semester if you need to do TA/RA. Typically TA/RA jobs pay $18/hour, and students are allowed to do 10-20 hours per week.\n\n\n\n\n\n\n\n\n\nNoteTuition for the course is high are there any scholarships for International students?\n\n\n\n\n\nCheck the university page for grants and scholarships. In my time there was a $10k scholarship given by the university to students. Students didn’t need to apply for it and were given automatically based on their prior background and GRE/TOEFL scores."
  },
  {
    "objectID": "other/carlson.html#accomodations",
    "href": "other/carlson.html#accomodations",
    "title": "Carlson Students Q&A",
    "section": "2.3 Accomodations",
    "text": "2.3 Accomodations\n\n\n\n\n\n\nNoteIn which area or society(if you prefer one) should I search for accommodation? I am looking both in terms of proximity and rent?\n\n\n\n\n\nYou can check out Grand Marc seven corners that are right next to Carlson and quite affordable(&lt;$500/month sharing, fully furnished). Many MSBA students including myself have lived there in the past. Otherwise, you can look in Minnehaha apartments or Stadium village apartments. Here is the FB page for Housing.\n\n\n\n\n\n\n\n\n\nNoteHow much overall monthly expense do you estimate?\n\n\n\n\n\nDepends on your lifestyle ~$1000 including rent is good enough to survive.\n\n\n\n\n\n\n\n\n\nNoteIs the weather very extreme and do you actually have to be indoors for a few months?\n\n\n\n\n\nWell someone in Minnesota will say “Its not cold, you just don’t have the right gear”. So it is cold in winters for atleast 4-5 months usually below -5 C. It doesn’t mean you can’t go out or life stops. People do business as usual in those conditions as well. In my time in Carlson I used to spend more than 12-14 hours in Carlson campus so none of this actually bothered me. But yeah it’s cold in winters so be prepared."
  },
  {
    "objectID": "other/carlson.html#course-and-skills-related",
    "href": "other/carlson.html#course-and-skills-related",
    "title": "Carlson Students Q&A",
    "section": "2.4 Course and Skills Related",
    "text": "2.4 Course and Skills Related\n\n\n\n\n\n\nNoteHow did you like the course and the skills development at Carlson?\n\n\n\n\n\nI found the course useful in what I was trying to achieve. I was more looking for a business-oriented analytics/data science course and Carlson does a really good job in balancing these two aspects.\n\n\n\n\n\n\n\n\n\nNoteWhat prerequisite would you recommend for better prospects?\n\n\n\n\n\nDepends on what you want to accomplish. If your aim is to be a data scientist then you need to be technically strong, if your aim is to become a consultant then you need to work more on soft skills. The ability to communicate with a business audience is a must and proficiency in python will give you an edge over others.\n\n\n\n\n\n\n\n\n\nNoteIs the curriculum good for someone who is more interested in a technical ds role or more of a consulting oriented role in the field of analytics?\n\n\n\n\n\nThe course does touch upon many DS technicalities, but if you don’t have any prior experience, you need to put extra effort into honing skills in those areas. I would say the course does a much better job to prepare students for consulting oriented roles in the field of analytics. It also depends on individual aspirations and current skill level.\n\n\n\n\n\n\n\n\n\nNoteOne of my great concern is the location. Minneapolis is not the big city and there might be less opportunity, especially for the international students. What do you think?\n\n\n\n\n\nMinneapolis is not a small city and is home to 17 Fortune 500 companies. Also, university have active alumni in many of these companies which hire almost every year from the MSBA program. Some of the companies which hire regularly are Target, Capital One, Amazon, Ameriprise, Best Buy, Slalom, etc. Also, the MSBA program have nearly 100% placement stats. Not sure if other universities or programs have such stellar stats but finding employment should not be a problem if you happen to be in the top 50% of the class."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Get notified when I publish new posts!"
  },
  {
    "objectID": "blog.html#subscribe-to-my-blog---here",
    "href": "blog.html#subscribe-to-my-blog---here",
    "title": "Blog",
    "section": "",
    "text": "Get notified when I publish new posts!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aayush Agrawal",
    "section": "",
    "text": "I am an experienced Machine Learning Engineer with a strong focus on designing and implementing machine learning-driven solutions. Passionate about cutting-edge data technologies, my expertise spans big data platforms, deep learning, optimization techniques, and business analytics.\nCurrently, I work as a Machine Learning Engineer on the Video Recommendations team at Meta, where I specialize in improving exploration strategies for the interest-oriented recommendations in Facebook Reels. Previously, at Microsoft, I developed data-driven products to deliver intelligent recommendations, empowering Microsoft Partners, M365 administrators, and end-users to optimize M365 service usage.\nMy diverse experience includes working across various industries such as agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing, contributing innovative solutions to complex challenges."
  },
  {
    "objectID": "about.html#technical",
    "href": "about.html#technical",
    "title": "Aayush Agrawal",
    "section": "Technical",
    "text": "Technical\n\n\nNatural Language Processing Specialization, Deeplearning.ai, Coursera\nCertificate\n\n\nOctober, 2022\n\n\n\n\nTriplebyte Engineering Certificate\nCertificate\n\n\nAugust, 2022\n\n\n\n\nParallel Programming with Dask in Python, DataCamp\nCertificate\n\n\nMarch, 2022\n\n\n\n\nMicrosoft Global Hackathon 2021 Award Winner, Microsoft\nCertificate\n\n\nDecember, 2021\n\n\n\n\nAI The LinkedIn Way, LinkedIn\nCertificate\n\n\nDecember, 2020\n\n\n\n\nLearning C#, LinkedIn\nCertificate\n\n\nJuly, 2019\n\n\n\n\nDEV262x: Logic and Computational Thinking, EDX\nCertificate\n\n\nMarch, 2019\n\n\n\n\nDevOps for Data Scientists, LinkedIn\nCertificate\n\n\nOctober, 2018\n\n\n\n\nGit Essential Training, LinkedIn\nCertificate\n\n\nOctober, 2018\n\n\n\n\nNatural Language Processing, HSE, Coursera\nCertificate\n\n\nJuly, 2018\n\n\n\n\nIntroduction to Deep Learning, HSE, Coursera\nCertificate\n\n\nMay, 2019\n\n\n\n\nDeveloping Data Products, John Hopkins University, Coursera\nCertificate\n\n\nNovember, 2014\n\n\n\n\nReproducible Research, John Hopkins University, Coursera\nCertificate\n\n\nNovember, 2014\n\n\n\n\nRegression Models, John Hopkins University, Coursera\nCertificate\n\n\nOctober, 2014\n\n\n\n\nMachine Learning, Stanford University, Coursera\nCertificate\n\n\nSeptember, 2014\n\n\n\n\nPractical Machine Learning, John Hopkins University, Coursera\nCertificate\n\n\nSeptember, 2014\n\n\n\n\nR Programming, John Hopkins University, Coursera\nCertificate\n\n\nSeptember, 2014"
  },
  {
    "objectID": "about.html#non---technical",
    "href": "about.html#non---technical",
    "title": "Aayush Agrawal",
    "section": "Non - Technical",
    "text": "Non - Technical\n\n\nCreativity at Work: A Short Course from Seth Godin, LinkedIn\nCertificate\n\n\nJuly, 2021\n\n\n\n\nLearning to Be Assertive, LinkedIn\nCertificate\n\n\nJune, 2021\n\n\n\n\nSucceeding in a New Role By Managing Up, LinkedIn\nCertificate\n\n\nMarch, 2021\n\n\n\n\nImproving Your Listening Skills, LinkedIn\nCertificate\n\n\nNovember, 2020\n\n\n\n\nGiving and Receiving Feedback, LinkedIn\nCertificate\n\n\nMay, 2020\n\n\n\n\nTime Management Fundamentals, LinkedIn\nCertificate\n\n\nApril, 2020\n\n\n\n\nCoaching Skills for Leaders and Managers, LinkedIn\nCertificate\n\n\nJune, 2019\n\n\n\n\nBecoming a Male Ally at Work, LinkedIn\nCertificate\n\n\nJune, 2019"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to aayushmnit.com",
    "section": "",
    "text": "Subscribe on Medium\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n      \nI am an experienced Machine Learning Engineer with a strong focus on designing and implementing machine learning-driven solutions. Passionate about cutting-edge data technologies, my expertise spans big data platforms, deep learning, optimization techniques, and business analytics.\nCurrently, I work as a Machine Learning Engineer on the Video Recommendations team at Meta, where I specialize in improving exploration strategies for the interest-oriented recommendations in Facebook Reels. Previously, at Microsoft, I developed data-driven products to deliver intelligent recommendations, empowering Microsoft Partners, M365 administrators, and end-users to optimize M365 service usage.\nMy diverse experience includes working across various industries such as agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing, contributing innovative solutions to complex challenges.\nRead more about me here.\nCheck out the latest draft of my book - “The Pytorch Book”. If you enjoy the content, please consider giving it a star✨ on Github."
  },
  {
    "objectID": "index.html#latest-blogs---subscribe",
    "href": "index.html#latest-blogs---subscribe",
    "title": "Welcome to aayushmnit.com",
    "section": "Latest Blogs - Subscribe",
    "text": "Latest Blogs - Subscribe\nClick here to check out more blogs.\n\n\n\n\n\n\n\n\n\n\nDiversity in Recommendations - Maximal Marginal Relevance (MMR)\n\n\n\n\n\n\n\n\nDec 25, 2025\n\n\nAayush Agrawal\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Write Technical Blogs (and How to Start)\n\n\n\n\n\n\n\n\nDec 13, 2025\n\n\nAayush Agrawal\n\n\n\n\n\n\n\n\n\n\n\n\nMeta MLE Interview Preparation Guide\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nAayush Agrawal\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-02-19-launch/index.html",
    "href": "posts/2018-02-19-launch/index.html",
    "title": "Website launch",
    "section": "",
    "text": "Finally got time to build my own website on Github. This is my first post and I would like to thank Academic Pages which provided with a wonderful repository to help me get started with building this website.\nI am intending to use this website to publish blogs, workshop and any material which would be releveant in Data science field.\nWill see you guys in next post.\n-Aayush"
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html",
    "title": "Building Neural Network from scratch",
    "section": "",
    "text": "In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let’s start with the very basics of a neural network which is Multilayer Perceptron. You can find the same blog in notebook version here."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-neural-network",
    "title": "Building Neural Network from scratch",
    "section": "What is a neural network?",
    "text": "What is a neural network?\nA neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it’s the same with a neural network."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#what-is-a-multi-layer-perceptron",
    "title": "Building Neural Network from scratch",
    "section": "What is a Multi layer perceptron?",
    "text": "What is a Multi layer perceptron?\nMulti-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let’s start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.   \n\nFig 1: Perceptron image\n\n\n\nImage credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/\n\nNow, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology.  Dense layer,   Note that bias term is now a vector and W is a weight matrix  \n\nFig: Single dense layer perceptron network\n\n\n\nImage credit=http://www.texample.net/tikz/examples/neural-network/\n\nNow we understand dense layer let’s add a bunch of them, and that network becomes a multi-layer perceptron network.\n\n\nFig: Multi layer perceptron network\n\n\n\nImage credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2s\n\nIf you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero.  ReLU function   Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a feed-forward loop and backpropagation loop. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don’t understand the concept of gradient weight updates and SGD, I recommend you to watch week 1 of Machine learning by Andrew NG lectures.\nSo, to summarize a neural network needs few building blocks\n\nDense layer - a fully-connected layer, \nReLU layer (or any other activation function to introduce non-linearity)\nLoss function - (crossentropy in case of multi-class classification problem)\nBackprop algorithm - a stochastic gradient descent with backpropageted gradients\n\nLet’s approach them one at a time."
  },
  {
    "objectID": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "href": "posts/2018-06-03-Building_neural_network_from_scratch/index.html#coding-starts-here",
    "title": "Building Neural Network from scratch",
    "section": "Coding Starts here:",
    "text": "Coding Starts here:\nLet’s start by importing some libraires required for creating our neural network.\nfrom __future__ import print_function\nimport numpy as np ## For numerical python\nnp.random.seed(42)\nEvery layer will have a forward pass and backpass implementation. Let’s create a main class layer which can do a forward pass .forward() and Backward pass .backward().\nclass Layer:\n    \n    #A building block. Each layer is capable of performing two things:\n\n    #- Process input to get output:           output = layer.forward(input)\n    \n    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n    \n    #Some layers also have learnable parameters which they update during layer.backward.\n    \n    def __init__(self):\n        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n        # A dummy layer does nothing\n        pass\n    \n    def forward(self, input):\n        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        \n        # A dummy layer just returns whatever it gets as input.\n        return input\n\n    def backward(self, input, grad_output):\n        # Performs a backpropagation step through the layer, with respect to the given input.\n        \n        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n        \n        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n        \n        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n        \n        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n        \n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n        num_units = input.shape[1]\n        \n        d_layer_d_input = np.eye(num_units)\n        \n        return np.dot(grad_output, d_layer_d_input) # chain rule\n\nNonlinearity ReLU layer\nThis is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\nclass ReLU(Layer):\n    def __init__(self):\n        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n        pass\n    \n    def forward(self, input):\n        # Apply elementwise ReLU to [batch, input_units] matrix\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    \n    def backward(self, input, grad_output):\n        # Compute gradient of loss w.r.t. ReLU input\n        relu_grad = input &gt; 0\n        return grad_output*relu_grad \n\n\nDense layer\nNow let’s build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\nA dense layer applies affine transformation. In a vectorized form, it can be described as: \nWhere * X is an object-feature matrix of shape [batch_size, num_features], * W is a weight matrix [num_features, num_outputs] * and b is a vector of num_outputs biases.\nBoth W and b are initialized during layer creation and updated each time backward is called. Note that we are using Xavier initialization which is a trick to train our model to converge faster read more. Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        # A dense layer is a layer which performs a learned affine transformation:\n        # f(x) = &lt;W*x&gt; + b\n        \n        self.learning_rate = learning_rate\n        self.weights = np.random.normal(loc=0.0, \n                                        scale = np.sqrt(2/(input_units+output_units)), \n                                        size = (input_units,output_units))\n        self.biases = np.zeros(output_units)\n        \n    def forward(self,input):\n        # Perform an affine transformation:\n        # f(x) = &lt;W*x&gt; + b\n        \n        # input shape: [batch, input_units]\n        # output shape: [batch, output units]\n        \n        return np.dot(input,self.weights) + self.biases\n    \n    def backward(self,input,grad_output):\n        # compute d f / d x = d f / d dense * d dense / d x\n        # where d dense/ d x = weights transposed\n        grad_input = np.dot(grad_output, self.weights.T)\n        \n        # compute gradient w.r.t. weights and biases\n        grad_weights = np.dot(input.T, grad_output)\n        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n        \n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        \n        # Here we perform a stochastic gradient descent step. \n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n        \n        return grad_input\n\n\nThe loss function\nSince we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\nIf we write down the expression for crossentropy as a function of softmax logits (a), you’ll see: \n  If we take a closer look, we’ll see that it can be rewritten as: \n  It’s called Log-softmax and it’s better than naive log(softmax(a)) in all aspects: * Better numerical stability * Easier to get derivative right * Marginally faster to compute\nSo why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\ndef softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n    \n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n    \n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits,reference_answers):\n    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n    \n    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- ones_for_answers + softmax) / logits.shape[0]\n\n\nFull network\nNow let’s combine what we’ve just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let’s import it!.\nimport keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) / 255.\n    X_test = X_test.astype(float) / 255.\n\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n\n## Let's look at some example\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n\nWe’ll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\nnetwork = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\ndef forward(network, X):\n    # Compute activations of all network layers by applying them sequentially.\n    # Return a list of activations for each layer. \n    \n    activations = []\n    input = X\n\n    # Looping through each layer\n    for l in network:\n        activations.append(l.forward(input))\n        # Updating input to last layer output\n        input = activations[-1]\n    \n    assert len(activations) == len(network)\n    return activations\n\ndef predict(network,X):\n    # Compute network predictions. Returning indices of largest Logit probability\n\n    logits = forward(network,X)[-1]\n    return logits.argmax(axis=-1)\n\ndef train(network,X,y):\n    # Train our network on a given batch of X and y.\n    # We first need to run forward to get all layer activations.\n    # Then we can run layer.backward going from last to first layer.\n    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n    \n    \n    # Get the layer activations\n    layer_activations = forward(network,X)\n    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n    logits = layer_activations[-1]\n    \n    # Compute the loss and the initial gradient\n    loss = softmax_crossentropy_with_logits(logits,y)\n    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n    \n    # Propagate gradients through the network\n    # Reverse propogation as this is backprop\n    for layer_index in range(len(network))[::-1]:\n        layer = network[layer_index]\n        \n        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n        \n    return np.mean(loss)\n\n\nTraining loop\nWe split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent.\nfrom tqdm import trange\ndef iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.random.permutation(len(inputs))\n    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\nfrom IPython.display import clear_output\ntrain_log = []\nval_log = []\nfor epoch in range(25):\n\n    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n        train(network,x_batch,y_batch)\n    \n    train_log.append(np.mean(predict(network,X_train)==y_train))\n    val_log.append(np.mean(predict(network,X_val)==y_val))\n    \n    clear_output()\n    print(\"Epoch\",epoch)\n    print(\"Train accuracy:\",train_log[-1])\n    print(\"Val accuracy:\",val_log[-1])\n    plt.plot(train_log,label='train accuracy')\n    plt.plot(val_log,label='val accuracy')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n    \nEpoch 24\nTrain accuracy: 1.0\nVal accuracy: 0.9809\n\nAs we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
  },
  {
    "objectID": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "href": "posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html",
    "title": "Leaf Disease detection by Tranfer learning using FastAI V1 library",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "href": "posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html",
    "title": "Real-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV",
    "section": "",
    "text": "Blog Transferred to Medium.com."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "",
    "text": "Understanding Synthetic Control and using Microsoft’s SparceSC package to run synthetic control on larger datasets."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#what-is-synthetic-control-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "What is Synthetic Control Method?",
    "text": "What is Synthetic Control Method?\nI will try to keep this part short and focus more on why Data scientists should care about such methods and how to use them on larger datasets based on practical experience using SparseSC package.\nThe Synthetic Control (SC) method is a statistical method used to estimate causal effects from binary treatments on observational panel (longitudinal) data. The method got quite a coverage by being described as “the most important innovation in the policy evaluation literature in the last few years” and got an article published in Washington Post - Seriously, here’s one amazing math trick to learn what can’t be known. “SC is a technique to create an artificial control group by taking a weighted average of untreated units in such a way that it reproduces the characteristics of the treated units before the intervention(treatment). The SC acts as the counterfactual for a treatment unit, and the estimate of a treatment effect is the difference between the observed outcome in the post-treatment period and the SC’s outcome.”1\n“One way to think of SC is as an improvement upon difference-in-difference (DiD) estimation. Typical DiD will compare a treated unit to the average of the control units. But often the treated unit does not look like a typical control (e.g., it might have a different growth rate), in which case the ‘parallel trend’ assumption of DiD is not valid. SC remedies this by choosing a smarter linear combination, rather than the simple average, to weigh more heavily the more similar units. SC’s assumption is if there are endogenous factors that affect treatment and future outcomes then you should be able to control them by matching past outcomes. The matching that SC provides can therefore deal with some problems in estimation that DiD cannot handle.”2\nHere is the link to the Causal inference book which I found most useful to understand the math behind SC- Causal Inference for The Brave and True by Matheus Facure - Chapter 15."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#why-should-any-data-scientist-care-about-this-method",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Why should any Data scientist care about this method?",
    "text": "Why should any Data scientist care about this method?\nOften as a Data Scientist, you will encounter situations as follows where running A/B testing is not feasible because of -\n\nLack of infrastructure\nLack of similar groups for running A/B testing (in case of evaluation of state policies, as there is no state equivalent of other)\nProviding unwanted advantage to one group over others. Sometimes running an A/B test can give an unfair advantage and lead you into anti-trust territory. For example, what if Amazon tries to charge differential pricing for different customers or apply different margins for their sellers for the same product?\n\nAs a data scientist, stakeholders may still ask you to estimate the impact of certain changes/treatments, and Synthetic controls can come to the rescue in this situation. For this reason, it is a valuable tool to keep in your algorithmic toolkit."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#problem-overview",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Problem Overview",
    "text": "Problem Overview\nWe will use the Proposition 99 data to explain the use case for this approach and also how to use the SparceSC library and its key features. “In 1988, California passed a famous Tobacco Tax and Health Protection Act, which became known as Proposition 99. Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements. To evaluate its effect, we can gather data on cigarette sales from multiple states and across a number of years. In our case, we got data from the year 1970 to 2000 from 39 states.”3\n\n\nCode\n# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport SparseSC\nfrom datetime import datetime\nimport warnings\nimport plotly.express as px\nimport plotly.graph_objects as pgo\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings('ignore')\n\n\nLet’s look at the data\n\n#Import data\ndata_dir = \"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/\"\ndf = pd.read_csv(data_dir + \"smoking_data\" + \".csv\").drop(columns=[\"lnincome\",\"beer\", \"age15to24\"])\ndf.head()\n\n\n\n\n\n\n\n\nstate\nyear\ncigsale\nretprice\n\n\n\n\n0\nAlabama\n1970.0\n89.8\n39.6\n\n\n1\nAlabama\n1971.0\n95.4\n42.7\n\n\n2\nAlabama\n1972.0\n101.1\n42.3\n\n\n3\nAlabama\n1973.0\n102.9\n42.1\n\n\n4\nAlabama\n1974.0\n108.2\n43.1\n\n\n\n\n\n\n\nWe have data per state as treatment unit and yearly (year column) per-capita sales of cigarettes in packs (cigsale column) and the cigarette retail price (retprice column). We are going to pivot this data so that each row is one treatment unit(state), and columns represent the yearly cigsale value.\n\ndf = df.pivot(index= 'state', columns = 'year', values = \"cigsale\")\ndf.head()\n\n\n\n\n\n\n\nyear\n1970.0\n1971.0\n1972.0\n1973.0\n1974.0\n1975.0\n1976.0\n1977.0\n1978.0\n1979.0\n1980.0\n1981.0\n1982.0\n1983.0\n1984.0\n1985.0\n1986.0\n1987.0\n1988.0\n1989.0\n1990.0\n1991.0\n1992.0\n1993.0\n1994.0\n1995.0\n1996.0\n1997.0\n1998.0\n1999.0\n2000.0\n\n\nstate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlabama\n89.8\n95.4\n101.1\n102.9\n108.2\n111.7\n116.2\n117.1\n123.0\n121.4\n123.2\n119.6\n119.1\n116.3\n113.0\n114.5\n116.3\n114.0\n112.1\n105.6\n108.6\n107.9\n109.1\n108.5\n107.1\n102.6\n101.4\n104.9\n106.2\n100.7\n96.2\n\n\nArkansas\n100.3\n104.1\n103.9\n108.0\n109.7\n114.8\n119.1\n122.6\n127.3\n126.5\n131.8\n128.7\n127.4\n128.0\n123.1\n125.8\n126.0\n122.3\n121.5\n118.3\n113.1\n116.8\n126.0\n113.8\n108.8\n113.0\n110.7\n108.7\n109.5\n104.8\n99.4\n\n\nCalifornia\n123.0\n121.0\n123.5\n124.4\n126.7\n127.1\n128.0\n126.4\n126.1\n121.9\n120.2\n118.6\n115.4\n110.8\n104.8\n102.8\n99.7\n97.5\n90.1\n82.4\n77.8\n68.7\n67.5\n63.4\n58.6\n56.4\n54.5\n53.8\n52.3\n47.2\n41.6\n\n\nColorado\n124.8\n125.5\n134.3\n137.9\n132.8\n131.0\n134.2\n132.0\n129.2\n131.5\n131.0\n133.8\n130.5\n125.3\n119.7\n112.4\n109.9\n102.4\n94.6\n88.8\n87.4\n90.2\n88.3\n88.6\n89.1\n85.4\n83.1\n81.3\n81.2\n79.6\n73.0\n\n\nConnecticut\n120.0\n117.6\n110.8\n109.3\n112.4\n110.2\n113.4\n117.3\n117.5\n117.4\n118.0\n116.4\n114.7\n114.1\n112.5\n111.0\n108.5\n109.0\n104.8\n100.6\n91.5\n86.7\n83.5\n79.1\n76.6\n79.3\n76.0\n75.9\n75.5\n73.4\n71.4\n\n\n\n\n\n\n\nLet’s observe how cigarettes sales per capita is trending over time w.r.t California and other states.\n\n\nCode\nplot_df = df.loc[df.index == \"California\"].T.reset_index(drop=False)\nplot_df[\"OtherStates\"] = df.loc[df.index != \"California\"].mean(axis=0).values\n\n\nfig = px.line(\n        data_frame = plot_df, \n        x = \"year\", \n        y = [\"California\",\"OtherStates\"], \n        template = \"plotly_dark\")\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[plot_df.California.min()*0.98,plot_df.OtherStates.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Gap in per-capita cigarette sales(in packs)\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Cigarette Sales Trend\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 1 - Cigarette sales comparison b/w California and other states\n\n\nAs we can see from the chart above, we can see that there is a general decline in cigarette sales after the 1980s, and with the introduction of Proposition 99, the decreasing trend accelerated for the state of California. We cannot say for sure if this is happening with any statistical significance, it is just something we observed by examining the chart above.\nTo answer the question of whether Proposition 99 influenced cigarette consumption, we will use the pre-intervention period (1970-1988) to build a synthetic control group that mimics California cigarette sales trend. Then, we will see how this synthetic control behaves after the intervention."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#fitting-synthetic-control-using-sparsesc-package",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Fitting Synthetic Control using SparseSC package",
    "text": "Fitting Synthetic Control using SparseSC package\nOn a high level SparseSC package provide two functions for fitting Synthetic controls i.e., fit() method and fit_fast() method. On a high level -\n\nfit() - This method tries to compute the weight jointly and results in SCs which are ‘optimal’. This is the most common method used in most of the code/libraries I have found but it is computationally expensive and takes a long time to run. Hence, does not scale for larger datasets.\nfit_fast()- This method tries to compute the weight separately by performing some non-matching analysis. This solution is much faster and often the only feasible method with larger datasets. The authors of this package recommend the fit_fast method to start with and only move towards the fit method if needed.\n\nThe SparseSC.fit_fast() method required at least three arguments -\n\nfeatures - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from pre-treatment (1970-1988), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntargets - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from post-treatment (1999-2000), and the value in the matrix is the metric of interest (in this case it is the cigsale value)\ntreatment_units - This is the list of integers containing the row index value of treated units\n\n\n\n\n\n\n\nNote\n\n\n\nNote that treatment units can be a list of multiple treatment indexes. Think of cases where the same treatment is applied to multiple groups, for example, what if proposition 99 was rolled in both California and Minnesota State, in this case, treatment_units will get [2, 15], which are the respective index of these states.\n\n\n\n## creating required features\nfeatures = df.iloc[:,df.columns &lt;= 1988].values\ntargets = df.iloc[:,df.columns &gt; 1988].values\ntreated_units = [idx for idx, val in enumerate(df.index.values) if val == 'California'] # [2]\n\n## Fit fast model for fitting Synthetic controls\nsc_model = SparseSC.fit_fast( \n    features=features,\n    targets=targets,\n    treated_units=treated_units\n)\n\nNow that we have fitted the model, let’s get the Synthetic Control output by using predict() function.\n\nresult = df.loc[df.index == 'California'].T.reset_index(drop=False)\nresult.columns = [\"year\", \"Observed\"] \nresult['Synthetic'] = sc_model.predict(df.values)[treated_units,:][0]\nresult.head(5)\n\n\n\n\n\n\n\n\nyear\nObserved\nSynthetic\n\n\n\n\n0\n1970.0\n123.0\n122.394195\n\n\n1\n1971.0\n121.0\n125.114849\n\n\n2\n1972.0\n123.5\n129.704372\n\n\n3\n1973.0\n124.4\n126.753988\n\n\n4\n1974.0\n126.7\n126.276394\n\n\n\n\n\n\n\nNow that we have our synthetic control, we can plot it with the outcome variable of the State of California.\n\n\nCode\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = [\"Observed\",\"Synthetic\"], \n        template = \"plotly_dark\",)\n\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result.Observed.min()*0.98,result.Observed.max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\nfig.update_layout(\n        title  = {\n            'text':\"Synthetic Control Assessment\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig - Assessment of Proposition 99 on state of California using Synthetic Control\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the pre-intervention period, the synthetic control does not reproduce the treatment exactly but follows the curve closely. This is a good sign, as it indicates that we are not overfitting. Also, note that we do see divergence after the intervention (introduction of Proposition 99) after 1988.\n\n\nWith the synthetic control groups in hand, we can estimate the treatment effect as the gap between the treated and the synthetic control outcomes.\n\n\nCode\nresult['California Effect'] = result.Observed - result.Synthetic\nfig = px.line(\n        data_frame = result, \n        x = \"year\", \n        y = \"California Effect\", \n        template = \"plotly_dark\",)\nfig.add_hline(0)\nfig.add_trace(\n    pgo.Scatter(\n        x=[1988,1988],\n        y=[result[\"California Effect\"].min()*0.98,result[\"California Effect\"].max()*1.02], \n        line={\n            'dash': 'dash',\n        }, name='Proposition 99'\n    ))\n\nfig.update_layout(\n        title  = {\n            'text':\"Difference across time\",\n            'y':0.95,\n            'x':0.5,\n        },\n        legend =  dict(y=1, x= 0.8, orientation='v'),\n        legend_title = \"\",\n        xaxis_title=\"Year\", \n        yaxis_title=\"Gap in Per-capita cigarette sales (in packs)\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig - Gap in Per-capita cigarette sales in California w.r.t Synthetic Control\n\n\n\n\nCode\nprint(f\"Effect of Proposition 99 w.r.t Synthetic Control =&gt; {np.round(result.loc[result.year==2000,'California Effect'].values[0],1)} packs\")\n\n\nEffect of Proposition 99 w.r.t Synthetic Control =&gt; -28.8 packs\n\n\nLooking at the chart above, we can observe that by the year 2000, Proposition 99 has reduced the sales of cigarettes by ~29 packs. Now we will figure out if this is statistically significant."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#making-inference",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Making Inference",
    "text": "Making Inference\nIn Synthetic control, to find if the effect we observed is significant or not, we run a placebo test. A placebo test is taking a random untreated unit and pretending all other units are in control and fit the Synthetic control over this randomly selected untreated unit to estimate the effect. Once we have repeated this placebo test multiple times, we can estimate the distribution of this randomly observed effect and see if the effect observed is significantly different from the placebo observed effect. In the SparceSC package, we can use the estimate_effects method to do this automatically for us. The estimate effects method takes a minimum of two arguments -\n\noutcomes - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), and each column is the period of both pre-treatment and post-treatment period and the value in the matrix is the metric of interest (in this case it is the cigsale value)\nunit_treatment_periods - Vector of treatment periods for each unit, (if a unit is never treated then use np.NaN if vector refers to periods by numerical index)\n\n\n## Creating unit treatment_periods\nunit_treatment_periods = np.full((df.values.shape[0]), np.nan)\nunit_treatment_periods[treated_units] = [idx for idx, colname in enumerate(df.columns) if colname == 1988][0]\n\n## fitting estimate effects method\nsc = SparseSC.estimate_effects(\n    outcomes = df.values,  \n    unit_treatment_periods = unit_treatment_periods, \n    max_n_pl=50, # Number of placebos\n    level=0.9 # Level for confidence intervals\n)\nprint(sc)\n\nPre-period fit diagnostic: Were the treated harder to match in the pre-period than the controls were.\nAverage difference in outcome for pre-period between treated and SC unit (concerning if p-value close to 0 ): \n1.8073663793403947 (p-value: 0.9743589743589743)\n\n(Investigate per-period match quality more using self.pl_res_pre.effect_vec)\n\nAverage Effect Estimation: -16.965374734951705 (p-value: 0.07692307692307693)\n\nEffect Path Estimation:\n -2.712194133284143 (p-value: 0.6923076923076923)\n-6.424223898557088 (p-value: 0.20512820512820512)\n-4.18303325159971 (p-value: 0.5128205128205128)\n-10.210104128966762 (p-value: 0.28205128205128205)\n-10.198518971790051 (p-value: 0.2564102564102564)\n-14.921163932323616 (p-value: 0.15384615384615385)\n-18.441946863077938 (p-value: 0.1282051282051282)\n-21.27793738802989 (p-value: 0.1794871794871795)\n-22.47075245885469 (p-value: 0.1794871794871795)\n-23.397467590595554 (p-value: 0.20512820512820512)\n-26.13637789807555 (p-value: 0.05128205128205128)\n-30.504443997992325 (p-value: 0.02564102564102564)\n-29.67170704122487 (p-value: 0.05128205128205128)\n\n \n\n\nThe estimate_effects method returns an object which will print the treatment effect of each post-treatment year and estimate the significance of the observed difference. The information printed can also be found in the pl_res_pre function of the returned object.\n\nprint(f\"Estimated effect of sales in California state in year 2000 because of preposition 99 is {np.round(sc.pl_res_post.effect_vec.effect[-1])}, \\\nwith a p-value of  {np.round(sc.pl_res_post.effect_vec.p[-1],2)}\")\n\nEstimated effect of sales in California state in year 2000 because of preposition 99 is -30.0, with a p-value of  0.05"
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#conclusion",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Conclusion",
    "text": "Conclusion\nHere are some key takeaways -\n\nSynthetic control allows us to combine multiple control units to make them resemble the treated unit. With synthetic control, we can estimate what would have happened to our treated unit in the absence of treatment.\nMicrosoft SparseSC library provides a fast and easy-to-use API to run synthetic control groups and allows us to run a placebo test to estimate the significance of the effects observed."
  },
  {
    "objectID": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#footnotes",
    "href": "posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html#footnotes",
    "title": "Causal inference with Synthetic Control using Python and SparseSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSparce SC documentation↩︎\nSparce SC documentation↩︎\nCausal Inference for The Brave and True by Matheus Facure - Chapter 15↩︎"
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html",
    "title": "Model calibration for classification tasks using Python",
    "section": "",
    "text": "A hands-on introduction to model calibration using Python."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#what-is-model-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "What is Model Calibration?",
    "text": "What is Model Calibration?\nWhen working with classification problems, machine learning models often produce a probabilistic outcome ranging between 0 to 1. This probabilistic output is then used by people to make decisions. Unfortunately, many machine learning models’ probabilistic outputs cannot be directly interpreted as the probability of an event happening. To achieve this outcome, the model needs to be calibrated.\nFormally, a model is said to be perfectly calibrated if, for any probability value p, a prediction of a class with confidence p is correct 100*p percent of the time. In more simple terms, the probabilistic output equals the probability of occurrence.\nLet us visualize a perfect calibrated and non-calibrated curve.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actual_prob\": np.arange(0,1.1, 0.1),\n    \"Calibrated\": np.arange(0,1.1, 0.1),\n    \"Non-Calibrated\":  [min(1,val - np.random.randn()*(1-idx)/20) for idx, val in enumerate(np.arange(0,1.1, 0.1))]\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"actual_prob\", \n        y = [\"Calibrated\", \"Non-Calibrated\"],\n        template = \"plotly_dark\")\n\nfig.update_layout(\n        title  = {\"text\": \"Calibrated vs Non-calibrated model\", \"y\": 0.98, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15), \n        legend=dict(yanchor=\"bottom\", y=0.95, orientation=\"h\")\n)\n\nfig.update_traces(patch={\"line\": {\"dash\": \"dash\"}}) \nfig.show(renderer=\"notebook\")\n\n\n                                                \nFig 1 - A visualization of calibrated and non-calibrated curve.\n\n\nOn the x-axis, we have model output p which is between 0 and 1 and on the y-axis, we have fractions of positive captured within the predicted probability bin. We expect a linear relationship with slope 1."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#why-do-we-need-model-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "Why do we need Model calibration?",
    "text": "Why do we need Model calibration?\nThere are many cases where model calibration is not required like in the case of ranking or selecting the top 20% for some targeting campaign. Calibrated models are especially important in making decision between multiple options of different magnitude or sizes, like expected value problems. In complex machine-learning decision engines, a machine-learning model might be used in conjunction with other machine-learning models.\nFor example, the business would like to prioritize upselling customer an additional product. So, a data scientist builds two ML models to estimate the probability of a customer buying two different products in addition to the products they are already purchasing.\n\n\n\nFig. 2: Upsell probability\n\n\nIn this case, Product 1 with $10 in revenue has an 80% probability of upselling, while Product 2 with $100 in revenue has a 60% chance of upselling. As a business, you might want to recommend Product 2 to the customer because of the higher expected value.\nTo do this expected value comparison between models, you need your models to be calibrated to output accurate probabilities."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#case-study-three-methods-for-calibration",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#case-study-three-methods-for-calibration",
    "title": "Model calibration for classification tasks using Python",
    "section": "Case study: Three methods for calibration",
    "text": "Case study: Three methods for calibration\nFor demonstration in this article, we will be solving a binary classification problem using the Adult Income Dataset from the UCI machine learning Repository1 to predict if a certain individual income based on various census information (education level, age, gender, occupation, and more) exceeds $50K/year. For this example, we will limit our scope to the United States and the following predictors:\n\nAge: continuous variable, individuals’ age\nOccupation: categorical variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nHoursPerWeek: continuous variable, amount of hours spent in a job per week\nEducation: categorical variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n\nIn the below code we:\n\nImport the libraries needed for this project\nLoad the data from our source\nLabel our columns\nFilter data to only our desired columns, and limit our scope just to the United States for simplicity\nLabel encode our categorical columns\nSet our random seed (for reproducibility)\nSplit our dataset into test and train\n\n\n\nCode\n## Importing required libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('dark_background')\n\ndef calibration_data(y_true, y_pred):\n    df = pd.DataFrame({'y_true':y_true, 'y_pred_bucket': (y_pred//0.05)*0.05 + 0.025})\n    cdf = df.groupby(['y_pred_bucket'], as_index=False).agg({'y_true':[\"mean\",\"count\"]})\n    return cdf.y_true.values[:,0][cdf.y_true.values[:,1]&gt;10], cdf.y_pred_bucket.values[cdf.y_true.values[:,1]&gt;10]\n\ndef label_encoder(df,columns):\n    '''\n    Function to label encode\n    Required Input - \n        - df = Pandas DataFrame\n        - columns = List input of all the columns which needs to be label encoded\n    Expected Output -\n        - df = Pandas DataFrame with lable encoded columns\n        - le_dict = Dictionary of all the column and their label encoders\n    '''\n    le_dict = {}\n    for c in columns:\n        print(\"Label encoding column - {0}\".format(c))\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values.astype('str')))\n        df[c] = lbl.transform(list(df[c].values.astype('str')))\n        le_dict[c] = lbl\n    return df, le_dict\n\n\n\ndf = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\ndf.columns = [\n    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n]\n\n## Filtering for Unites states\ndf = df.loc[df.NativeCountry == ' United-States',:]\n\n## Only - Taking required columns\ndf = df.loc[:,[\"Education\", \"Age\",\"Occupation\", \"HoursPerWeek\", \"Income\"]]\ndf, _ = label_encoder(df, columns = [\"Education\", \"Occupation\", \"Income\"])\nX = df.loc[:,[\"Education\",\"Age\", \"Occupation\", \"HoursPerWeek\"]]\ny = df.Income\n\nseed = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n\nLabel encoding column - Education\nLabel encoding column - Occupation\nLabel encoding column - Income\n\n\nNext, we:\n\nFit a random forest classifier on this data\nVisualize the calibration plot\n\n\n## Fitting the model on training data\nrf_model = RandomForestClassifier(random_state=seed)\nrf_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = rf_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Without calibration\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 3 - An example of non-calibrated classifier\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is over-predicting in lower deciles and under-predicting in higher deciles.\n\n\n\nCalibration method 1: Isotonic Regression\nIsotonic regression is a variation of ordinary least squares regression. Isotonic regression has the added constraints that the predicted values must always be increasing or decreasing, and the predicted values have to lie as close to the observation as possible. Isotonic regression is often used in situations where the relationship between the input and output variables is known to be monotonic, but the exact form of the relationship is not known. In our case, we want a monotonic behavior where the actual probabilities must always be increasing with increasing predicted probability.\nMathematically, we can write the Isotonic regression as following - \\[p_{calib} = \\operatorname{iso}(f(x)) + \\epsilon_i\\] Where-\n\n\\(\\operatorname{iso}\\) is the isotonic function\n\\(f(x)\\) is the predicted probability from the original classifier f\n\\(p_{calib}\\) is the calibrated probability\n\nand we want to optimize the following function - \\[\\operatorname{argmin} \\sum(p_{actual}-p_{calib})^2\\]\nIsotonic regressions is prone to overfitting, hence it works better when there is enough training data available.\nWe will be using CalibratedClassifierCVto calibrate our classifier. The isotonic method fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function. 2\n\n## training model using random forest and isotonic regression for calibration\ncalibrated_rf = CalibratedClassifierCV(RandomForestClassifier(random_state=seed), method = 'isotonic')\ncalibrated_rf.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_rf.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Isotonic\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 4 - An example of calibration using Isotonic regression\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is well calibrated once Isotonic regression is used because the actual probability lies much closer to the expected probability.\n\n\n\n\nCalibration method 2: Sigmoid method / Platt scaling\nThe sigmoid method, also known as Platt scaling, works by transforming the outputs of the classifier using a sigmoid function. The calibrated probabilities are obtained using the following sigmoid function - \\[p_{calib} = \\frac{1}{1+e^{(Af(x) + B)}}\\]\nWhere -\n\n\\(p_{calib}\\) is the calibrated probability\n\\(f(x)\\) is the predicted probability from the original classifier f\nA and B are scalar parameters that learned by the algorithm\n\nThe parameters A and B are estimated using a maximum likelihood method that optimizes on the same training set as that for the original classifier f. \nSigmoid/Platt scaling was originally invented for SVMs and it works well for other methods as well. This method is less prone to overfitting and should be preferred over Isotonic regression if you have less training data.\n\n## training model using random forest and sigmoid method for calibration\ncalibrated_sigmoid = CalibratedClassifierCV(\n     RandomForestClassifier(random_state=seed), method = 'sigmoid')\ncalibrated_sigmoid.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true_calib, prob_pred_calib  = calibration_data(y_test, calibrated_sigmoid.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true_calib,\n    \"predicted\": prob_pred_calib,\n    \"expected\": prob_pred_calib\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot : Sigmoid\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 5 - An example of calibration using sigmoid method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model is more calibrated than our default model but not as well as the isotonic method.\n\n\n\n\nCalibration method 3: Train with “Logloss” metric\nMany boosting based ensembling classifier models like GradientBoostingClassifier, LightGBM, and Xgboost use log_loss as their default loss function. Training with log_loss helps the output probabilities to be calibrated. To explain why log loss helps in calibration, let’s look at binary log loss function -\n\\[logloss = \\frac{-1}{N}\\sum_{i=1}^{N}y_i\\log{(p(y_i))} + (1-y_i)\\log{(1-p(y_i)})\\]\nNow let’s take an instance of observation where the true outcome is 1. Consider two predictions of 0.7 and 0.9 from the model. If we take the cutoff &gt; 0.5 both of these predictions are correct but the logloss for 0.7 and 0.9 prediction is 0.36 and 0.1 respectively. As we can see log loss penalizes uncertainty in prediction forcing the model to predict close to the actual outcome.\nTraining with logloss is preferred over fitting a calibration function as model is already calibrated and reduces overhead of training and maintaining an extra model for calibration.\n\n## Fitting the model on training data\ngb_model = GradientBoostingClassifier()\ngb_model.fit(X_train, y_train)\n\n## getting the output to visualize on test data\nprob_true, prob_pred  = calibration_data(y_true = y_test, \n                                          y_pred = gb_model.predict_proba(X_test)[:,1])\n\n\n\nCode\nchart_df = pd.DataFrame({\n    \"actuals\": prob_true,\n    \"predicted\": prob_pred,\n    \"expected\": prob_pred\n})\nfig = px.line(\n        data_frame = chart_df, \n        markers = True,\n        x = \"predicted\", \n        y = [\"actuals\", \"expected\"], \n        template = \"plotly_dark\")\nfig.update_layout(\n        title  = {\"text\": \"Calibration Plot: Using boosting method\", \"y\": 0.95, \"x\": 0.5},\n        xaxis_title=\"Predicted Probability\",\n        yaxis_title=\"Actual Probability\",\n        font = dict(size=15)\n)\nfig.show(renderer='notebook')\n\n\n                                                \nFig 6 - An example of calibration using boosting method\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we can see above, the model seems to be well calibrated. This is usually the case with most of the boosted methods as they use the log_loss function as their default loss function for binary classification."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#conclusion-and-practical-guidance",
    "title": "Model calibration for classification tasks using Python",
    "section": "Conclusion and Practical Guidance",
    "text": "Conclusion and Practical Guidance\nIn this article, we discussed what calibration is, in which applications it is important and why, and three different methods for calibration.\nWe demonstrated isotonic regression which can be used when there is enough training data available and model is not pre-calibrated.\nWe also demonstrated the Sigmoid method of calibration which works better when there is not enough training data and model is not pre-calibrated.\nAnd finally, we demonstrated the Logloss metric for calibration which is the default in boosting methods and is our preferred method since the generated model is pre-calibrated and doesn’t require training and maintaining an extra model for calibration."
  },
  {
    "objectID": "posts/2022-10-26-Model_Calibration/Model Calibration.html#footnotes",
    "href": "posts/2022-10-26-Model_Calibration/Model Calibration.html#footnotes",
    "title": "Model calibration for classification tasks using Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.↩︎\nScikit learn calibration documentation↩︎"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "",
    "text": "An introduction into what goes on in the pipe function of 🤗 hugging face diffusers library StableDiffusionPipeline function.\nThis is my second post of the Stable diffusion series, if you haven’t checked out the first one, you can read it here -  1. Part 1 - Introduction to Stable diffusion using 🤗 Hugging Face.\nIn this post, we will understand the basic components of a stable diffusion pipeline and their purpose. Later we will reconstruct StableDiffusionPipeline.from_pretrained function using these components. Let’s get started -"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#introduction",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "1 Introduction",
    "text": "1 Introduction\nDiffusion models as seen in the previous post can generate high-quality images. Stable diffusion models are a special kind of diffusion model called the Latent Diffusion model. They have first proposed in this paper High-Resolution Image Synthesis with Latent Diffusion Models. The original Diffusion model tends to consume a lot more memory, so latent diffusion models were created which can do the diffusion process in lower dimension space called Latent Space. On a high level, diffusion models are machine learning models that are trained to denoise random Gaussian noise step by step, to get the result i.e., image. In latent diffusion, the model is trained to do this same process in a lower dimension. \nThere are three main components in latent diffusion - \n\nA text encoder, in this case, a CLIP Text encoder\nAn autoencoder, in this case, a Variational Auto Encoder also referred to as VAE\nA U-Net\n\nLet’s dive into each of these components and understand their use in the diffusion process. The way I will be attempting to explain these components is by talking about them in the following three stages - \n\nThe Basics: What goes in the component and what comes out of the component - This is an important, and key part of the top-down learning approach of understanding “the whole game”\nDeeper explanation using 🤗 code. - This part will provide more understanding of what the model produces using the code\nWhat’s their role in the Stable diffusion pipeline - This will build your intuition around how this component fits in the Stable diffusion process. This will help your intuition on the diffusion process"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#clip-text-encoder",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "2 CLIP Text Encoder",
    "text": "2 CLIP Text Encoder\n\n2.1 Basics - What goes in and out of the component?\nCLIP(Contrastive Language–Image Pre-training) text encoder takes the text as an input and generates text embeddings that are close in latent space as it may be if you would have encoded an image through a CLIP model.\n\n\n\nFig. 2: CLIP text encoder\n\n\n\n\n2.2 Deeper explanation using 🤗 code\nAny machine learning model doesn’t understand text data. For any model to understand text data, we need to convert this text into numbers that hold the meaning of the text, referred to as embeddings. The process of converting a text to a number can be broken down into two parts -  1. Tokenizer - Breaking down each word into sub-words and then using a lookup table to convert them into a number  2. Token_To_Embedding Encoder - Converting those numerical sub-words into a representation that contains the representation of that text \nLet’s look at it through code. We will start by importing the relevant artifacts.\n\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nLet’s initialize a prompt and tokenize it.\n\nprompt = [\"a dog wearing hat\"]\ntok =tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \nprint(tok.input_ids.shape)\ntok\n\ntorch.Size([1, 77])\n\n\n{'input_ids': tensor([[49406,   320,  1929,  3309,  3801, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\nA tokenizer returns two objects in the form of a dictionary -  1. input_ids - A tensor of size 1x77 as one prompt was passed and padded to 77 max length. 49406 is a start token, 320 is a token given to the word “a”, 1929 to the word dog, 3309 to the word wearing, 3801 to the word hat, and 49407 is the end of text token repeated till the pad length of 77.  2. attention_mask - 1 representing an embedded value and 0 representing padding.\n\nfor token in list(tok.input_ids[0,:7]): print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token))}\")\n\n49406:&lt;|startoftext|&gt;\n320:a&lt;/w&gt;\n1929:dog&lt;/w&gt;\n3309:wearing&lt;/w&gt;\n3801:hat&lt;/w&gt;\n49407:&lt;|endoftext|&gt;\n49407:&lt;|endoftext|&gt;\n\n\nSo, let’s look at the Token_To_Embedding Encoder which takes the input_ids generated by the tokenizer and converts them into embeddings -\n\nemb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\nprint(f\"Shape of embedding : {emb.shape}\")\nemb\n\nShape of embedding : torch.Size([1, 77, 768])\n\n\ntensor([[[-0.3887,  0.0229, -0.0522,  ..., -0.4902, -0.3066,  0.0673],\n         [ 0.0292, -1.3242,  0.3074,  ..., -0.5264,  0.9766,  0.6655],\n         [-1.5928,  0.5063,  1.0791,  ..., -1.5283, -0.8438,  0.1597],\n         ...,\n         [-1.4688,  0.3113,  1.1670,  ...,  0.3755,  0.5366, -1.5049],\n         [-1.4697,  0.3000,  1.1777,  ...,  0.3774,  0.5420, -1.5000],\n         [-1.4395,  0.3137,  1.1982,  ...,  0.3535,  0.5400, -1.5488]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\nAs we can see above, each tokenized input of size 1x77 has now been translated to 1x77x768 shape embedding. So, each word got represented in a 768-dimensional space.\n\n\n2.3 What’s their role in the Stable diffusion pipeline\nStable diffusion only uses a CLIP trained encoder for the conversion of text to embeddings. This becomes one of the inputs to the U-net. On a high level, CLIP uses an image encoder and text encoder to create embeddings that are similar in latent space. This similarity is more precisely defined as a Contrastive objective. For more information on how CLIP is trained, please refer to this Open AI blog.\n\n\n\nFig. 3: CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. Credit - OpenAI"
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#vae---variational-auto-encoder",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "3 VAE - Variational Auto Encoder",
    "text": "3 VAE - Variational Auto Encoder\n\n3.1 Basics - What goes in and out of the component?\nAn autoencoder contains two parts -  1. Encoder takes an image as input and converts it into a low dimensional latent representation  2. Decoder takes the latent representation and converts it back into an image\n\n\n\nFig. 4: A Variational autoencoder. Original bird pic credit.\n\n\nAs we can see above, the Encoder acts like a compressor that squishes the image into lower dimensions and the decoder recreates the original image back from the compressed version.\n\n\n\n\n\n\nNote\n\n\n\nEncoder-Decoder compression-decompression is not lossless.\n\n\n\n\n3.2 Deeper explanation using 🤗 code\nLet’s start looking at VAE through code. We will start by importing the required libraries and defining some helper functions.\n\n\nCode\n## To import an image from a URL\nfrom fastdownload import FastDownload\n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n## Basic libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## Loading a VAE model\nfrom diffusers import AutoencoderKL\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nLet’s download an image from the internet.\n\np = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')\nimg = load_image(p)\nprint(f\"Dimension of this image: {np.array(img).shape}\")\nimg\n\nDimension of this image: (512, 512, 3)\n\n\n\n\n\n\n\n\n\n\nFig. 5: Original bird pic credit.\n\nNow let’s compress this image by using the VAE encoder, we will be using the pil_to_latents helper function.\n\nlatent_img = pil_to_latents(img)\nprint(f\"Dimension of this latent representation: {latent_img.shape}\")\n\nDimension of this latent representation: torch.Size([1, 4, 64, 64])\n\n\nAs we can see how the VAE compressed a 3 x 512 x 512 dimension image into a 4 x 64 x 64 image. That’s a compression ratio of 48x! Let’s visualize these four channels of latent representations.\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')\n\n\n\n\n\n\n\n\n\nFig. 6: Visualization of latent representation from VAE encoder. \n\nThis latent representation in theory should capture a lot of information about the original image. Let’s use the decoder on this representation to see what we get back. For this, we will use the latents_to_pil helper function.\n\ndecoded_img = latents_to_pil(latent_img)\ndecoded_img[0]\n\n\n\n\n\n\n\n\n\nFig. 7: Visualization of decoded latent representation from VAE decoder. \n\nAs we can see from the figure above VAE decoder was able to recover the original image from a 48x compressed latent representation. That’s impressive!\n\n\n\n\n\n\nNote\n\n\n\nIf you look closely at the decoded image, it’s not the same as the original image, notice the difference around the eyes. That’s why VAE encoder/decoder is not a lossless compression.\n\n\n\n\n3.3 What’s their role in the Stable diffusion pipeline\nStable diffusion can be done without the VAE component but the reason we use VAE is to reduce the computational time to generate High-resolution images. The latent diffusion models can perform diffusion in this latent space produced by the VAE encoder and once we have our desired latent outputs produced by the diffusion process, we can convert them back to the high-resolution image by using the VAE decoder. To get a better intuitive understanding of Variation Autoencoders and how they are trained, read this blog by Irhum Shafkat."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#u-net",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "4 U-Net",
    "text": "4 U-Net\n\n4.1 Basics - What goes in and out of the component?\nThe U-Net model takes two inputs -  1. Noisy latent or Noise- Noisy latents are latents produced by a VAE encoder (in case an initial image is provided) with added noise or it can take pure noise input in case we want to create a random new image based solely on a textual description  2. Text embeddings - CLIP-based embedding generated by input textual prompts \nThe output of the U-Net model is the predicted noise residual which the input noisy latent contains. In other words, it predicts the noise which is subtracted from the noisy latents to return the original de-noised latents.\n\n\n\nFig. 8: A U-Net representation.\n\n\n\n\n4.2 Deeper explanation using 🤗 code\nLet’s start looking at U-Net through code. We will start by importing the required libraries and initiating our U-Net model.\n\nfrom diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initializing a scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n## Setting number of sampling steps\nscheduler.set_timesteps(51)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nAs you may have noticed from code above, we not only imported unet but also a scheduler. The purpose of a schedular is to determine how much noise to add to the latent at a given step in the diffusion process. Let’s visualize the schedular function -\n\n\nCode\nplt.plot(scheduler.sigmas)\nplt.xlabel(\"Sampling step\")\nplt.ylabel(\"sigma\")\nplt.title(\"Schedular routine\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFig. 9: Sampling schedule visualization.\n\nThe diffusion process follows this sampling schedule where we start with high noise and gradually denoise the image. Let’s visualize this process -\n\n\nCode\nnoise = torch.randn_like(latent_img) # Random noise\nfig, axs = plt.subplots(2, 3, figsize=(16, 12))\nfor c, sampling_step in enumerate(range(0,51,10)):\n    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])\n    axs[c//3][c%3].set_title(f\"Step - {sampling_step}\")\n\n\n\n\n\n\n\n\n\n\nFig. 10: Noise progression through steps.\n\nLet’s see how a U-Net removes the noise from the image. Let’s start by adding some noise to the image.\n\n\nCode\nencoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\nlatents_to_pil(encoded_and_noised)[0]\n\n\n\n\n\n\n\n\n\n\nFig. 11: Noised Input fed to the U-Net.\n\nLet’s run through U-Net and try to de-noise this image.\n\n## Unconditional textual prompt\nprompt = [\"\"]\n\n## Using clip model to get embeddings\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad(): \n    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n    \n## Using U-Net to predict noise    \nlatent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()]).half()\nwith torch.no_grad():\n    noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n## Visualize after subtracting noise \nlatents_to_pil(encoded_and_noised- noise_pred)[0]\n\n\n\n\n\n\n\n\n\nFig. 12: De-Noised Output from U-Net\n\nAs we can see above the U-Net output is clearer than the original noisy input passed.\n\n\n4.3 What’s their role in the Stable diffusion pipeline\nLatent diffusion uses the U-Net to gradually subtract noise in the latent space over several steps to reach the desired output. With each step, the amount of noise added to the latents is reduced till we reach the final de-noised output. U-Nets were first introduced by this paper for Biomedical image segmentation. The U-Net has an encoder and a decoder which are comprised of ResNet blocks. The stable diffusion U-Net also has cross-attention layers to provide them with the ability to condition the output based on the text description provided. The Cross-attention layers are added to both the encoder and the decoder part of the U-Net usually between ResNet blocks. You can learn more about this U-Net architecture here."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#conclusion",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn this post, we saw the key components of a Stable diffusion pipeline i.e., CLIP Text encoder, VAE, and U-Net. In the next post, we will look at the diffusion process using these components. Read the next part here.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "href": "posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html#references",
    "title": "Stable diffusion using 🤗 Hugging Face - Looking under the hood",
    "section": "6 References",
    "text": "6 References\n\nStable Diffusion with 🧨 Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html",
    "title": "Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion",
    "section": "",
    "text": "An introduction to negative prompting and image to image stable diffusion pipeline using 🤗 hugging face diffusers library.\nThis is my fourth post of the Stable diffusion series, if you haven’t checked out the previous ones, you can read it here -  1. Part 1 - Stable diffusion using 🤗 Hugging Face - Introduction.  2. Part 2 - Stable diffusion using 🤗 Hugging Face - Looking under the hood.  3. Part 3 - Stable diffusion using 🤗 Hugging Face - Putting everything together\nIn previous posts, I went over all the key components of Stable Diffusion and how to get a prompt to image pipeline working. In this post, I will show how to edit the prompt to image function to add additional functionality to our Stable diffusion pipeline i.e., Negative prompting and Image to Image pipeline. Hopefully, this will provide enough motivation to play around with this function and conduct your research."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-1-negative-prompt",
    "title": "Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion",
    "section": "1 Variation 1: Negative Prompt",
    "text": "1 Variation 1: Negative Prompt\n\n1.1 What is negative prompting?\nA negative prompt is an additional capability we can add to our model to tell the stable diffusion model what we don’t want to see in the generated image. This feature is popular to remove anything a user doesn’t want to see from the original generated image.\n\n\n\nFig. 2: Negative prompt example\n\n\n\n\n1.2 Understanding negative prompting through code\nLet’s start by importing the required libraries and helper functions. All of this was already used and explained in the previous part 2 and part 3 of the series.\n\n\nCode\nimport torch, logging\n\n## disable warnings\nlogging.disable(logging.WARNING)  \n\n## Imaging  library\nfrom PIL import Image\nfrom torchvision import transforms as tfms\n\n\n## Basic libraries\nfrom fastdownload import FastDownload\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\nimport shutil\nimport os\n\n## For video display\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\n## Import the CLIP artifacts \nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n\n## Initiating tokenizer and encoder.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initiating the VAE\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Initializing a scheduler and Setting number of sampling steps\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(50)\n\n## Initializing the U-Net model\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n## Helper functions\ndef load_image(p):\n    '''\n    Function to load images from a defined path\n    '''\n    return Image.open(p).convert('RGB').resize((512,512))\n\ndef pil_to_latents(image):\n    '''\n    Function to convert image to latents\n    '''\n    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n    return init_latent_dist\n\ndef latents_to_pil(latents):\n    '''\n    Function to convert latents to images\n    '''\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef text_enc(prompts, maxlen=None):\n    '''\n    A function to take a texual promt and convert it into embeddings\n    '''\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\n\nNow we are going to change the prompt_2_img function from part 3 by passing an additional function neg_prompts. The way negative prompt works is by using user-specified text instead of an empty string for unconditional embedding(uncond) when doing sampling.\n\n\n\nFig. 3: Negative prompt code change\n\n\nSo let’s make this change and update our prompt_2_img function.\n\ndef prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    \n    # Defining batch size\n    bs = len(prompts) \n    \n    # Converting textual prompts to embedding\n    text = text_enc(prompts) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"] * bs, text.shape[1])\n    else: uncond =  text_enc(neg_prompts, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Initiating random noise\n    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Adding noise to the latents \n    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nLet’s see if the function works as intended.\n\n## Image without neg prompt\nimages = [None, None]\nimages[0] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"\"],steps=50, save_int=False)[0]\nimages[1] = prompt_2_img(prompts = [\"A dog wearing a white hat\"], neg_prompts=[\"White hat\"],steps=50, save_int=False)[0]\n    \n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate(images): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"A dog wearing a white hat\")\n    else: axs[c].set_title(f\"Neg prompt - white hat\")\n\n\n\n\n\n\n\n\n\nFig. 4: Visualization of negative prompting. Left SD generated with prompt “A dog wearing a white hat” and on right the same caption with negative prompt of “White hat”\n\nAs we can see it can be a really handy feature to fine-tune the image to your liking. You can also use it to generate a pretty realistic face by being really descriptive as this Reddit post. Let’s try it -\n\nprompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']\nneg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']\nimages = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)\nimages[0]\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 5: An image generated using negative prompting.\n\n Pretty neat! I hope this gives you some ideas on how to get going with your own variations of stable diffusion. Now let’s look at another variation of Stable diffusion."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline",
    "title": "Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion",
    "section": "2 Variation 2: Image to Image pipeline",
    "text": "2 Variation 2: Image to Image pipeline\n\n2.1 What is an image to image pipeline?\nAs seen above, prompt_2_img functions start generating an image from random gaussian noise, but what if we feed an initial seed image to guide the diffusion process? This is exactly how the image to image pipeline works. Instead of purely relying on text conditioning for the output image, we can use an initial seed image mix it with some noise (which can be guided by a strength parameter), and then run the diffusion loop.\n\n\n\nFig. 6: Image to image pipeline example.\n\n\n\n\n2.2 Understanding image to image prompting through code\nNow we are going to change the prompt_2_img function defined above. We will introduce two more parameters to our prompt_2_img_i2i function -  1. init_img: Which is going to be the Image object containing the seed image  2. strength: This parameter will take a value between 0 and 1. The higher the value less the final image is going to look similar to the seed image.\n\ndef prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):\n    \"\"\"\n    Diffusion process to convert prompt to image\n    \"\"\"\n    # Converting textual prompts to embedding\n    text = text_enc(prompt) \n    \n    # Adding an unconditional prompt , helps in the generation process\n    if not neg_prompts: uncond =  text_enc([\"\"], text.shape[1])\n    else: uncond =  text_enc(neg_prompt, text.shape[1])\n    emb = torch.cat([uncond, text])\n    \n    # Setting the seed\n    if seed: torch.manual_seed(seed)\n    \n    # Setting number of steps in scheduler\n    scheduler.set_timesteps(steps)\n    \n    # Convert the seed image to latent\n    init_latents = pil_to_latents(init_img)\n    \n    # Figuring initial time step based on strength\n    init_timestep = int(steps * strength) \n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=\"cuda\")\n    \n    # Adding noise to the latents \n    noise = torch.randn(init_latents.shape, generator=None, device=\"cuda\", dtype=init_latents.dtype)\n    init_latents = scheduler.add_noise(init_latents, noise, timesteps)\n    latents = init_latents\n    \n    # Computing the timestep to start the diffusion loop\n    t_start = max(steps - init_timestep, 0)\n    timesteps = scheduler.timesteps[t_start:].to(\"cuda\")\n    \n    # Iterating through defined steps\n    for i,ts in enumerate(tqdm(timesteps)):\n        # We need to scale the i/p latents to match the variance\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        \n        # Predicting noise residual using U-Net\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            \n        # Performing Guidance\n        pred = u + g*(t-u)\n        \n        # Conditioning  the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n        # Saving intermediate images\n        if save_int: \n            if not os.path.exists(f'./steps'):\n                os.mkdir(f'./steps')\n            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n            \n    # Returning the latent representation to output an image of 3x512x512\n    return latents_to_pil(latents)\n\nInstead of using random noise, you will notice we use the strength parameter to figure out how much noise to add and also the number of steps to run the diffusion loop for. The amount of noise is calculated by multiplying strength(default = 0.8) with steps (default = 50) which is the 10th (50 - 50 * 0.8) step and running the diffusion loop for 40(50*0.8) remaining steps. Let’s load an initial image and pass it through the prompt_2_img_i2i function.\n\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\nimage = Image.open(p).convert('RGB').resize((512,512))\nprompt = [\"Wolf howling at the moon, photorealistic 4K\"]\nimages = prompt_2_img_i2i(prompts = prompt, init_img = image)\n\n\n\n\n/home/aayush/miniconda3/envs/fastai/lib/python3.9/site-packages/diffusers/schedulers/scheduling_lms_discrete.py:146: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  integrated_coeff = integrate.quad(lms_derivative, self.sigmas[t], self.sigmas[t + 1], epsrel=1e-4)[0]\n\n\n\n\nCode\n## Plotting side by side\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfor c, img in enumerate([image, images[0]]): \n    axs[c].imshow(img)\n    if c == 0 : axs[c].set_title(f\"Initial image\")\n    else: axs[c].set_title(f\"Image 2 Image output\")\n\n\n\n\n\n\n\n\n\n\nFig. 7: Visualization of image to image pipeline. Left is initial image passed in img2img pipeline and right is the output of the img2img pipeline.\n\nWe can see our prompt_2_img_i2i function creates a pretty epic image from the initial sketch provided."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#conclusion",
    "title": "Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nI hope this gives a good overview of how to tweak the prompt_2_img function to add additional capabilities to your stable diffusion loop. The understanding of this lower-level function is useful for trying your own idea to improve stable diffusion or implement new papers which I might cover in my next post.\nI hope you enjoyed reading it, and feel free to use my code and try it out for generating your images. Also, if there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com."
  },
  {
    "objectID": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "href": "posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#references",
    "title": "Stable diffusion using 🤗 Hugging Face - Variations of Stable Diffusion",
    "section": "4 References",
    "text": "4 References\n\nFast.ai course - 1st Two Lessons of From Deep Learning Foundations to Stable Diffusion\nStable Diffusion with 🧨 Diffusers\nGetting Started in the World of Stable Diffusion"
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html",
    "title": "Python OOPs fundamentals",
    "section": "",
    "text": "An introduction to Object Oriented programming using Python.\nIncreasingly it’s becoming important for Data professionals to become better at programming and modern programming is centered around Object Oriented programming paradigm. This article helps in explaining some important programming concepts which are mostly language agnostic but we will be using Python in this article.\nObject-oriented programming (OOPs) is a programming paradigm that relies on the concept of classes and objects. The basic idea of OOP is to divide a sophisticated program into a number of objects that interact with each other to achieve the desired functionality. There are several advantages of using OOP for data science:\nOverall, OOP can help data professionals organize and manage their code more effectively, making it easier to develop and maintain data science projects. Let’s dive into the OOPs concept."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#what-are-objects-and-classes",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#what-are-objects-and-classes",
    "title": "Python OOPs fundamentals",
    "section": "1 What are Objects and Classes?",
    "text": "1 What are Objects and Classes?\nClasses are the blueprint for defining an Object. While an Object is a collection of data/properties and their behaviors/methods.\nFor example- Think of a class Bulb that will have a state (On/Off) and methods to turnOn and turnoff the bulb.\n\nclass Bulb():\n    def __init__(self, onOff=False): self.onOff = onOff    \n    def turnOn(self): self.onOff = True\n    def turnOff(self): self.onOff = False\n\nNow we can create multiple bulb objects from this Bulb class.\n\nb1 = Bulb(onOff=True)\nb2 = Bulb()\nprint(f\"Bulb 1 state is :{b1.onOff}, Bulb 2 state is :{b2.onOff}\")\n\nBulb 1 state is :True, Bulb 2 state is :False\n\n\nb1 and b2 are objects of the class Bulb. Let’s use the turnOn and turnOff methods to update the bulb properties.\n\nb1.turnOff(); b2.turnOn()\nprint(f\"Bulb 1 state is :{b1.onOff}, Bulb 2 state is :{b2.onOff}\")\n\nBulb 1 state is :False, Bulb 2 state is :True\n\n\nWe can see from the example above, a Bulb object contains the onOff property. Properties are variables that contain information regarding the object of a class and Methods like turnOn and turnOff in our Bulb class are functions that have access to the properties of a class. Methods can accept additional parameters, modify properties and return values."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-and-instance-variables",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-and-instance-variables",
    "title": "Python OOPs fundamentals",
    "section": "2 Class and Instance variables",
    "text": "2 Class and Instance variables\nIn Python, properties can be defined in two ways -\n\nClass Variables - Class variables are shared by all objects of the class. A change in the class variable will change the value of that property in all the objects of the class.\nInstance Variables - Instance variables are unique to each instance or object of the class. A change in instance variable will change the value of the property in that specific object only.\n\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n    \ne1 = Employee('Aayush')\ne2 = Employee('John')\n\nprint(f'Name :{e1.name}')\nprint(f'Company Name: {e1.companyName}')\nprint(f'Name :{e2.name}')\nprint(f'Company Name: {e2.companyName}')\n\nName :Aayush\nCompany Name: Microsoft\nName :John\nCompany Name: Microsoft\n\n\nWe can see above, the class variable is defined outside of the initializer and the instance variable is defined inside the initializer.\n\nEmployee.companyName = \"Amazon\"\nprint(e1.companyName, e2.companyName)\n\nAmazon Amazon\n\n\nWe can see above changing a class variable in the Employee class changes the class variable in all objects of the class. Most of the time we will be using instance variables but knowledge about class variables can come in handy. Let’s look at an interesting use of class variable -\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    companyEmployees = []\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n        self.companyEmployees.append(self.name)\n    \ne1 = Employee('Aayush')\ne2 = Employee('John')\n\nprint(f'Name :{e1.name}')\nprint(f'Team Members: {e1.companyEmployees}')\nprint(f'Name :{e2.name}')\nprint(f'Company Name: {e2.companyEmployees}')\n\nName :Aayush\nTeam Members: ['Aayush', 'John']\nName :John\nCompany Name: ['Aayush', 'John']\n\n\nWe can see above, we are saving all objects of the Employee class in companyEmployees which is a list shared by all objects of the class Employee."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-static-and-instance-methods",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#class-static-and-instance-methods",
    "title": "Python OOPs fundamentals",
    "section": "3 Class, Static and Instance methods",
    "text": "3 Class, Static and Instance methods\nIn Python classes, we have three types of methods -\n\nClass Methods - Class methods work with class variables and are accessible using the class name rather than its object.\nStatic Methods - Static methods are methods that are usually limited to class only and not their objects. They don’t typically modify or access class and instance variables. They are used as utility functions inside the class and we don’t want the inherited class to modify them.\nInstance Methods - Instance methods are the most used methods and have access to instance variables within the class. They can also take new parameters to perform desired operations.\n\n\nclass Employee:\n    # Creating a class variable\n    companyName = \"Microsoft\"\n    companyEmployees = []\n    \n    def __init__(self, name):\n        # creating an instance variable\n        self.name = name\n        self.companyEmployees.append(self.name)\n    \n    @classmethod\n    def getCompanyName(cls): # This is a class method\n         return cls.companyName\n    \n    @staticmethod\n    def plusTwo(x): # This is a static method\n        return x+2\n    \n    def getName(self): # This is an instance method\n        return self.name\n    \ne1 = Employee('Aayush')\nprint(f\"Calling class method. Company name is {e1.getCompanyName()}\")\nprint(f\"Calling Static method. {e1.plusTwo(2)}\")\nprint(f\"Calling instance method. Employee name is {e1.getName()}\")\n\nCalling class method. Company name is Microsoft\nCalling Static method. 4\nCalling instance method. Employee name is Aayush\n\n\nWe can see above we use the @classmethod decorator to define the class method. cls is used to refer to the class just as self is used to refer to the object of the class. The class method at least takes one argument cls.\n\n\n\n\n\n\nNote\n\n\n\nWe can use any other name instead of cls but cls is used as a convention.\n\n\nWe use @staticmethod decorator to define static class plusTwo. We can see that static methods don’t take any argument like self and cls.\nThe most commonly used methods are instance methods and they can be defined without a decorator within the class. Just like the class method they take at least one argument which is self by convention.\n\n\n\n\n\n\nNote\n\n\n\nWe can use any other name instead of self but self is used as a convention."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#access-modifiers",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#access-modifiers",
    "title": "Python OOPs fundamentals",
    "section": "4 Access Modifiers",
    "text": "4 Access Modifiers\nAccess modifiers limit access to the variables and functions of a class. There are three types of access modifiers - public, protected, and private.\n\n4.1 Public Attributes\nPublic attributes are those methods and properties which can be accessed anywhere inside and outside of the class. By default, all the member variables and functions are public.\n\nclass Employee:\n    def __init__(self, name):\n        self.name = name ## Public variable\n        \n    def getName(self): ## Public method\n        return self.name\n\ne1 = Employee(\"Aayush\")\nprint(f\"Employee Name: {e1.getName()}\")\n\nEmployee Name: Aayush\n\n\nIn the case above, both property name and method getName are public attributes.\n\n\n4.2 Protected Attributes\nProtected attributes are similar to public attributes which can be accessed within the class and also available to subclasses. The only difference is the convention, which is to define each protected member with a single underscore “_”.\n\nclass Employee:\n    def __init__(self, name, project):\n        self.name = name ## Public variable\n        self._project = project ## Protected variable\n        \n    def getName(self): ## Public method\n        return self.name\n    \n    def _getProject(self): ## Protected method\n        return self._project\n    \ne1 = Employee(\"Aayush\", \"Project Orland\")\nprint(f\"Employee Name: {e1.getName()}\")\nprint(f\"Project Name: {e1._getProject()}\")\n\nEmployee Name: Aayush\nProject Name: Project Orland\n\n\nIn the case above, both property _project and method _getProject are protected attributes.\n\n\n4.3 Private Attributes\nPrivate attributes are accessible within the class but not outside of the class. To define a private attribute, prefix the method or property with the double underscore”_“.\n\nclass Employee:\n    def __init__(self, name, project, salary):\n        self.name = name ## Public variable\n        self._project = project ## Protected variable\n        self.__salary = salary\n        \n    def getName(self): ## Public method\n        return self.name\n    \n    def _getProject(self): ## Protected method\n        return self._project\n    \n    def __getSalary(self): ## Protected method\n        return self.__salary\n    \ne1 = Employee(\"Aayush\", \"Project Orland\", \"3500\")\nprint(f\"Employee Name: {e1.getName()}\")\nprint(f\"Project Name: {e1.__getSalary()}\") \n\nEmployee Name: Aayush\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [14], in &lt;cell line: 18&gt;()\n     16 e1 = Employee(\"Aayush\", \"Project Orland\", \"3500\")\n     17 print(f\"Employee Name: {e1.getName()}\")\n---&gt; 18 print(f\"Project Name: {e1.__getSalary()}\")\n\nAttributeError: 'Employee' object has no attribute '__getSalary'\n\n\n\nWe can see above, __salary property and __getSalary method are both private attributes and when we call them outside of the class they throw an error that the 'Employee' object has no attribute '__getSalary'."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#encapsulation",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#encapsulation",
    "title": "Python OOPs fundamentals",
    "section": "5 Encapsulation",
    "text": "5 Encapsulation\nEncapsulation in OOP refers to binding data and the methods to manipulate that data together in a single unit, that is, class. Encapsulation is usually used to hide the state and representation of the object from the outside. A good use of encapsulation is to make all properties private of a class to prevent direct access from outside and use public methods to let the outside world communicate with the class.\n\nclass Employee:\n    def __init__(self, name, project, salary):\n        self.__name = name ## Public variable\n        self.__project = project ## Protected variable\n        self.__salary = salary\n        \n    def getName(self): ## Public method\n        return self.__name\n    \ne1 = Employee(\"Aayush\", \"Project Orland\", \"3500\")\nprint(f\"Employee Name: {e1.getName()}\")\n\nEmployee Name: Aayush\n\n\nEncapsulation has several advantages -\n\nProperties of the class can be hidden from the outside world\nMore control over what the outside world can access from the class\n\nA good example of encapsulation would be an access control class based on username and password.\n\nclass Auth:\n    def __init__(self, userName=None, password=None):\n        self.__userName = userName\n        self.__password = password\n        \n    def login(self, userName, password):\n        if (self.__userName == userName) and (self.__password == password):\n            print (f\"Access granted to {userName}\")\n        else:\n            print(\"Invalid credentials\")\n            \ne1 = Auth(\"Aayush\", \"whatever\")\ne1.login(\"Aayush\", \"whatever\") ## This will grant access\n\ne1.login(\"Aayush\", \"aasdasd\") ## This will say invalid creds\ne1.__password ## This will raise an error as private properties can't be accessed from outside.\n\nAccess granted to Aayush\nInvalid credentials\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [4], in &lt;cell line: 16&gt;()\n     13 e1.login(\"Aayush\", \"whatever\") ## This will grant access\n     15 e1.login(\"Aayush\", \"aasdasd\") ## This will say invalid creds\n---&gt; 16 e1.__password\n\nAttributeError: 'Auth' object has no attribute '__password'\n\n\n\nAs we can see above __username and __password are protected properties and can only be used by the class to grand or reject access requests."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#inheritance",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#inheritance",
    "title": "Python OOPs fundamentals",
    "section": "6 Inheritance",
    "text": "6 Inheritance\nInheritance provides a way to create new classes from the existing classes. The new class will inherit all the non-private attributes(properties and methods) from the existing class. The new class can be called a child class and the existing class can be called a parent class.\n\nimport math\nclass Shape:\n    def __init__(self, name):\n        self.name = name\n        \n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    ## Overiding the getArea function\n    def getArea(self):\n        return self.edge**2\n    \nclass Circle(Shape):\n    def __init__(self, radius):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Circle\")\n        self.radius = radius\n    ## Overiding the getArea function\n    def getArea(self):\n        return math.pi * (self.radius**2)\n    \nobj1 = Square(4)\nobj1.printDetails()\n\nobj2 = Circle(3)\nobj2.printDetails()\n\nThis shape is called Square and area is 16.\nThis shape is called Circle and area is 28.274333882308138.\n\n\nWe can see above we defined a parent class Shape and then we inherited it to create a Square and Circle child class. While defining the Square and Circle class we overwrote the getArea function pertinent to the class but we used the printDetails function from the parent class to print details about child classes. The more common example in the machine learning world would be to create your own models in Pytorch where we inherit from nn.Module class to create a new model.\n\n6.1 Use of super() Function\nsuper() function comes into play when we implement inheritance. The super() function is used to refer to the parent class without explicitly naming the class. super() function can be used to access parent class properties, calling the parent class, and can be used as initializers. Let’s look at the example above and see how we can modify the Square class to use super() function.\n\nclass Shape:\n    maxArea = 100\n    def __init__(self, name): self.name = name\n    def getArea(self): pass\n    def printDetails(self): \n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \n\nclass Square(Shape):\n    maxArea = 50\n    def __init__(self, edge):\n        super().__init__(name = \"Square\") ## Initializing parent class\n        self.edge = edge\n    \n    def getName(self):\n        return super().maxArea\n    \n    def getArea(self):\n        return self.edge**2\n    \n    def printDetails(self):\n        super().printDetails() ## Calling a parent class function\n        print(f\"Max area from Shape class: {super().maxArea}\") ## Accessing parent class property\n        print(f\"Max area from Square class: {self.maxArea}\")\n\nobj1 = Square(4)\nobj1.getName()\nobj1.printDetails()\n\nThis shape is called Square and area is 16.\nMax area from Shape class: 100\nMax area from Square class: 50\n\n\nAs we can see in the example above we have used -\n\nsuper().__init__ to initialize the parent Shape class\nsuper().printDetails() function to use a method from parent class\nsuper().maxArea to access a property of a parent class\n\nThere are many advantages of inheritance -\n\nReusability - Inheritance makes the code reusable. Common methods and properties can be stored in a parent class and child classes can inherit these methods.\nModification - Code modification becomes easier if we use inheritance, if we want to make a change in the base class function it will be propagated to the child classes.\nExtensibility - We can derive new classes from the old ones by keeping things we need in the derived class."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#polymorphism",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#polymorphism",
    "title": "Python OOPs fundamentals",
    "section": "7 Polymorphism",
    "text": "7 Polymorphism\nPolymorphism refers to the same object exhibiting different forms and behaviors. For example consider our shape class which could be a square, rectangle, polygon, etc. Instead of writing multiple functions to get the area of these shapes, we can use a common function like getArea() and implement this function in the derived class.\n\nimport math\nclass Shape:\n    def __init__(self, name):\n        self.name = name\n        \n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    ## Overiding the getArea function\n    def getArea(self):\n        return self.edge**2\n    \nclass Circle(Shape):\n    def __init__(self, radius):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Circle\")\n        self.radius = radius\n    ## Overiding the getArea function\n    def getArea(self):\n        return math.pi * (self.radius**2)\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nobj2 = Circle(3)\nprint(f\"Area of this {obj2.name} is {obj2.getArea()}\")\n\nArea of this Square is 16\nArea of this Circle is 28.274333882308138\n\n\nAs we can see above there is a pre-defined dummy method called getArea in the Shape class. We override this method in the Square and Circle class. This technique is called method overriding. The advantage of method overriding is that the derived class can write its own specific implementation based on the requirement while using the same function name.\n\n7.1 Abstract base classes\nAbstract base classes define a set of methods and properties that a class must implement in order to inherit the parent class. This is a useful technique to enforce that certain functions within the derived class must exist. To define an abstract base class, we use the abc module. The abstract base class inherits from the built-in ABC class and we use the decorator @abstractmethod to declare an abstract method.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    def __init__(self, name):\n        self.name = name\n    \n    @abstractmethod\n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-9-047869063ef7&gt; in &lt;module&gt;\n     18         self.edge = edge\n     19 \n---&gt; 20 obj1 = Square(4)\n     21 print(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nTypeError: Can't instantiate abstract class Square with abstract methods getArea\n\n\n\nWe can see above that we have created a Shape class from the ABC class which has an abstract method getArea. Since our child class Square didn’t have getArea implemented we get an error instantiating this class.\n\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    def __init__(self, name):\n        self.name = name\n    \n    @abstractmethod\n    def getArea(self):\n        pass\n    \n    def printDetails(self):\n        print(f\"This shape is called {self.name} and area is {self.getArea()}.\")\n        \nclass Square(Shape):\n    def __init__(self, edge):\n        ## calling the constructor from parent class Shape\n        Shape.__init__(self, name = \"Square\")\n        self.edge = edge\n    \n    def getArea(self): return self.edge**2\n    \nobj1 = Square(4)\nprint(f\"Area of this {obj1.name} is {obj1.getArea()}\")\n\nArea of this Square is 16\n\n\nWe can see above, once we implemented the getArea method, the code runs fine.\n\nAbstract base classes serve as a blueprint for derived classes to implement methods that are required to run the function appropriately."
  },
  {
    "objectID": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#conclusion",
    "href": "posts/2022-12-20-PythonFundamentals/Python OOPs Fundamentals.html#conclusion",
    "title": "Python OOPs fundamentals",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nIn this article, we learned about what is object-oriented programming and key concepts using Python. A good understanding of these concepts will lay a solid foundation for any software professional to write and understand python code better.\nI hope you enjoyed reading it. If there is any feedback on the code or just the blog post, feel free to comment below or reach out on LinkedIn."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "",
    "text": "A collection of resources while preparing for MLE interviews at Meta or other big tech companies.\nBlog Resources: MLE Interview Prep Template. This resource contains a template to track your coding questions, ML system design notes, and a list of questions to prepare for behavioral interviews.\nNot long ago, I transitioned from a Senior ML Scientist role at Microsoft to a Machine Learning Engineer position at Meta, and the journey was anything but quick. The preparation process was extensive, especially since it was my first experience with LeetCode-style coding interviews and ML system design interviews. While there are many resources available for preparation, I’ll be sharing the ones that helped me navigate and succeed in this challenging process.\nI will try to cover the following -"
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-coding-problems-in-interview",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-coding-problems-in-interview",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "2.1 A structured approach to solving coding problems in interview",
    "text": "2.1 A structured approach to solving coding problems in interview\nWhen tackling a coding problem, following this structured approach can be very helpful:\n\nAsk Clarifying Questions(~3mins): When the problem is presented, read it aloud to ensure you fully understand the requirements before jumping to a solution. Ask follow-up questions to clarify any ambiguities. This might involve discussing test cases, considering edge cases, and understanding the expected input range or type. For example, think about how the solution should handle null inputs or extreme values. The ideal state is to get an alignment with your interviewer by writing out some test cases and expected output for the same.\nPlan Your Approach(~5 mins): Outline your solution strategy and explain it to your interviewer while typing it out in the shared text window. Break down the problem into smaller parts if possible and decide on the most appropriate algorithm or data structure and discuss any trade-offs you are making and write down potential time and space complexity of the solution you are proposing. Once your interviewer agrees with your approach and then ask permission to code it out.\nWrite the code(~5 mins): Implement your solution, keeping your code clean and well-organized. As you code, ensure that you handle edge cases. Make sure to name your functions, classes and variables appropriately so anybody reading your code can follow.\nPseudo run your solution(~2 mins): Manually run your code against various test cases while explaining it to your interviewer, including both typical and edge cases, to ensure it behaves as expected. This will help you find potential bugs and an opportunity to correct them before your interviewer points it out.\nClose(~2 mins): Explain time and space complexity of the solution and answer any follow up questions your interviewer might have.\n\nBy following these steps, you can effectively navigate coding problems and demonstrate a clear, methodical problem-solving approach.\n\n\n\n\n\n\nTipUseful Tips for Coding Interviews\n\n\n\n\n\n\nKeep your introduction brief (~30 seconds) to have more time for solving the problem. For example - “Hey I am [Your Name], I currently work as [Title] at [Employer Name]. I have been working here from past [N] years. I’m now seeking new opportunities, which brings me here today.”\nIf you’re running out of time, it’s acceptable to manually walk through one or two test cases with your interviewer. You can then suggest moving on to the next question to ensure you cover everything within the allotted time.\nIt’s okay to ask for help from an interviewer if you are stuck on a problem\n\n\n\n\nAs you can see from above, coding rounds are really fast paced and you need to be well prepared to get through it. That brings us to prepration."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-coding-interviews",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-coding-interviews",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "2.2 How to prepare for coding interviews",
    "text": "2.2 How to prepare for coding interviews\nHere is a simple guide on how to prepare -\n\nPurchase a leetcode subscription: This website is the only paid resource you need to prepare for coding interviews.\nGetting started with Leetcode learn: If you are like me who doesn’t come from a CS degree then going through leetcode learn cards is a good starting point. Here is the structure I followed-\n\nArray\nLinked List\nStack & Queue\nArray & Strings\nBinary Tree\nBinary Search\nBinary Search Tree\nHeap\nGraph\nSorting\nDynamic Programming\n\nFollowing Neetcode.io Roadmap: This roadmap contains 75 leetcode questions which will familiarize you with common coding patterns useful in coding interviews\nSolving company tagged questions: On leetcode.com you can filter for the company you are interviewing and see top tagged questions for the same. I would recommend solving the top 100 tagged questions based on frequency which are asked in last six months.\nPay attention to design questions: While I recommend focusing on the top 100 questions tagged by the company of interest, interviews often place special emphasis on design-related questions. In these cases, you may be asked to design a class to solve a specific use case. I will go beyond the top 100 questions to find every relevant design question asked in the past year. Here are some top design questions tagged for Meta on Leetcode at the time of writing -\n\nRandom Pick with Weight - LeetCode\nDot Product of Two Sparse Vectors - LeetCode\nLRU Cache\nMoving Average from Data Stream\nInsert Delete GetRandom O(1) - Leetcode\nProduct of two run length encoded arrays\n\n\n\nPractice: Once you are done with the above then you can practice timed assessment on leetcode.com for your specific employer or generic ones if not listed. If you want more realistic practice then you can buy some mock interviews on interviewing.io where an engineer from top tech company will take your mock and provide feedback on your performance.\n\n\n\n\n\n\n\nTipUseful tips for coding interviews preparation\n\n\n\n\n\n\nThe more you practice in conditions like the actual interview—such as using a text editor and working within timed constraints—the better you will perform\nWhen practicing, try to solve the problem on your own for 20-30 minutes before consulting the solution.\nIf you find a problem challenging to understand, search for [LeetCode Problem #XYZ] on YouTube; you’ll likely find a video with a clearer explanation.\nKeeping an Excel sheet to track the problems you’ve solved during practice, along with notes such as ’needs revision, time/space complexity and a summary of the solution, can be very helpful for reviewing later. Refer to Coding Tracking Sheet tab for template in MLE Interview Prep Template."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-machine-learning-system-design-problems-in-interview",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-solving-machine-learning-system-design-problems-in-interview",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "3.1 A structured approach to solving machine learning system design problems in interview",
    "text": "3.1 A structured approach to solving machine learning system design problems in interview\nIn an MLSD interview, it’s crucial for the interviewee to take the lead in the discussion and ensure all aspects of the ML design are covered. The interviews are fast-paced and following a structured method can be highly beneficial:\n\nClarifying Requirements: In every MLSD interview, you’re typically presented with an abstract problem. For example, you might be asked to “Design a ‘People You May Follow’ recommendation system for Threads or Twitter.” It’s essential to clarify the scope of the problem, ensuring it can be managed within the 45-minute timeframe. Asking the right clarifying questions not only helps you gain clarity but also shows your product awareness. For instance, in the case of a “People You May Follow” recommendation system, you might ask:\n\n“Can I assume the purpose of this feature is to help users find influencers or people aligned with their interests?”\n“On Threads/Twitter, following is unidirectional—one user can follow another without reciprocation. Is that correct?”\n“What is the estimated total number of users on the platform?\n“What is the current count of daily active users (DAUs)?”\n“What’s the average number of people each user follows?”\n“Since this is a mature platform, can we assume that the user-follow graph is relatively stable and doesn’t change drastically over short periods?”\n\nOnce you are done with the clarifying questions, it’s important to summarize them back to the interviewer, for example: “Okay, we are designing a ‘People You May Follow’ recommendation system for the Threads/Twitter platform. We have around XYZ million daily active users, and we’re assuming that the followership graph remains relatively stable over time.” This helps ensure alignment before diving into the design process.\nFrame the problem as an ML task: Once you’ve clarified the requirements, the next step is to map the problem to a known ML objective. This could be something like binary classification, learning to rank, edge prediction, or even visual object detection. Often, a problem can be framed using multiple objectives, and it’s important to explain what each objective aims to achieve and the pros and cons of each approach. For example, in the case of a recommendation system, you could frame it as a learning-to-rank problem or as edge prediction in a user graph. The key is to pick the objective that best aligns with the problem at hand.\nData sources for training labels: Next, brainstorm potential data sources for defining training labels. There could be various options, each with its own trade-offs. For instance, if you’re working on a video recommendation system, you might need to decide between implicit and explicit feedback. Explicit feedback (such as likes, shares, or subscriptions) tends to be high quality but sparse, as not all users provide these signals. On the other hand, implicit feedback (like user’s dwell time or watch time on a post, time spent in the app etc.) is lower quality but available for every user interaction. It’s essential to highlight these trade-offs and choose the best data source for your model’s needs.\nData preparation and Feature Engineering: Once you have identified some potential data sources, it’s good to talk about what features you will create from this data to feed into your ML model. The typical answer revolves around these three “focus area”-\n\nListing entities which are important for the problem: Entities are concepts around which you want to create model features. Some of the typical entities in Meta are users, authors/creators, pages, posts, images, videos, text, events, connections, advertisements, marketplace listings, groups and most importantly user engagement (with these entities).\nFeatures we can derive around this entity from the available data sources: For example, for a “User entity” we can utilize many features readily available in user profile table such as their ID, name, age, gender, city, country, language, time zone etc.\nHow to process this data to make it a highly robust and informational feature: For example, user id is a high cardinality feature, and we can convert it into a embedding which our model can learn while user’s gender is a low cardinality feature and can be processed using one-hot encoding.\n\nModel selection: After finishing the data pre-processing step we can talk about choosing the best ML algorithm and architecture for a predictive modeling problem. A typical approach is to go from simple to more complex solutions while explaining its pros and cons. For example, consider a video recommendation system, we can start by establishing a simple baseline by recommending everyone the most popular content, but we will run into situations like no personalization and a bias towards over-represented groups. You can progressively make it complex by talking about collaborative filtering, content-based filtering or a two-tower deep learning model. It’s important to talk about each of these modelling approaches by briefly explaining the algorithm and discussing the tradeoffs. For example, logistic learning might be a good option for you learning a linear task but if the task is complex, we may need to choose more complex models such as ensemble learners or deep learning models.\nModel training: Once you have selected the model and defined features, it’s important to talk about some of the details you will consider while training this model. Here are the things to talk about in this section if pertinent to your problem –\n\nSplitting data b/w training, testing and evaluation\nIdentifying class imbalanced datasets and techniques to rectify\nChoosing the right loss function for the model\nHandling Overfitting and Underfitting\n\nEvaluation: Once you have trained the model, it is important to talk about how you will evaluate the model’s performance. This part can be split into two categories:\n\nOffline Evaluation: This is the process of evaluating the model during development phase. An important part here is choosing the right offline metrics to measure how close the predictions are to the ground truth. It’s important to talk about multiple metrics and their pros and cons for the problem in hand. For example, in case of video recommendations there are different metrics to consider like Precision@K, Recall@K, Mean Average Precision(mAP), Normalized Discounted Cumulative Gain (nDCG), Mean Reciprocal Rank (MRR) etc.\nOnline Evaluations: This process involves evaluating the model’s performance in production after deployment. Online metrics, closely aligned with business objectives, typically include multiple measures. As with offline metrics, it is essential to select the appropriate evaluation criteria and provide clear justification for each choice. For instance, in the context of video recommendations, relevant metrics might include the model’s click-through rate (CTR), total watch time, the ratio of completed videos to total views, and the skip rate.\n\nDeployment: Next, if time permits talk about deployment and testing in production. Few things to consider in this section:\n\nDoes the model need to be deployed online or would batch prediction suffice?\nHow are we going to roll out this change? Running an experiment, or with a phased rollout?\nHow frequently does the model need to be re-trained?\n\n\n\n\n\n\n\n\nTipTips for MLSD interview:\n\n\n\n\n\n\nTime Management is Key: Aim to allocate time thoughtfully across all sections of the interview. Keep a mental clock to ensure you provide a well-rounded answer that covers all aspects of ML system design. If you find yourself spending too much time on one section, communicate with the interviewer by saying, “We can revisit this section later if time allows, but I’d like to move on to cover other areas as well.” This demonstrates both maturity and effective time management—qualities crucial at higher levels.\nPrioritize Practicality Over Perfection: MLSD interviews don’t expect you to design a state-of-the-art (SOTA) solution within 45 minutes. Instead, focus on creating a feasible, well-reasoned solution. Emphasize why you’re choosing specific components and how they contribute to the overall system rather than chasing an ideal but overly complex design.\nAdapt to Your Interviewer’s Cues: Pay close attention to suggestions from your interviewer. If they ask to deep-dive into a particular section, focus on providing detailed insights into that area. Conversely, if they signal it is time to move on, gracefully wrap up and proceed to the next section. Flexibility and responsiveness highlight your ability to collaborate effectively.\nRegularly engage with your interviewer: Approach the interviewer as a collaborator in solving the problem. After completing each section, ask for feedback and check whether they would like you to elaborate further or proceed to the next part. This fosters dynamic conversation and ensures you align with their expectations."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-ml-system-design-interview",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-ml-system-design-interview",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "3.2 How to prepare for ML system design interview",
    "text": "3.2 How to prepare for ML system design interview\nPreparing for Machine Learning System Design (MLSD) interviews requires a structured approach that combines technical knowledge, system design principles, and practical problem-solving skills. Here’s a step-by-step guide to help you get ready:\n\nPurchase ByteByteGo MLSD Interview book: This book is the only paid resource you need for preparing for MLSD interview. You can buy it from Amazon or you can buy an annual pass on bytebytego website. The first chapter introduces you with ML fundamentals and rest other chapters are focused on ten real ML system design interview questions with detailed solutions. My recommendation is to go thoroughly with the book and read every supplemental links in the reference after every chapter. Once you are done with book, you can refer to these MLSD notes I have created, refer to ML System Design tab in MLE Interview Prep Template.\nML interview prep guide: Meta recruiters provide an interview prep guide which talks about the whole interview experience and MLSD round as well. Its important to read through it and watch the Meta field guide to machine learning series to get more perspective on companies approach to ML.\nMeta engineering blog: Reading engineering blog can offer you insights on how MLE’s at Meta have designed some of the key components of most notable features in Meta applications and some of the recent frameworks used internally. Here are some of the most notable blogs are –\n\nPowered by AI: Instagram’s Explore recommender system\nScaling the Instagram Explore recommendations system - Engineering at Meta\nNews Feed ranking, powered by machine learning - Engineering at Meta\nMeta’s approach to machine learning prediction robustness - Engineering at Meta\n\nSweat more in training, bleed less in battle: This analogy perfectly captures the essence of preparation for MLSD interviews. The more effort you put into practicing beforehand, the less you’ll need to improvise during the interview. Here are some practical ways to prepare effectively:\n\nDesign Random Features: Open platforms like Instagram, Facebook, or Threads, pick a random feature, and outline how you would design it from an ML Engineer’s perspective. Consider how the design might differ depending on the app’s unique goals and user base. Here are some examples of features to practice:\n\nFriend suggestions (e.g., “People You May Know”).\nNews Feed ranking.\nGroup recommendations.\nShort-form video recommendations.\nExplore tab content discovery.\nIntegrity filtering for harmful content.\n\nPrepare Features in Advance: As discussed earlier, feature engineering is a crucial part of MLSD interviews. Create a comprehensive list of:\n\nData sources: Identify potential sources of data (e.g., user activity, content metadata, social graphs).\nEntities: Determine key entities like users, items, or sessions.\nFeatures: Brainstorm features around these entities, such as engagement patterns, temporal trends, or semantic embeddings.\nProcessing Steps: Document how you would pre-process and transform these features for your ML models.\n\n\nPreparing these ahead of time will save valuable mental bandwidth during the interview and elevate the quality of your responses. Pre-built templates and examples can help you focus on tailoring solutions to specific problems rather than scrambling to generate ideas under pressure.\nSimulate real interview scenarios : Practice solving MLSD problems under realistic conditions by setting a strict time limit (35–40 minutes) and using tools like Excalidraw to create diagrams and workflows. Work through a complete problem, starting from clarifying requirements to designing the entire system. Prioritize clear structure and ensure you address all critical aspects, such as data flow, model selection, architecture, and trade-offs. Recording your sessions can help you pinpoint areas for improvement, such as pacing, clarity, or the depth of your explanations, enabling you to refine your approach and being comfortable with the tools for actual interviews."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-answering-behavioral-questions",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#a-structured-approach-to-answering-behavioral-questions",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "4.1 A Structured Approach to Answering Behavioral Questions",
    "text": "4.1 A Structured Approach to Answering Behavioral Questions\nOne of the best ways to frame your answers is by using the STAR method, which helps you organize your responses clearly and concisely: - Situation: Set the context for your story. Describe the challenge or situation you were in, including the key players and the environment. - Task: Outline your specific responsibility or what you were tasked to do in that situation. - Action: Explain the steps you took to address the task or solve the problem. Focus on the actions you personally took, rather than what the team did as a whole. - Result: Share the outcome of your actions. Quantify the results if possible (e.g., increased efficiency by 20%, reduced costs by 15%) and mention what you learned from the experience.\nUsing this structure helps you keep your answers clear, logical, and impactful. For example, when asked about teamwork, describe a scenario where you worked with a cross-functional team, the challenge you faced, the actions you took to collaborate, and the successful outcome."
  },
  {
    "objectID": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-behavioral-interviews",
    "href": "posts/2024-12-15-MLInterviewPrep/MLInterviewPrep.html#how-to-prepare-for-behavioral-interviews",
    "title": "Meta MLE Interview Preparation Guide",
    "section": "4.2 How to Prepare for Behavioral Interviews",
    "text": "4.2 How to Prepare for Behavioral Interviews\nHere’s a step-by-step guide to help you get prepared for Behavioral interviews -\n\nUnderstand Meta’s Values and Culture: Familiarize yourself with the company’s mission, principle and core values to ensure that your answers reflect how you embody these traits. Think about how your experiences align with Meta’s core values and communicate that through your stories.\nInterview Jedi Youtube Video series(Part1, Part2): This series contains top questions asked in behavioral rounds at Meta and covers evaluation criteria, response framework, tips and tricks to prepare + answer them effectively. Having answers pre-prepared to these questions will help you setup for a successful behavioral interview. You can use the Behavioral Tab in MLE Interview Prep Template to write your answers to these questions.\nPractice and Refine Your Stories: Once you’ve gone through the videos and written your stories, practice delivering them succinctly using the STAR method. Try to frame each story in a way that highlights your skills and accomplishments while demonstrating your alignment with Meta’s values. Mock interviews with peers or mentors can help you fine-tune your answers and gain confidence in presenting your experiences.\nStriking the right balance b/w abstraction and technicality: It’s essential to strike the right balance between abstraction and technicality in your responses. You want to prove your problem-solving and leadership abilities without getting bogged down in excessive technical details. While it’s important to avoid unnecessary technical jargon, you should still provide sufficient detail to show the depth of your involvement. For example, instead of providing internal abbreviations like ECR say managed Docker container registry. Watch this section from A Life Engineered Video to understand the difference b/w answer to the same behavioral question at different levels.\nBe Honest and Authentic: Meta values authenticity and integrity, so avoid overselling yourself. Be honest about your experiences, especially when discussing challenges or failures. What matters most is your ability to reflect on your experiences, learn from them, and demonstrate how you’ve grown.\n\nBy following these strategies, you can approach Meta’s behavioral interviews with a structured, thoughtful approach that showcases your qualifications and demonstrates your alignment with the company’s values."
  },
  {
    "objectID": "posts/2025-12-25-DiversityMMRPart1/DiversityMMRPart1.html",
    "href": "posts/2025-12-25-DiversityMMRPart1/DiversityMMRPart1.html",
    "title": "Diversity in Recommendations - Maximal Marginal Relevance (MMR)",
    "section": "",
    "text": "Beyond Accuracy: Mastering Maximal Marginal Relevance to build recommendation engines that surprise and engage.\n\n\n\n\nFigure 1: Diverse recommendations cover multiple user intents. Credit: Gemini\n\n\n\n\n\n\n\n\nWarningDisclaimer\n\n\n\n\n\nThe views expressed in this blog are solely my own and are not affiliated with my employer in any way. In writing this, I have not utilized any proprietary or confidential information.\n\n\n\nPicture this: you open YouTube looking for something to watch. The algorithm shows you 10 videos, and they’re all basketball highlights from the same game. Sure, you like basketball, but maybe right now you’re in the mood for that ML lecture you’ve been putting off. Or maybe something else entirely.\nMost recommendation systems score each item independently based on predicted relevance. The result? If your model is confident you like basketball, you get basketball. All basketball. The system optimizes for the single most likely intent while ignoring the possibility that you might have multiple interests, or that its prediction might simply be wrong.\nDiversity addresses this by explicitly penalizing redundancy. Instead of asking “what’s most relevant?”, we ask “what’s most relevant that we haven’t already covered?”\nThis isn’t just theoretically appealing. Diversity boosting delivers real results in production systems and can dramatically improve engagement metrics. Nobel laureate Harry Markowitz called diversification “the only free lunch in finance” when applied to portfolio management. The same principle applies to recommendations: hedge your bets across the user’s possible intents.\nIn this post, we’ll:\n\nExplain why diversity matters and when pointwise scoring fails\nIntroduce Maximal Marginal Relevance (MMR), a simple and battle-tested diversity method\nWalk through the algorithm step-by-step with code\nExplore how the λ parameter lets you tune the quality-diversity tradeoff\n\n\nWhy Diversity Matters\nRecommendation systems face a fundamental challenge: they don’t know what the user wants right now. They have historical signals, but a user who watched NBA highlights yesterday might be looking for a cooking video today. A user who binges Peppa Pig with their kids on weekends has different needs on a Tuesday night.\nPointwise ranking models score each candidate independently. If your embedding space clusters similar content together (as it should), similar items receive similar scores. The top-K results end up being variations of the same thing.\nThis creates several problems:\n\nMissed opportunities: If the user’s current intent doesn’t match your top prediction, you’ve wasted valuable screen real estate\nFilter bubbles: Users get stuck in narrow content loops, reducing long-term engagement\nPoor fault tolerance: When your model is wrong about intent, it’s wrong across the entire slate\n\nDiversity isn’t unique to recommendations. Portfolio managers diversify across asset classes to reduce risk. Autonomous driving systems like IntentNet diversify over possible agent intents rather than betting on a single predicted trajectory. The core insight is the same: when you’re uncertain about what the environment wants, spread your bets.\nThe business case is compelling. Users who see a mix of content that spans their interests are more likely to find something they want right now, and more likely to come back.\n\n\nThe Problem Setup\nLet’s make this concrete with an example. Imagine you have 6 candidate videos for a user:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom fastcore.utils import L\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nrelevance = np.array([0.9, 0.85, 0.8, 0.7, 0.6, 0.5])\ngenre = L(['Action', 'Action', 'Action', 'Comedy', 'Documentary', 'Mixed comedy/documentary'])\n\ndf = pd.DataFrame({\n    'video_id': [i for i in range(len(relevance))],\n    'genre': genre,\n    'relevance': relevance\n})\ndf\n\n\n\n\n\n\n\n\n\nvideo_id\ngenre\nrelevance\n\n\n\n\n0\n0\nAction\n0.90\n\n\n1\n1\nAction\n0.85\n\n\n2\n2\nAction\n0.80\n\n\n3\n3\nComedy\n0.70\n\n\n4\n4\nDocumentary\n0.60\n\n\n5\n5\nMixed comedy/documentary\n0.50\n\n\n\n\n\n\n\nPure relevance ranking returns: Action, Action, Action, Comedy, Documentary, Mixed\nThe top three slots go to nearly identical content. If the user wanted comedy, they have to scroll past three action videos. If they wanted a documentary, it’s buried at position 5.\nNow consider what happens with diversity-aware ranking: Action, Comedy, Documentary, Action, Mixed\nThe slate now covers three distinct genres in the top three positions. The second action video drops to position 4 since its marginal value is low once we’ve already shown an action video.\nThis is the core insight: marginal relevance matters more than absolute relevance. The 3rd action video isn’t 3x as valuable as the first. Each additional similar item has diminishing returns.\nWe can represent each video as an embedding vector over genres:\n\nembeddings = np.array([\n    [1.0, 0.0, 0.0],   # video 0: pure action\n    [0.9, 0.1, 0.0],   # video 1: action (very similar to 0)\n    [0.8, 0.2, 0.0],   # video 2: action (also very similar to 0)\n    [0.0, 1.0, 0.0],   # video 3: comedy\n    [0.0, 0.0, 1.0],   # video 4: documentary\n    [0.1, 0.5, 0.5],   # video 5: mixed\n])\n\nsimilarities = cosine_similarity(embeddings)\n\nprint(f\"Similarity b/w Video 0 and 1 is {similarities[0,1]:.2f}\")\nprint(f\"Similarity b/w Video 0 and 3 is {similarities[0,3]:.2f}\")\n\nSimilarity b/w Video 0 and 1 is 0.99\nSimilarity b/w Video 0 and 3 is 0.00\n\n\nVideos 0 and 1 have cosine similarity near 1.0. Videos 0 and 3 are orthogonal (similarity = 0). This similarity structure is exactly what MMR exploits to rerank results.\n\n\nMaximal Marginal Relevance: The Algorithm\nMMR was introduced by Carbonell and Goldstein in 1998 for document retrieval and summarization. The core formula is:\nMMR = ArgMax[λ × Rel(item, user) - (1-λ) × Max(Sim(item, selected_items))]\nWhere:\n\nitem = candidate video not yet selected\nuser = user profile \nselected_items = set of items already chosen for the slate \nλ = trade-off parameter between relevance and diversity (0 to 1)\n\nYou can see the first part λ × Rel(item, user) is just relevance. The second part (1-λ) × Max(Sim(item, selected_items)) is the diversity penalty.\nThe algorithm is greedy and iterative:\n\nFirst pick: Select the item with highest relevance (the similarity penalty term is zero since nothing has been selected yet)\nSubsequent picks: For each remaining candidate, compute relevance minus a penalty for similarity to the most similar already-selected item. Pick the candidate with the highest score.\nRepeat until you’ve filled K slots\n\nThe key insight is the Max in the second term. You’re penalized based on your similarity to whichever selected item you’re most similar to. Being different from 2 out of 3 selected items doesn’t help if you’re a near-duplicate of the third.\nThe λ parameter controls the tradeoff:\n\nλ = 1.0 → Pure relevance ranking, no diversity\nλ = 0.0 → Maximum diversity, ignore relevance\nλ = 0.5 → Equal weight to both\nλ = 0.7 → The paper’s suggested starting point\n\n\n\nWalking Through MMR Step-by-Step\nLet’s trace through the algorithm by hand before writing any code.\nStep 1: First selection\nWith an empty slate, there’s no diversity penalty.\n\nselected_item = np.argmax(relevance)\nprint(f\"First selected item: {int(selected_item)}, Relevance: {relevance[selected_item]}\")\n\nFirst selected item: 0, Relevance: 0.9\n\n\nWe pick the highest relevance item: Video 0 (Action, relevance = 0.9).\nStep 2: Second selection\nNow we need to choose between the remaining videos. Let’s compare Video 1 (another Action, relevance=0.85) vs Video 3 (Comedy, relevance=0.7)\n\nlam = 0.5\nvideo_1_mmr_score = lam * relevance[1] - (1-lam) * cosine_similarity(embeddings[[1]], embeddings[[selected_item]])\nvideo_3_mmr_score = lam * relevance[3] - (1-lam) * cosine_similarity(embeddings[[3]], embeddings[[selected_item]])\nprint(f\"MMR score of Video 1: {video_1_mmr_score.item():.3f}\")\nprint(f\"MMR score of Video 3: {video_3_mmr_score.item():.3f}\")\n\nMMR score of Video 1: -0.072\nMMR score of Video 3: 0.350\n\n\nVideo 3 wins despite having lower relevance. The similarity penalty makes Video 1’s score negative. This is MMR in action: redundancy is penalized so heavily that a lower-quality diverse item beats a high-quality duplicate.\nThe full implementation\n\ndef mmr_select(relevance, embeddings, k=3, lam=0.5):\n    n = len(relevance)\n    selected = []\n    remaining = list(range(n))\n\n    # First pick: pure relevance (no penalty term)\n    first = int(np.argmax(relevance))\n    selected.append(first)\n    remaining.remove(first)\n\n    # Subsequent picks: balance relevance vs redundancy\n    while len(selected) &lt; k:\n        best_score, best_idx = -float('inf'), None\n\n        for i in remaining:\n            # Penalty = similarity to most similar already-selected item\n            max_sim = max(similarities[i, j] for j in selected)\n            mmr_score = lam * relevance[i] - (1 - lam) * max_sim\n\n            if mmr_score &gt; best_score:\n                best_score, best_idx = mmr_score, i\n\n        selected.append(best_idx)\n        remaining.remove(best_idx)\n\n    return selected\n\n\nresult = mmr_select(relevance, embeddings, k=3, lam=0.5)\nprint(genre[result])\n\n['Action', 'Comedy', 'Documentary']\n\n\nPure relevance ranking would give us Action, Action, Action. MMR gives us Action, Comedy, Documentary. Three genres covered in three slots.\n\n\nTuning λ: The Quality-Diversity Tradeoff\nThe λ parameter is your dial between relevance and diversity. Let’s see how it affects the ranking:\n\nfor lam in [0.3, 0.5, 0.7, 0.9, 1.0]:\n    result = mmr_select(relevance, embeddings, k=6, lam=lam)\n    print(f\"λ={lam}: {genre[result]}\")\n\nλ=0.3: ['Action', 'Comedy', 'Documentary', 'Mixed comedy/documentary', 'Action', 'Action']\nλ=0.5: ['Action', 'Comedy', 'Documentary', 'Action', 'Action', 'Mixed comedy/documentary']\nλ=0.7: ['Action', 'Comedy', 'Documentary', 'Action', 'Action', 'Mixed comedy/documentary']\nλ=0.9: ['Action', 'Action', 'Action', 'Comedy', 'Documentary', 'Mixed comedy/documentary']\nλ=1.0: ['Action', 'Action', 'Action', 'Comedy', 'Documentary', 'Mixed comedy/documentary']\n\n\nAt λ=1.0 we recover pure relevance ranking. As λ decreases, diverse items get promoted earlier in the slate.\nChoosing λ in practice:\n\nStart with λ=0.7 (the original paper’s suggestion)\nA/B test different values against engagement metrics\n\n\n\nSystem Design of Diversity Boosting\nIn terms of system design, diversity boosting can be added at various stages by using different techniques, but MMR falls into the class of re-rank algorithms which are typically applied after pointwise scoring.\n\n\n\nFigure 2: Typical RecSys pipeline showing where MMR fits in the recommendation pipeline\n\n\nThe diagram above shows a typical recsys pipeline :\n\nCandidate Videos → Raw pool of items to consider\nPointwise video scorer → Takes user features and video features, outputs a relevance score per video independently\nRanking and Diversity Boosting (highlighted in green) → This is where MMR lives. It takes the pointwise scores and reorders them to balance relevance with diversity\nPolicy → Business rules layer (e.g., content safety filters, frequency caps)\nUser → Final slate shown\n\nMMR is a post-scoring reranker. The pointwise model does its job first (predicting relevance), then MMR adjusts the ordering to reduce redundancy. This is a clean separation of concerns - you don’t need to retrain your ranking model to add diversity.\n\n\nKey Takeaways\n\nPointwise ranking has a blind spot. Scoring items independently leads to redundant results when similar items cluster in your embedding space.\nDiversity hedges against uncertainty. When you don’t know the user’s current intent, covering multiple possibilities increases the chance of a hit.\nMMR is simple and effective. The algorithm is greedy, easy to implement, and has powered production systems for decades.\n\nI hope you enjoyed reading it. If there is any feedback on the code or just the blog post, feel free to reach out on LinkedIn."
  },
  {
    "objectID": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "href": "talks/2017-10-11-deep-dive-in-hierarchichal-clustering/index.html",
    "title": "Deep dive in Hierarchical clustering",
    "section": "",
    "text": "Presentation and codes on “Deep dive in Hierarchical Clustering”. This was material presented by me in Analyze This! meetup on 11th October, 2017\nLink to the meetup group\nLink to Git repo\nPhoto of me presenting -"
  },
  {
    "objectID": "talks/2018-10-18-msba_devops/index.html",
    "href": "talks/2018-10-18-msba_devops/index.html",
    "title": "DevOps for Data scientist",
    "section": "",
    "text": "Conducted a DevOps for data scientist workshop for 2019 batch of Masters in Business Analytics student in Carlson school of management. The workshop was attended by 100+ students.\nLink to workshop content\nWorkshop Content\n\nWhat is DevOps? Why is it needed?\nModel building using cookbooks in Python\nVersion control using GIT\nDeploying Model as an API using Flask and microservices framework\nPackaging applications using Docker\nScaling and Deploying applications in Google cloud using Docker and Kubernetes"
  },
  {
    "objectID": "talks/2020-12-15-OrcaHello/index.html",
    "href": "talks/2020-12-15-OrcaHello/index.html",
    "title": "ML in the wild",
    "section": "",
    "text": "Sharing our insights from deploying OrcaHello - an open source, AI-assisted 24x7 hydrophone monitoring & Southern Resident Killer Whale alert system in Puget Sound.\n\nYoutube Link - (1:10:00)\nAI For Orcas Webiste"
  }
]