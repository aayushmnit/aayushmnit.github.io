<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Aayush Agrawal</title>
<link>https://aayushmnit.github.io/blog.html</link>
<atom:link href="https://aayushmnit.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Aayush's personal website</description>
<generator>quarto-1.2.124</generator>
<lastBuildDate>Wed, 21 Sep 2022 07:00:00 GMT</lastBuildDate>
<item>
  <title>Mixing art in science of model explainability</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2022-09-21-Explainability/2022-09-21-Explainability.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Overview on <a href="https://interpret.ml/docs/ebm.html">Explainable Boosting Machine</a> and an approach for converting explanation to more human-friendly explanation.</p>
</blockquote>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/lego_scientist.jpg" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig.1 - A lego figure on my desk
</figcaption>
</figure>
<section id="science-of-ml-explainability" class="level1">
<h1>1. Science of ML explainability</h1>
<section id="the-interpretability-vs-accuracy-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="the-interpretability-vs-accuracy-trade-off">1.1 The Interpretability vs Accuracy Trade-off</h2>
<p>In traditional tabular machine learning approaches, Data scientists often deal with the trade-off b/w interpretability and accuracy.</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/interprebility.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig.2: Interpretibility and Accuracy Tradeoff <br> Image Credit - <a href="https://www.youtube.com/watch?v=MREiHgHgl0k">The Science Behind InterpretML: Explainable Boosting Machine</a>
</figcaption>
</figure>
<p>As shown in the chart above, we can see that <strong>Glass-Box models</strong> like Linear Regression, Naive Bayes, and Decision Trees are simple models to interpret, and predictions from these models are not highly accurate. On the other hand, <strong>Black-Box models</strong> like Boosted Trees, Random Forest, and Neural Nets are hard to interpret but lead to highly accurate predictions.</p>
</section>
<section id="introducing-ebms" class="level2">
<h2 class="anchored" data-anchor-id="introducing-ebms">1.2 Introducing EBM’s</h2>
<p>To solve the problem just mentioned above EBMs(Explainable Boosted Machine) model was developed by Microsoft Research<sup>1</sup>. “Explainable Boosting Machine (EBM) is a tree-based, cyclic gradient boosting Generalized Additive Model with automatic interaction detection. EBMs are often as accurate as state-of-the-art BlackBox models while remaining completely interpretable. Although EBMs are often slower to train than other modern algorithms, EBMs are extremely compact and fast at prediction time.”<sup>2</sup></p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/interprebility2.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig.3: EBMs breaking the Interpretability vs Accuracy paradox<br> Image Credit - <a href="https://www.youtube.com/watch?v=MREiHgHgl0k">The Science Behind InterpretML: Explainable Boosting Machine</a>
</figcaption>
</figure>
<p>As we can see from the chart above, EBMs help us break out of this trade-off paradox and help us build models which are both highly interpretable and accurate. To further understand the math behind EBMs I highly encourage watching this 12-minute YouTube video -</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MREiHgHgl0k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<figure class="figure">
<figcaption class="figure-caption">
Video - <a href="https://www.youtube.com/watch?v=MREiHgHgl0k">The Science Behind InterpretML: Explainable Boosting Machine</a>
</figcaption>
</figure>
</center>
</section>
<section id="glass-box-vs-black-box-models.-what-to-choose" class="level2">
<h2 class="anchored" data-anchor-id="glass-box-vs-black-box-models.-what-to-choose">1.3 Glass box vs Black box models. What to choose?</h2>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The answer to every complex question in life is “It depends”.</p>
</div>
</div>
<p>There are trade-offs b/w using Glassbox models as compared to Blackbox models. There is no clear winner in picking one model over the other but depending on the situation DS can make an educated guess on what model to pick.</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/glassboxVsblackbox.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig.4: Glassbox models vs BlackBox models
</figcaption>
</figure>
<p>Two considerations to think about while picking glass box vs black box models are the following-</p>
<p><strong>1) Explainability Requirements</strong> - In the domain where there is no need for explanation or it is needed for a data scientist or technical audience for intuition/inspection purposes, in these cases, DS are well off using black box models. In the domain where an explanation is needed because of business or regulatory requirements or where these explanations are served to a non-technical audience (humans), glass-box models have an upper hand. This is because explanations coming out of the glass box models are <code>exact</code> and <code>global</code>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>Exact</code> and <code>global</code> just means that a value of a particular feature will always have the same effect on each prediction explanation. For example, in the case of the prediction of income of a particular individual being above $50k with age as one of the predictors, if the <code>age</code> is 40 and it will impact the target variable with the same proportion let us say <span style="color:green">5%</span> in each observation in the data where the age is 40. This is not the case when we build explanations through LIME and Shapely for black box models. In black-box models, <code>age</code> with the value 40 for example can have a <span style="color:green">10%</span> lift in an individual probability of their income being above 50k for one observation and <span style="color:red">-10%</span> lift in the other.</p>
</div>
</div>
<p><strong>2) Compute Requirement</strong> - DS needs to pay attention to various compute requirements for testing and training a model depending on its use case. EBMs are particularly slow in the training phase but provide fast predictions with built-in explanations. So, in cases where you need to train your model every hour, EBMs might not suffice your need. But, in cases where the training of the model happens monthly/weekly, and scores are generated on a more frequent basis (hourly/daily) EBMs might fit the use case well. Also, in cases where you might be required to produce an explanation for each prediction EBMs can save a lot of computing and might be the only feasible technique to use for millions of observations. Look below to understand the operational difference b/w EBMs and other tree-based ensemble methods.</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/trade-off.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig. 5: EBMs vs XgBoost/LightGBM
</figcaption>
</figure>
</section>
</section>
<section id="hands-on-with-ebms" class="level1">
<h1>2. Hands on with EBMs</h1>
<section id="data-overview" class="level2">
<h2 class="anchored" data-anchor-id="data-overview">2.1 Data Overview</h2>
<p>For this example, we will use <a href="https://archive.ics.uci.edu/ml/datasets/Adult">Adult Income Dataset</a> from the <strong>UCI machine learning Repository</strong><sup>3</sup>. The problem in this dataset is set up as a binary classification problem to predict if a certain individual income based on various census information (education level, age, gender, occupation, etc.) exceeds $50K/year. For sake of simplicity, we are only going to use observations of individuals in the United States and the following predictors -</p>
<ul>
<li><code>Age</code>: <code>continuous</code> variable, individuals’ age</li>
<li><code>Occupation</code>: <code>categorical</code> variable, Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.</li>
<li><code>HoursPerWeek</code>: <code>continuous</code> variable, amount of hours spent in a job per week</li>
<li><code>Education</code>: <code>categorical</code> variable, Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:11.476333Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:10.188600Z&quot;}" data-cell_style="center" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;">## Importing required libraries</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> sklearn <span class="im" style="color: #00769E;">import</span> metrics</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> interpret.glassbox <span class="im" style="color: #00769E;">import</span> ExplainableBoostingClassifier</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">from</span> interpret <span class="im" style="color: #00769E;">import</span> show</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> warnings</span>
<span id="cb1-9"><span class="im" style="color: #00769E;">import</span> plotly.io <span class="im" style="color: #00769E;">as</span> pio</span>
<span id="cb1-10"><span class="im" style="color: #00769E;">import</span> plotly.express <span class="im" style="color: #00769E;">as</span> px</span>
<span id="cb1-11">warnings.filterwarnings(<span class="st" style="color: #20794D;">'ignore'</span>)</span>
<span id="cb1-12">pio.renderers.default <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"plotly_mimetype+notebook_connected"</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:11.985854Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:11.477920Z&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;">## Loading the data</span></span>
<span id="cb2-2">df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv( <span class="st" style="color: #20794D;">"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"</span>, header<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>)</span>
<span id="cb2-3">df.columns <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb2-4">    <span class="st" style="color: #20794D;">"Age"</span>, <span class="st" style="color: #20794D;">"WorkClass"</span>, <span class="st" style="color: #20794D;">"fnlwgt"</span>, <span class="st" style="color: #20794D;">"Education"</span>, <span class="st" style="color: #20794D;">"EducationNum"</span>,</span>
<span id="cb2-5">    <span class="st" style="color: #20794D;">"MaritalStatus"</span>, <span class="st" style="color: #20794D;">"Occupation"</span>, <span class="st" style="color: #20794D;">"Relationship"</span>, <span class="st" style="color: #20794D;">"Race"</span>, <span class="st" style="color: #20794D;">"Gender"</span>,</span>
<span id="cb2-6">    <span class="st" style="color: #20794D;">"CapitalGain"</span>, <span class="st" style="color: #20794D;">"CapitalLoss"</span>, <span class="st" style="color: #20794D;">"HoursPerWeek"</span>, <span class="st" style="color: #20794D;">"NativeCountry"</span>, <span class="st" style="color: #20794D;">"Income"</span></span>
<span id="cb2-7">]</span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;">## Filtering for Unites states</span></span>
<span id="cb2-10">df <span class="op" style="color: #5E5E5E;">=</span> df.loc[df.NativeCountry <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">' United-States'</span>,:]</span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;">## Only - Taking required columns</span></span>
<span id="cb2-13">df <span class="op" style="color: #5E5E5E;">=</span> df.loc[:,[<span class="st" style="color: #20794D;">"Education"</span>, <span class="st" style="color: #20794D;">"Age"</span>,<span class="st" style="color: #20794D;">"Occupation"</span>, <span class="st" style="color: #20794D;">"HoursPerWeek"</span>, <span class="st" style="color: #20794D;">"Income"</span>]]</span>
<span id="cb2-14"></span>
<span id="cb2-15">df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Education</th>
      <th>Age</th>
      <th>Occupation</th>
      <th>HoursPerWeek</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bachelors</td>
      <td>39</td>
      <td>Adm-clerical</td>
      <td>40</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bachelors</td>
      <td>50</td>
      <td>Exec-managerial</td>
      <td>13</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>2</th>
      <td>HS-grad</td>
      <td>38</td>
      <td>Handlers-cleaners</td>
      <td>40</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11th</td>
      <td>53</td>
      <td>Handlers-cleaners</td>
      <td>40</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Masters</td>
      <td>37</td>
      <td>Exec-managerial</td>
      <td>40</td>
      <td>&lt;=50K</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Let’s look at target variable distribution.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:12.563586Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:11.987560Z&quot;}" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">plot_df <span class="op" style="color: #5E5E5E;">=</span> df.Income.value_counts().reset_index().rename(columns <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">"index"</span>:<span class="st" style="color: #20794D;">"Income"</span>, <span class="st" style="color: #20794D;">"Income"</span>:<span class="st" style="color: #20794D;">"Count"</span>})</span>
<span id="cb3-2">fig <span class="op" style="color: #5E5E5E;">=</span> px.bar(plot_df, x <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"Income"</span>, y <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'Count'</span>)</span>
<span id="cb3-3">fig.update_layout(</span>
<span id="cb3-4">        title  <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb3-5">            <span class="st" style="color: #20794D;">'text'</span>:<span class="st" style="color: #20794D;">"Target variable distribution"</span>,</span>
<span id="cb3-6">            <span class="st" style="color: #20794D;">'y'</span>:<span class="fl" style="color: #AD0000;">0.95</span>,</span>
<span id="cb3-7">            <span class="st" style="color: #20794D;">'x'</span>:<span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb3-8">        },</span>
<span id="cb3-9">        legend <span class="op" style="color: #5E5E5E;">=</span>  <span class="bu" style="color: null;">dict</span>(y<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, x<span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span>, orientation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'v'</span>),</span>
<span id="cb3-10">        legend_title <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span>,</span>
<span id="cb3-11">        xaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Income"</span>, </span>
<span id="cb3-12">        yaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Count of obersvations"</span>,</span>
<span id="cb3-13">        font <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb3-14">)</span>
<span id="cb3-15">fig.show(renderer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'notebook'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="c761c3b2-ccc6-49fb-a103-d4c4935a8d20" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c761c3b2-ccc6-49fb-a103-d4c4935a8d20")) {                    Plotly.newPlot(                        "c761c3b2-ccc6-49fb-a103-d4c4935a8d20",                        [{"alignmentgroup":"True","hovertemplate":"Income=%{x}<br>Count=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"","offsetgroup":"","orientation":"v","showlegend":false,"textposition":"auto","x":[" <=50K"," >50K"],"xaxis":"x","y":[21999,7171],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Income"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Count of obersvations"}},"legend":{"tracegroupgap":0,"y":1,"x":0.8,"orientation":"v","title":{"text":""}},"margin":{"t":60},"barmode":"relative","title":{"text":"Target variable distribution","y":0.95,"x":0.5},"font":{"size":15}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('c761c3b2-ccc6-49fb-a103-d4c4935a8d20');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Fig 1 - Cigarette sales comparison b/w California and other states</p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:12.571194Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:12.565313Z&quot;}" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="bu" style="color: null;">print</span>(df.Income.value_counts(normalize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> &lt;=50K    0.754165
 &gt;50K     0.245835
Name: Income, dtype: float64</code></pre>
</div>
</div>
<p>~24.6% of people in our dataset have income greater than $50K. The data looks good, we have the columns we need. We will use Education, Age, Occupation, and HoursPerWeek columns and predict Income. Before modeling, let us perform an 80-20 train-test split.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:12.589773Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:12.572717Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;">## Train-Test Split</span></span>
<span id="cb6-2">X <span class="op" style="color: #5E5E5E;">=</span> df[df.columns[<span class="dv" style="color: #AD0000;">0</span>:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb6-3">y <span class="op" style="color: #5E5E5E;">=</span> df[df.columns[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb6-4">seed <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb6-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.20</span>, random_state<span class="op" style="color: #5E5E5E;">=</span>seed)</span>
<span id="cb6-6"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Data in training </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(y_train)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, Data in testing </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(y_test)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data in training 23336, Data in testing 5834</code></pre>
</div>
</div>
</section>
<section id="fitting-an-ebm-model" class="level2">
<h2 class="anchored" data-anchor-id="fitting-an-ebm-model">2.2 Fitting an EBM Model</h2>
<p>EBMs have a scikit-compatible API, so fitting the model and making predictions are the same as any scikit learn model.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:14.754928Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:12.591632Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">ebm <span class="op" style="color: #5E5E5E;">=</span> ExplainableBoostingClassifier(random_state<span class="op" style="color: #5E5E5E;">=</span>seed, interactions<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb8-2">ebm.fit(X_train, y_train)</span>
<span id="cb8-3"></span>
<span id="cb8-4">auc <span class="op" style="color: #5E5E5E;">=</span> np.<span class="bu" style="color: null;">round</span>(metrics.roc_auc_score((y_test <span class="op" style="color: #5E5E5E;">!=</span> <span class="st" style="color: #20794D;">' &lt;=50K'</span>).astype(<span class="bu" style="color: null;">int</span>).values, ebm.predict_proba(X_test)[:,<span class="dv" style="color: #AD0000;">1</span>], ),<span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb8-5"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Accuracy: </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span><span class="bu" style="color: null;">round</span>(np.mean(ebm.predict(X_test) <span class="op" style="color: #5E5E5E;">==</span> y_test)<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span>,<span class="dv" style="color: #AD0000;">2</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">%, AUC: </span><span class="sc" style="color: #5E5E5E;">{</span>auc<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 80.12%, AUC: 0.828</code></pre>
</div>
</div>
<p>I hope the above code block shows how similar the interpret-ml API is to the scikit learn API. Based on AUC on the validation set we can say our model is better than random predictions.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practice, if you are dealing with millions of observations, Try doing feature selection using LightGBM/XGboost and only train your final models using EBMs. This will save you time in feature exploration.</p>
</div>
</div>
</section>
<section id="explaination-from-ebms" class="level2">
<h2 class="anchored" data-anchor-id="explaination-from-ebms">2.3 Explaination from EBMs</h2>
<p>Interpret package comes with both global and local explanations and has a variety of visualization tools to inspect what the model is learning.</p>
<section id="global-explanations" class="level3">
<h3 class="anchored" data-anchor-id="global-explanations">2.3.1 Global explanations</h3>
<p>Global explanations provide the following visualization -</p>
<ol type="1">
<li><strong>Summary -</strong> Feature importance plot, this chart provides the importance of each predictor in predicting the target variable</li>
<li><strong>Feature interaction with Prediction -</strong> This chart is the same look-up table EBM uses in making the actual prediction. This can help you in the inspection of how the feature value is contributing to prediction.</li>
</ol>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:15.259476Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:14.757055Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">ebm_global <span class="op" style="color: #5E5E5E;">=</span> ebm.explain_global()</span>
<span id="cb10-2">show(ebm_global)</span></code></pre></div>
<div class="cell-output cell-output-display">
<!-- http://127.0.0.1:7001/140463753787088/ -->
<iframe src="http://127.0.0.1:7001/140463753787088/" width="100%" height="800" frameborder="0"></iframe>
</div>
</div>
</section>
<section id="local-explanations" class="level3">
<h3 class="anchored" data-anchor-id="local-explanations">2.3.2 Local explanations</h3>
<p>The local explanation is our per-observation level explanation. EBMs have a great in-built visualization for displaying this information.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:15.326622Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:15.265597Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">ebm_local <span class="op" style="color: #5E5E5E;">=</span> ebm.explain_local(X_test.iloc[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">5</span>,:], y_test)</span>
<span id="cb11-2">show(ebm_local)</span></code></pre></div>
<div class="cell-output cell-output-display">
<!-- http://127.0.0.1:7001/140465372566528/ -->
<iframe src="http://127.0.0.1:7001/140465372566528/" width="100%" height="800" frameborder="0"></iframe>
</div>
</div>
<p>Let’s take one example of this explanation for observation at index 0 and look at it -</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:15.340188Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:15.329086Z&quot;}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">explainDF <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame.from_dict(</span>
<span id="cb12-2">    {</span>
<span id="cb12-3">        <span class="st" style="color: #20794D;">'names'</span>: ebm_local.data(<span class="dv" style="color: #AD0000;">0</span>)[<span class="st" style="color: #20794D;">'names'</span>], </span>
<span id="cb12-4">        <span class="st" style="color: #20794D;">'data'</span>:ebm_local.data(<span class="dv" style="color: #AD0000;">0</span>)[<span class="st" style="color: #20794D;">'values'</span>], </span>
<span id="cb12-5">        <span class="st" style="color: #20794D;">'contribution'</span>:ebm_local.data(<span class="dv" style="color: #AD0000;">0</span>)[<span class="st" style="color: #20794D;">'scores'</span>]</span>
<span id="cb12-6">    })</span>
<span id="cb12-7">explainDF</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>names</th>
      <th>data</th>
      <th>contribution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Education</td>
      <td>Bachelors</td>
      <td>0.733420</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Age</td>
      <td>47</td>
      <td>1.048227</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Occupation</td>
      <td>?</td>
      <td>-0.318846</td>
    </tr>
    <tr>
      <th>3</th>
      <td>HoursPerWeek</td>
      <td>18</td>
      <td>-0.854202</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>As we can see from the data, we can see we have the Name of the columns, the actual values, and the contribution of that value to the actual prediction score. For this observation let us see what the model is learning -</p>
<ol type="1">
<li>Education as Bachelors is working in favor of &gt;50K income</li>
<li>Age value 47 is also in favor of &gt;50K</li>
<li>Occupation being “?” has a negative impact on &gt;50K income</li>
<li>HoursPerWeek being 18 has a negative impact on &gt;50K income (Average work week hours in the US are around 40, so this makes sense)</li>
</ol>
<p>You can also do it for the entire dataset and collect the importance of each feature. Here is a sample code to do the same.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T00:17:15.361165Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T00:17:15.342591Z&quot;}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">scores <span class="op" style="color: #5E5E5E;">=</span> [x[<span class="st" style="color: #20794D;">'scores'</span>] <span class="cf" style="color: #003B4F;">for</span> x <span class="kw" style="color: #003B4F;">in</span> ebm_local._internal_obj[<span class="st" style="color: #20794D;">'specific'</span>]]</span>
<span id="cb13-2">summary <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(scores)</span>
<span id="cb13-3">summary.columns <span class="op" style="color: #5E5E5E;">=</span> ebm_local.data(<span class="dv" style="color: #AD0000;">0</span>)[<span class="st" style="color: #20794D;">'names'</span>]</span>
<span id="cb13-4">summary.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Education</th>
      <th>Age</th>
      <th>Occupation</th>
      <th>HoursPerWeek</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.733420</td>
      <td>1.048227</td>
      <td>-0.318846</td>
      <td>-0.854202</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.990661</td>
      <td>0.309251</td>
      <td>0.171131</td>
      <td>0.002109</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.257254</td>
      <td>0.735232</td>
      <td>0.171131</td>
      <td>0.002109</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.193118</td>
      <td>0.682721</td>
      <td>-0.417499</td>
      <td>0.279677</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.733420</td>
      <td>0.085672</td>
      <td>0.389171</td>
      <td>0.002109</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Now we can extract the importance of all data rows in our test set.</p>
</section>
</section>
<section id="draw-back-and-concerns" class="level2">
<h2 class="anchored" data-anchor-id="draw-back-and-concerns">2.4 Draw back and concerns</h2>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/xkcd.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Credit - <a href="https://www.facebook.com/TheXKCD/photos/a.10157101743316691/10157786457411691/?type=3">XKCD</a>
</figcaption>
</figure>
<p>These kinds of explanations are still very abstract, even at the observational level reasoning is not human(non-technical) friendly. When the feature count grows this becomes even non-human friendly. Typical business consumers of your model might not be well versed in reading such charts and shy away from trying the insights/predictions the model is giving them. After all, if I don’t understand something, I don’t trust it. That is where art comes in, let’s see how we can build on the above-derived observations and make it easier to understand.</p>
</section>
</section>
<section id="the-art-of-ml-explainability" class="level1">
<h1>3. The art of ML explainability</h1>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The ideas I am going to share now are more marketing than real science.</p>
</div>
</div>
<p>To make people act on your model’s recommendation you must <strong>build trust</strong>. One idea is to build trust to support your explanations with data anecdotes.</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/building_trust.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Credit - <a href="https://www.flickr.com/photos/59632563@N04/6239670686">Flickr</a>
</figcaption>
</figure>
<p>Data anecdotes exist everywhere, this idea came to my mind by looking at a stock market tool (image below). Notice two things they are highlighting -</p>
<ol type="1">
<li><p><strong>What is it? -</strong> The tool does an excellent job showing an event that has just happened. Example - <em>“Microsoft Corp due to dividend announcement</em> or <em>“DELTA AIR LINES 14 Day RSI broke below the 70 level”</em>.</p></li>
<li><p><strong>Why it matters? -</strong> Then the tool points to historical data and tells the significance of this event. In Microsoft’s example, when the event happens “Historically, the price of MSFT has risen an average of 11.9%”.</p></li>
</ol>
<p>What if we can do the same with our models?</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/fidelitySnapshot.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig 6. Snapshots taken from my fidelity tool
</figcaption>
</figure>
<p>We can build a historical odds table from our training data. Let us try building one.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T01:19:13.208838Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T01:19:13.078675Z&quot;}" data-scrolled="true" data-execution_count="138">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">odds_data <span class="op" style="color: #5E5E5E;">=</span> X_train.copy()</span>
<span id="cb14-2">odds_data[<span class="st" style="color: #20794D;">'income'</span>] <span class="op" style="color: #5E5E5E;">=</span> (y_train <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">" &gt;50K"</span>).astype(<span class="bu" style="color: null;">int</span>) </span>
<span id="cb14-3"><span class="co" style="color: #5E5E5E;">## Converting continous variables in buckets</span></span>
<span id="cb14-4">odds_data[<span class="st" style="color: #20794D;">'AgeBucket'</span>] <span class="op" style="color: #5E5E5E;">=</span>  (odds_data.Age <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb14-5">odds_data[<span class="st" style="color: #20794D;">'HoursPerWeekBucket'</span>] <span class="op" style="color: #5E5E5E;">=</span> (odds_data.HoursPerWeek <span class="op" style="color: #5E5E5E;">//</span> <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb14-6"></span>
<span id="cb14-7"><span class="co" style="color: #5E5E5E;"># Creating placeholder for odds dictionary</span></span>
<span id="cb14-8">odds_dict <span class="op" style="color: #5E5E5E;">=</span> {} </span>
<span id="cb14-9"></span>
<span id="cb14-10"><span class="co" style="color: #5E5E5E;"># Columns for which we need odds</span></span>
<span id="cb14-11">columns <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'Education'</span>, <span class="st" style="color: #20794D;">'AgeBucket'</span>, <span class="st" style="color: #20794D;">'HoursPerWeekBucket'</span>, <span class="st" style="color: #20794D;">'Occupation'</span>]</span>
<span id="cb14-12"><span class="cf" style="color: #003B4F;">for</span> colname <span class="kw" style="color: #003B4F;">in</span> columns: <span class="co" style="color: #5E5E5E;">#iterating through each column</span></span>
<span id="cb14-13">    unique_val <span class="op" style="color: #5E5E5E;">=</span> odds_data[colname].unique() <span class="co" style="color: #5E5E5E;"># Finding unique values in column</span></span>
<span id="cb14-14">    ddict <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb14-15">    <span class="cf" style="color: #003B4F;">for</span> val <span class="kw" style="color: #003B4F;">in</span> unique_val: <span class="co" style="color: #5E5E5E;"># iterating each unique value in the column</span></span>
<span id="cb14-16">        <span class="co" style="color: #5E5E5E;">## Odds that income is above &gt; 50 in presence of the val</span></span>
<span id="cb14-17">        val_p <span class="op" style="color: #5E5E5E;">=</span> odds_data.loc[odds_data[colname] <span class="op" style="color: #5E5E5E;">==</span> val, <span class="st" style="color: #20794D;">'income'</span>].mean() </span>
<span id="cb14-18">        <span class="co" style="color: #5E5E5E;">## Odds that income is above &gt; 50 in absence of the val</span></span>
<span id="cb14-19">        val_np <span class="op" style="color: #5E5E5E;">=</span> odds_data.loc[odds_data[colname] <span class="op" style="color: #5E5E5E;">!=</span> val, <span class="st" style="color: #20794D;">'income'</span>].mean()</span>
<span id="cb14-20">        </span>
<span id="cb14-21">        <span class="co" style="color: #5E5E5E;">## Calculate lift</span></span>
<span id="cb14-22">        <span class="cf" style="color: #003B4F;">if</span> val_p <span class="op" style="color: #5E5E5E;">&gt;=</span> val_np:</span>
<span id="cb14-23">            odds <span class="op" style="color: #5E5E5E;">=</span> val_p <span class="op" style="color: #5E5E5E;">/</span> val_np</span>
<span id="cb14-24">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb14-25">            odds <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">*</span>val_np<span class="op" style="color: #5E5E5E;">/</span>(val_p<span class="op" style="color: #5E5E5E;">+</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span>
<span id="cb14-26">        </span>
<span id="cb14-27">        <span class="co" style="color: #5E5E5E;">## Add to the col dict</span></span>
<span id="cb14-28">        ddict[val] <span class="op" style="color: #5E5E5E;">=</span> np.<span class="bu" style="color: null;">round</span>(odds,<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb14-29">    <span class="co" style="color: #5E5E5E;">## Add to the sub dict to odds dict</span></span>
<span id="cb14-30">    odds_dict[colname] <span class="op" style="color: #5E5E5E;">=</span> ddict</span>
<span id="cb14-31"><span class="bu" style="color: null;">print</span>(odds_dict)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>{'Education': {' HS-grad': -1.8, ' Some-college': -1.4, ' 10th': -4.0, ' Masters': 2.5, ' Assoc-voc': 1.1, ' Bachelors': 2.0, ' Assoc-acdm': 1.0, ' 1st-4th': -9.0, ' 11th': -4.7, ' 9th': -4.1, ' Doctorate': 3.1, ' Prof-school': 3.2, ' 12th': -2.9, ' 7th-8th': -3.8, ' 5th-6th': -6.2, ' Preschool': -245.4}, 'AgeBucket': {9: 1.8, 11: 1.5, 7: 1.4, 12: 1.2, 6: -1.1, 10: 1.8, 5: -2.3, 14: -1.2, 8: 1.6, 4: -20.0, 3: -100.0, 15: -1.3, 13: -1.1, 16: -2.1, 18: -2.4, 17: -245.3}, 'HoursPerWeekBucket': {8: -1.2, 9: 1.5, 12: 1.8, 3: -5.2, 7: -1.5, 6: -3.1, 5: -4.7, 10: 2.0, 4: -3.7, 14: 1.6, 2: -3.9, 13: 1.7, 11: 1.9, 15: 1.4, 1: -2.1, 0: -2.4, 19: 1.2, 16: 1.7, 17: 1.1, 18: 1.1}, 'Occupation': {' Machine-op-inspct': -1.8, ' ?': -2.4, ' Other-service': -7.1, ' Craft-repair': -1.0, ' Prof-specialty': 2.1, ' Handlers-cleaners': -3.9, ' Exec-managerial': 2.3, ' Sales': 1.1, ' Adm-clerical': -2.0, ' Transport-moving': -1.2, ' Tech-support': 1.2, ' Protective-serv': 1.4, ' Farming-fishing': -2.0, ' Priv-house-serv': -15.2, ' Armed-Forces': -1.9}}</code></pre>
</div>
</div>
<p>Now using this odds table, we can generate predictions with the pre-filled template. Refer to the image below.</p>
<figure class="figure">
<img src="https://aayushmnit.github.io/posts/2022-09-21-Explainability/outputToText.png" style="width:100%" class="figure-img">
<figcaption align="center" class="figure-caption">
Fig 7. Output to human-readable text
</figcaption>
</figure>
<p>With our <code>explainDF</code> data generated for row index 0 previously, we can use the above framework to convert it to text. Let us see what the output looks like -</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T01:21:35.419255Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T01:21:35.414622Z&quot;}" data-execution_count="141">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;">def</span> explainPredictions(df, pred, odds_dict):</span>
<span id="cb16-2">    reasons <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb16-3">    <span class="cf" style="color: #003B4F;">if</span> pred <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb16-4">        sdf <span class="op" style="color: #5E5E5E;">=</span>  df.loc[df.contribution <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">0</span>, :].sort_values([<span class="st" style="color: #20794D;">'contribution'</span>]).reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).copy()</span>
<span id="cb16-5">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb16-6">        sdf <span class="op" style="color: #5E5E5E;">=</span>  df.loc[df.contribution <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, :].reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).copy()</span>
<span id="cb16-7">    </span>
<span id="cb16-8">    <span class="cf" style="color: #003B4F;">for</span> idx <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(sdf.shape[<span class="dv" style="color: #AD0000;">0</span>]):</span>
<span id="cb16-9">        col_name <span class="op" style="color: #5E5E5E;">=</span> sdf.names[idx]</span>
<span id="cb16-10">        data <span class="op" style="color: #5E5E5E;">=</span> sdf.data[idx]</span>
<span id="cb16-11">        <span class="cf" style="color: #003B4F;">if</span> col_name <span class="kw" style="color: #003B4F;">in</span> odds_dict:</span>
<span id="cb16-12">            odd_value <span class="op" style="color: #5E5E5E;">=</span> odds_dict[col_name][data]</span>
<span id="cb16-13">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb16-14">            odd_value <span class="op" style="color: #5E5E5E;">=</span> odds_dict[col_name<span class="op" style="color: #5E5E5E;">+</span><span class="st" style="color: #20794D;">'Bucket'</span>][data<span class="op" style="color: #5E5E5E;">//</span><span class="dv" style="color: #AD0000;">5</span>]</span>
<span id="cb16-15">            </span>
<span id="cb16-16">        s1 <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"This individual have </span><span class="sc" style="color: #5E5E5E;">{</span>col_name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> value '</span><span class="sc" style="color: #5E5E5E;">{</span>data<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'."</span> </span>
<span id="cb16-17">        s2 <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"Historically, people with this behavior have </span><span class="sc" style="color: #5E5E5E;">{</span>odd_value<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">x likely to have income over $50k."</span></span>
<span id="cb16-18">        reasons.append(s1<span class="op" style="color: #5E5E5E;">+</span>s2)</span>
<span id="cb16-19">    <span class="cf" style="color: #003B4F;">return</span> reasons</span></code></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T01:21:38.957601Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T01:21:38.951508Z&quot;}" data-execution_count="142">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">explainPredictions(explainDF, ebm_local.data(<span class="dv" style="color: #AD0000;">0</span>)[<span class="st" style="color: #20794D;">'perf'</span>][<span class="st" style="color: #20794D;">'predicted'</span>], odds_dict)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="142">
<pre><code>["This individual have HoursPerWeek value '18'.Historically, people with this behavior have -5.2x likely to have income over $50k.",
 "This individual have Occupation value ' ?'.Historically, people with this behavior have -2.4x likely to have income over $50k."]</code></pre>
</div>
</div>
<p>What if this was a &gt;50k prediction?</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-23T01:22:58.736003Z&quot;,&quot;start_time&quot;:&quot;2022-09-23T01:22:58.730966Z&quot;}" data-execution_count="144">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">explainPredictions(explainDF, <span class="dv" style="color: #AD0000;">1</span>, odds_dict)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="144">
<pre><code>["This individual have Education value ' Bachelors'.Historically, people with this behavior have 2.0x likely to have income over $50k.",
 "This individual have Age value '47'.Historically, people with this behavior have 1.8x likely to have income over $50k."]</code></pre>
</div>
</div>
<p>Looks great! We can generate human-readable recommendations for both &gt;50K and &lt;= 50K cases. Hope this provides you with some ideas on how to implement such human-readable explanations.</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>In the blog, we saw the interpretability-accuracy trade-off. How Explainable Boosting Machines (EBMs) work and how they help in building models which are highly interpretable and accurate. We saw how we can use EBMs through code to generate local and global explanations and how historical odd tables and some pre-generated text can help you convert these explanations into more human-friendly text.</p>
<p>I hope you enjoyed reading it, and feel free to use my approach to try it out for your purposes. Also, if there is any feedback on the code or just the blog post, feel free to reach out on <a href="https://www.linkedin.com/in/aayushmnit/">LinkedIn</a> or email me at aayushmnit@gmail.com. You can also follow me on <a href="https://medium.com/@aayushmnit">Medium</a> and <a href="https://github.com/aayushmnit">Github</a> for future blog posts and exploration project codes I might share.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“InterpretML: A Unified Framework for Machine Learning Interpretability” (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)↩︎</p></li>
<li id="fn2"><p><a href="https://interpret.ml/docs/ebm.html">“Interpret ML - EBM documentation”</a>↩︎</p></li>
<li id="fn3"><p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Explainability</category>
  <category>Machine Learning</category>
  <guid>https://aayushmnit.github.io/posts/2022-09-21-Explainability/2022-09-21-Explainability.html</guid>
  <pubDate>Wed, 21 Sep 2022 07:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2022-09-21-Explainability/lego_scientist.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Causal inference with Synthetic Control using Python and SparseSC</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html</link>
  <description><![CDATA[ 



<blockquote class="blockquote">
<p>Understanding Synthetic Control and using <a href="https://github.com/microsoft/SparseSC">Microsoft’s SparceSC</a> package to run synthetic control on larger datasets.</p>
</blockquote>
<section id="what-is-synthetic-control-method" class="level2">
<h2 class="anchored" data-anchor-id="what-is-synthetic-control-method">What is Synthetic Control Method?</h2>
<p>I will try to keep this part short and focus more on why Data scientists should care about such methods and how to use them on larger datasets based on practical experience using <a href="https://github.com/microsoft/SparseSC">SparseSC package</a>.</p>
<p>The Synthetic Control (SC) method is a statistical method used to estimate causal effects from binary treatments on observational panel (longitudinal) data. The method got quite a coverage by being described as <a href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.3">“the most important innovation in the policy evaluation literature in the last few years”</a> and got an article published in <a href="https://www.washingtonpost.com/news/wonk/wp/2015/10/30/how-to-measure-things-in-a-world-of-competing-claims/">Washington Post - Seriously, here’s one amazing math trick to learn what can’t be known</a>. “SC is a technique to create an artificial control group by taking a weighted average of untreated units in such a way that it reproduces the characteristics of the treated units before the intervention(treatment). The SC acts as the counterfactual for a treatment unit, and the estimate of a treatment effect is the difference between the observed outcome in the post-treatment period and the SC’s outcome.”<sup>1</sup></p>
<p>“One way to think of SC is as an improvement upon <a href="https://en.wikipedia.org/wiki/Difference_in_differences">difference-in-difference (DiD) estimation</a>. Typical DiD will compare a treated unit to the average of the control units. But often the treated unit does not look like a typical control (e.g., it might have a different growth rate), in which case the ‘parallel trend’ assumption of DiD is not valid. SC remedies this by choosing a smarter linear combination, rather than the simple average, to weigh more heavily the more similar units. SC’s assumption is if there are endogenous factors that affect treatment and future outcomes then you should be able to control them by matching past outcomes. The matching that SC provides can therefore deal with some problems in estimation that DiD cannot handle.”<sup>2</sup></p>
<p>Here is the link to the Causal inference book which I found most useful to understand the math behind SC- <a href="https://matheusfacure.github.io/python-causality-handbook/15-Synthetic-Control.html">Causal Inference for The Brave and True by Matheus Facure - Chapter 15</a>.</p>
</section>
<section id="why-should-any-data-scientist-care-about-this-method" class="level2">
<h2 class="anchored" data-anchor-id="why-should-any-data-scientist-care-about-this-method">Why should any Data scientist care about this method?</h2>
<p>Often as a Data Scientist, you will encounter situations as follows where running A/B testing is not feasible because of -</p>
<ol type="1">
<li>Lack of infrastructure</li>
<li>Lack of similar groups for running A/B testing (in case of evaluation of state policies, as there is no state equivalent of other)</li>
<li>Providing unwanted advantage to one group over others. Sometimes running an A/B test can give an unfair advantage and lead you into anti-trust territory. For example, what if Amazon tries to charge differential pricing for different customers or apply different margins for their sellers for the same product?</li>
</ol>
<p>As a data scientist, stakeholders may still ask you to estimate the impact of certain changes/treatments, and Synthetic controls can come to the rescue in this situation. For this reason, it is a valuable tool to keep in your algorithmic toolkit.</p>
</section>
<section id="problem-overview" class="level2">
<h2 class="anchored" data-anchor-id="problem-overview">Problem Overview</h2>
<p>We will use the Proposition 99 data to explain the use case for this approach and also how to use the SparceSC library and its key features. “In 1988, California passed a famous Tobacco Tax and Health Protection Act, which became known as Proposition 99. Its primary effect is to impose a 25-cent per pack state excise tax on the sale of tobacco cigarettes within California, with approximately equivalent excise taxes similarly imposed on the retail sale of other commercial tobacco products, such as cigars and chewing tobacco. Additional restrictions placed on the sale of tobacco include a ban on cigarette vending machines in public areas accessible by juveniles, and a ban on the individual sale of single cigarettes. Revenue generated by the act was earmarked for various environmental and health care programs, and anti-tobacco advertisements. To evaluate its effect, we can gather data on cigarette sales from multiple states and across a number of years. In our case, we got data from the year 1970 to 2000 from 39 states.”<sup>3</sup></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:23.628623Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:23.003370Z&quot;}" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Importing required libraries</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> SparseSC</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> datetime <span class="im" style="color: #00769E;">import</span> datetime</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> warnings</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> plotly.express <span class="im" style="color: #00769E;">as</span> px</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> plotly.graph_objects <span class="im" style="color: #00769E;">as</span> pgo</span>
<span id="cb1-9">pd.set_option(<span class="st" style="color: #20794D;">"display.max_columns"</span>, <span class="va" style="color: #111111;">None</span>)</span>
<span id="cb1-10">warnings.filterwarnings(<span class="st" style="color: #20794D;">'ignore'</span>)</span></code></pre></div>
</details>
</div>
<p>Let’s look at the data</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:25.416941Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:25.234802Z&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;">#Import data</span></span>
<span id="cb2-2">data_dir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/"</span></span>
<span id="cb2-3">df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(data_dir <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">"smoking_data"</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">".csv"</span>).drop(columns<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"lnincome"</span>,<span class="st" style="color: #20794D;">"beer"</span>, <span class="st" style="color: #20794D;">"age15to24"</span>])</span>
<span id="cb2-4">df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>state</th>
      <th>year</th>
      <th>cigsale</th>
      <th>retprice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Alabama</td>
      <td>1970.0</td>
      <td>89.8</td>
      <td>39.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Alabama</td>
      <td>1971.0</td>
      <td>95.4</td>
      <td>42.7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Alabama</td>
      <td>1972.0</td>
      <td>101.1</td>
      <td>42.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Alabama</td>
      <td>1973.0</td>
      <td>102.9</td>
      <td>42.1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Alabama</td>
      <td>1974.0</td>
      <td>108.2</td>
      <td>43.1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We have data per <code>state</code> as treatment unit and yearly (<code>year</code> column) per-capita sales of cigarettes in packs (<code>cigsale</code> column) and the cigarette retail price (<code>retprice</code> column). We are going to pivot this data so that each row is one treatment unit(<code>state</code>), and columns represent the yearly <code>cigsale</code> value.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:27.373157Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:27.340194Z&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df <span class="op" style="color: #5E5E5E;">=</span> df.pivot(index<span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'state'</span>, columns <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'year'</span>, values <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"cigsale"</span>)</span>
<span id="cb3-2">df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>year</th>
      <th>1970.0</th>
      <th>1971.0</th>
      <th>1972.0</th>
      <th>1973.0</th>
      <th>1974.0</th>
      <th>1975.0</th>
      <th>1976.0</th>
      <th>1977.0</th>
      <th>1978.0</th>
      <th>1979.0</th>
      <th>1980.0</th>
      <th>1981.0</th>
      <th>1982.0</th>
      <th>1983.0</th>
      <th>1984.0</th>
      <th>1985.0</th>
      <th>1986.0</th>
      <th>1987.0</th>
      <th>1988.0</th>
      <th>1989.0</th>
      <th>1990.0</th>
      <th>1991.0</th>
      <th>1992.0</th>
      <th>1993.0</th>
      <th>1994.0</th>
      <th>1995.0</th>
      <th>1996.0</th>
      <th>1997.0</th>
      <th>1998.0</th>
      <th>1999.0</th>
      <th>2000.0</th>
    </tr>
    <tr>
      <th>state</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>89.8</td>
      <td>95.4</td>
      <td>101.1</td>
      <td>102.9</td>
      <td>108.2</td>
      <td>111.7</td>
      <td>116.2</td>
      <td>117.1</td>
      <td>123.0</td>
      <td>121.4</td>
      <td>123.2</td>
      <td>119.6</td>
      <td>119.1</td>
      <td>116.3</td>
      <td>113.0</td>
      <td>114.5</td>
      <td>116.3</td>
      <td>114.0</td>
      <td>112.1</td>
      <td>105.6</td>
      <td>108.6</td>
      <td>107.9</td>
      <td>109.1</td>
      <td>108.5</td>
      <td>107.1</td>
      <td>102.6</td>
      <td>101.4</td>
      <td>104.9</td>
      <td>106.2</td>
      <td>100.7</td>
      <td>96.2</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>100.3</td>
      <td>104.1</td>
      <td>103.9</td>
      <td>108.0</td>
      <td>109.7</td>
      <td>114.8</td>
      <td>119.1</td>
      <td>122.6</td>
      <td>127.3</td>
      <td>126.5</td>
      <td>131.8</td>
      <td>128.7</td>
      <td>127.4</td>
      <td>128.0</td>
      <td>123.1</td>
      <td>125.8</td>
      <td>126.0</td>
      <td>122.3</td>
      <td>121.5</td>
      <td>118.3</td>
      <td>113.1</td>
      <td>116.8</td>
      <td>126.0</td>
      <td>113.8</td>
      <td>108.8</td>
      <td>113.0</td>
      <td>110.7</td>
      <td>108.7</td>
      <td>109.5</td>
      <td>104.8</td>
      <td>99.4</td>
    </tr>
    <tr>
      <th>California</th>
      <td>123.0</td>
      <td>121.0</td>
      <td>123.5</td>
      <td>124.4</td>
      <td>126.7</td>
      <td>127.1</td>
      <td>128.0</td>
      <td>126.4</td>
      <td>126.1</td>
      <td>121.9</td>
      <td>120.2</td>
      <td>118.6</td>
      <td>115.4</td>
      <td>110.8</td>
      <td>104.8</td>
      <td>102.8</td>
      <td>99.7</td>
      <td>97.5</td>
      <td>90.1</td>
      <td>82.4</td>
      <td>77.8</td>
      <td>68.7</td>
      <td>67.5</td>
      <td>63.4</td>
      <td>58.6</td>
      <td>56.4</td>
      <td>54.5</td>
      <td>53.8</td>
      <td>52.3</td>
      <td>47.2</td>
      <td>41.6</td>
    </tr>
    <tr>
      <th>Colorado</th>
      <td>124.8</td>
      <td>125.5</td>
      <td>134.3</td>
      <td>137.9</td>
      <td>132.8</td>
      <td>131.0</td>
      <td>134.2</td>
      <td>132.0</td>
      <td>129.2</td>
      <td>131.5</td>
      <td>131.0</td>
      <td>133.8</td>
      <td>130.5</td>
      <td>125.3</td>
      <td>119.7</td>
      <td>112.4</td>
      <td>109.9</td>
      <td>102.4</td>
      <td>94.6</td>
      <td>88.8</td>
      <td>87.4</td>
      <td>90.2</td>
      <td>88.3</td>
      <td>88.6</td>
      <td>89.1</td>
      <td>85.4</td>
      <td>83.1</td>
      <td>81.3</td>
      <td>81.2</td>
      <td>79.6</td>
      <td>73.0</td>
    </tr>
    <tr>
      <th>Connecticut</th>
      <td>120.0</td>
      <td>117.6</td>
      <td>110.8</td>
      <td>109.3</td>
      <td>112.4</td>
      <td>110.2</td>
      <td>113.4</td>
      <td>117.3</td>
      <td>117.5</td>
      <td>117.4</td>
      <td>118.0</td>
      <td>116.4</td>
      <td>114.7</td>
      <td>114.1</td>
      <td>112.5</td>
      <td>111.0</td>
      <td>108.5</td>
      <td>109.0</td>
      <td>104.8</td>
      <td>100.6</td>
      <td>91.5</td>
      <td>86.7</td>
      <td>83.5</td>
      <td>79.1</td>
      <td>76.6</td>
      <td>79.3</td>
      <td>76.0</td>
      <td>75.9</td>
      <td>75.5</td>
      <td>73.4</td>
      <td>71.4</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Let’s observe how cigarettes sales per capita is trending over time w.r.t California and other states.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:29.930653Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:29.658886Z&quot;}" data-code_folding="[]" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">plot_df <span class="op" style="color: #5E5E5E;">=</span> df.loc[df.index <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"California"</span>].T.reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb4-2">plot_df[<span class="st" style="color: #20794D;">"OtherStates"</span>] <span class="op" style="color: #5E5E5E;">=</span> df.loc[df.index <span class="op" style="color: #5E5E5E;">!=</span> <span class="st" style="color: #20794D;">"California"</span>].mean(axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>).values</span>
<span id="cb4-3"></span>
<span id="cb4-4"></span>
<span id="cb4-5">fig <span class="op" style="color: #5E5E5E;">=</span> px.line(</span>
<span id="cb4-6">        data_frame <span class="op" style="color: #5E5E5E;">=</span> plot_df, </span>
<span id="cb4-7">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"year"</span>, </span>
<span id="cb4-8">        y <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"California"</span>,<span class="st" style="color: #20794D;">"OtherStates"</span>], </span>
<span id="cb4-9">        template <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"plotly_dark"</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11">fig.add_trace(</span>
<span id="cb4-12">    pgo.Scatter(</span>
<span id="cb4-13">        x<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">1988</span>,<span class="dv" style="color: #AD0000;">1988</span>],</span>
<span id="cb4-14">        y<span class="op" style="color: #5E5E5E;">=</span>[plot_df.California.<span class="bu" style="color: null;">min</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">0.98</span>,plot_df.OtherStates.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">1.02</span>], </span>
<span id="cb4-15">        line<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb4-16">            <span class="st" style="color: #20794D;">'dash'</span>: <span class="st" style="color: #20794D;">'dash'</span>,</span>
<span id="cb4-17">        }, name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Proposition 99'</span></span>
<span id="cb4-18">    ))</span>
<span id="cb4-19">fig.update_layout(</span>
<span id="cb4-20">        title  <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb4-21">            <span class="st" style="color: #20794D;">'text'</span>:<span class="st" style="color: #20794D;">"Gap in per-capita cigarette sales(in packs)"</span>,</span>
<span id="cb4-22">            <span class="st" style="color: #20794D;">'y'</span>:<span class="fl" style="color: #AD0000;">0.95</span>,</span>
<span id="cb4-23">            <span class="st" style="color: #20794D;">'x'</span>:<span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb4-24">        },</span>
<span id="cb4-25">        legend <span class="op" style="color: #5E5E5E;">=</span>  <span class="bu" style="color: null;">dict</span>(y<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, x<span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span>, orientation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'v'</span>),</span>
<span id="cb4-26">        legend_title <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span>,</span>
<span id="cb4-27">        xaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Year"</span>, </span>
<span id="cb4-28">        yaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Cigarette Sales Trend"</span>,</span>
<span id="cb4-29">        font <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb4-30">)</span>
<span id="cb4-31">fig.show(renderer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'notebook'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="9049172b-d9c4-4be8-84f9-fd42e5504e2e" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("9049172b-d9c4-4be8-84f9-fd42e5504e2e")) {                    Plotly.newPlot(                        "9049172b-d9c4-4be8-84f9-fd42e5504e2e",                        [{"hovertemplate":"variable=California<br>year=%{x}<br>value=%{y}<extra></extra>","legendgroup":"California","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"California","orientation":"v","showlegend":true,"x":[1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0,2000.0],"xaxis":"x","y":[123.0,121.0,123.5,124.4,126.7,127.1,128.0,126.4,126.1,121.9,120.2,118.6,115.4,110.8,104.8,102.8,99.7,97.5,90.1,82.4,77.8,68.7,67.5,63.4,58.6,56.4,54.5,53.8,52.3,47.2,41.6],"yaxis":"y","type":"scatter"},{"hovertemplate":"variable=OtherStates<br>year=%{x}<br>value=%{y}<extra></extra>","legendgroup":"OtherStates","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"OtherStates","orientation":"v","showlegend":true,"x":[1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0,2000.0],"xaxis":"x","y":[120.08421052631579,123.86315789473683,129.17894736842106,131.53947368421052,134.6684210526316,136.9315789473684,141.26052631578943,141.0894736842105,140.47368421052633,138.08684210526317,138.08947368421053,137.98684210526318,136.2947368421053,131.25,124.90263157894735,123.11578947368423,120.59473684210529,117.58684210526314,113.82368421052634,109.66315789473686,105.66578947368424,104.34210526315789,103.39473684210526,102.69473684210524,102.11842105263159,103.15789473684208,101.1842105263158,101.78947368421052,100.95789473684208,97.59473684210526,92.13421052631578],"yaxis":"y","type":"scatter"},{"line":{"dash":"dash"},"name":"Proposition 99","x":[1988,1988],"y":[40.768,144.08573684210523],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Cigarette Sales Trend"}},"legend":{"title":{"text":""},"tracegroupgap":0,"y":1,"x":0.8,"orientation":"v"},"margin":{"t":60},"title":{"text":"Gap in per-capita cigarette sales(in packs)","y":0.95,"x":0.5},"font":{"size":15}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('9049172b-d9c4-4be8-84f9-fd42e5504e2e');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Fig 1 - Cigarette sales comparison b/w California and other states</p>
</div>
</div>
<p>As we can see from the chart above, we can see that there is a general decline in cigarette sales after the 1980s, and with the introduction of Proposition 99, the decreasing trend accelerated for the state of California. We cannot say for sure if this is happening with any statistical significance, it is just something we observed by examining the chart above.</p>
<p>To answer the question of whether Proposition 99 influenced cigarette consumption, we will use the pre-intervention period (1970-1988) to build a synthetic control group that mimics California cigarette sales trend. Then, we will see how this synthetic control behaves after the intervention.</p>
</section>
<section id="fitting-synthetic-control-using-sparsesc-package" class="level2">
<h2 class="anchored" data-anchor-id="fitting-synthetic-control-using-sparsesc-package">Fitting Synthetic Control using SparseSC package</h2>
<p>On a high level <code>SparseSC</code> package provide two functions for fitting Synthetic controls i.e., <code>fit()</code> method and <code>fit_fast()</code> method. On a high level -</p>
<ul>
<li><code>fit()</code> - This method tries to compute the weight jointly and results in SCs which are ‘optimal’. This is the most common method used in most of the code/libraries I have found but it is computationally expensive and takes a long time to run. Hence, does not scale for larger datasets.</li>
<li><code>fit_fast()</code>- This method tries to compute the weight separately by performing some non-matching analysis. This solution is much faster and often the only feasible method with larger datasets. The authors of this package recommend the <code>fit_fast</code> method to start with and only move towards the <code>fit</code> method if needed.</li>
</ul>
<p>The <a href="https://sparsesc.readthedocs.io/en/latest/api_ref.html#fit-a-synthetic-controls-model-fast-separate"><code>SparseSC.fit_fast()</code></a> method required at least three arguments -</p>
<ol type="1">
<li><strong>features</strong> - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from pre-treatment (1970-1988), and the value in the matrix is the metric of interest (in this case it is the <code>cigsale</code> value)</li>
<li><strong>targets</strong> - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), each column is the period from post-treatment (1999-2000), and the value in the matrix is the metric of interest (in this case it is the <code>cigsale</code> value)</li>
<li><strong>treatment_units</strong> - This is the list of integers containing the row index value of treated units</li>
</ol>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that treatment units can be a list of multiple treatment indexes. Think of cases where the same treatment is applied to multiple groups, for example, what if proposition 99 was rolled in both California and Minnesota State, in this case, treatment_units will get [2, 15], which are the respective index of these states.</p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:35.677803Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:33.370988Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;">## creating required features</span></span>
<span id="cb5-2">features <span class="op" style="color: #5E5E5E;">=</span> df.iloc[:,df.columns <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="dv" style="color: #AD0000;">1988</span>].values</span>
<span id="cb5-3">targets <span class="op" style="color: #5E5E5E;">=</span> df.iloc[:,df.columns <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">1988</span>].values</span>
<span id="cb5-4">treated_units <span class="op" style="color: #5E5E5E;">=</span> [idx <span class="cf" style="color: #003B4F;">for</span> idx, val <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(df.index.values) <span class="cf" style="color: #003B4F;">if</span> val <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'California'</span>] <span class="co" style="color: #5E5E5E;"># [2]</span></span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;">## Fit fast model for fitting Synthetic controls</span></span>
<span id="cb5-7">sc_model <span class="op" style="color: #5E5E5E;">=</span> SparseSC.fit_fast( </span>
<span id="cb5-8">    features<span class="op" style="color: #5E5E5E;">=</span>features,</span>
<span id="cb5-9">    targets<span class="op" style="color: #5E5E5E;">=</span>targets,</span>
<span id="cb5-10">    treated_units<span class="op" style="color: #5E5E5E;">=</span>treated_units</span>
<span id="cb5-11">)</span></code></pre></div>
</div>
<p>Now that we have fitted the model, let’s get the Synthetic Control output by using <code>predict()</code> function.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:37.998206Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:37.987033Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">result <span class="op" style="color: #5E5E5E;">=</span> df.loc[df.index <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'California'</span>].T.reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb6-2">result.columns <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"year"</span>, <span class="st" style="color: #20794D;">"Observed"</span>] </span>
<span id="cb6-3">result[<span class="st" style="color: #20794D;">'Synthetic'</span>] <span class="op" style="color: #5E5E5E;">=</span> sc_model.predict(df.values)[treated_units,:][<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb6-4">result.head(<span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>year</th>
      <th>Observed</th>
      <th>Synthetic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1970.0</td>
      <td>123.0</td>
      <td>122.394195</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1971.0</td>
      <td>121.0</td>
      <td>125.114849</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1972.0</td>
      <td>123.5</td>
      <td>129.704372</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1973.0</td>
      <td>124.4</td>
      <td>126.753988</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1974.0</td>
      <td>126.7</td>
      <td>126.276394</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Now that we have our synthetic control, we can plot it with the outcome variable of the State of California.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:42.730642Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:42.650636Z&quot;}" data-code_folding="[]" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">fig <span class="op" style="color: #5E5E5E;">=</span> px.line(</span>
<span id="cb7-2">        data_frame <span class="op" style="color: #5E5E5E;">=</span> result, </span>
<span id="cb7-3">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"year"</span>, </span>
<span id="cb7-4">        y <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"Observed"</span>,<span class="st" style="color: #20794D;">"Synthetic"</span>], </span>
<span id="cb7-5">        template <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"plotly_dark"</span>,)</span>
<span id="cb7-6"></span>
<span id="cb7-7">fig.add_trace(</span>
<span id="cb7-8">    pgo.Scatter(</span>
<span id="cb7-9">        x<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">1988</span>,<span class="dv" style="color: #AD0000;">1988</span>],</span>
<span id="cb7-10">        y<span class="op" style="color: #5E5E5E;">=</span>[result.Observed.<span class="bu" style="color: null;">min</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">0.98</span>,result.Observed.<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">1.02</span>], </span>
<span id="cb7-11">        line<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb7-12">            <span class="st" style="color: #20794D;">'dash'</span>: <span class="st" style="color: #20794D;">'dash'</span>,</span>
<span id="cb7-13">        }, name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Proposition 99'</span></span>
<span id="cb7-14">    ))</span>
<span id="cb7-15">fig.update_layout(</span>
<span id="cb7-16">        title  <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb7-17">            <span class="st" style="color: #20794D;">'text'</span>:<span class="st" style="color: #20794D;">"Synthetic Control Assessment"</span>,</span>
<span id="cb7-18">            <span class="st" style="color: #20794D;">'y'</span>:<span class="fl" style="color: #AD0000;">0.95</span>,</span>
<span id="cb7-19">            <span class="st" style="color: #20794D;">'x'</span>:<span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb7-20">        },</span>
<span id="cb7-21">        legend <span class="op" style="color: #5E5E5E;">=</span>  <span class="bu" style="color: null;">dict</span>(y<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, x<span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span>, orientation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'v'</span>),</span>
<span id="cb7-22">        legend_title <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span>,</span>
<span id="cb7-23">        xaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Year"</span>, </span>
<span id="cb7-24">        yaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Per-capita cigarette sales (in packs)"</span>,</span>
<span id="cb7-25">        font <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb7-26">)</span>
<span id="cb7-27">fig.show(renderer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'notebook'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="5b638e3c-41d1-456b-9e6a-405ce56a52fa" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5b638e3c-41d1-456b-9e6a-405ce56a52fa")) {                    Plotly.newPlot(                        "5b638e3c-41d1-456b-9e6a-405ce56a52fa",                        [{"hovertemplate":"variable=Observed<br>year=%{x}<br>value=%{y}<extra></extra>","legendgroup":"Observed","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"Observed","orientation":"v","showlegend":true,"x":[1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0,2000.0],"xaxis":"x","y":[123.0,121.0,123.5,124.4,126.7,127.1,128.0,126.4,126.1,121.9,120.2,118.6,115.4,110.8,104.8,102.8,99.7,97.5,90.1,82.4,77.8,68.7,67.5,63.4,58.6,56.4,54.5,53.8,52.3,47.2,41.6],"yaxis":"y","type":"scatter"},{"hovertemplate":"variable=Synthetic<br>year=%{x}<br>value=%{y}<extra></extra>","legendgroup":"Synthetic","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"Synthetic","orientation":"v","showlegend":true,"x":[1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0,2000.0],"xaxis":"x","y":[122.39419543795634,125.11484907716071,129.70437153278635,126.75398805805592,126.27639360089289,125.86385415224272,129.67871364848983,126.76338485105019,124.82565047900707,121.53770717432565,119.07320698588394,118.94467812486528,116.03906494940466,112.00211743939204,104.84744039907635,101.9020730834,98.98909548538309,97.48915714644018,90.57919413391346,87.17813806953373,81.2825247021521,77.88963882286453,76.53169130078273,77.58881937062345,76.92113408610153,76.96745449890892,76.49989174501248,76.9078326174167,77.93215325057477,76.91902987423963,70.38613013210903],"yaxis":"y","type":"scatter"},{"line":{"dash":"dash"},"name":"Proposition 99","x":[1988,1988],"y":[40.768,130.56],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Per-capita cigarette sales (in packs)"}},"legend":{"title":{"text":""},"tracegroupgap":0,"y":1,"x":0.8,"orientation":"v"},"margin":{"t":60},"title":{"text":"Synthetic Control Assessment","y":0.95,"x":0.5},"font":{"size":15}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('5b638e3c-41d1-456b-9e6a-405ce56a52fa');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Fig - Assessment of Proposition 99 on state of California using Synthetic Control</p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the pre-intervention period, the synthetic control does not reproduce the treatment exactly but follows the curve closely. This is a good sign, as it indicates that we are not overfitting. Also, note that we do see divergence after the intervention (introduction of Proposition 99) after 1988.</p>
</div>
</div>
<p>With the synthetic control groups in hand, we can estimate the treatment effect as the gap between the treated and the synthetic control outcomes.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:50.865003Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:50.790688Z&quot;}" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">result[<span class="st" style="color: #20794D;">'California Effect'</span>] <span class="op" style="color: #5E5E5E;">=</span> result.Observed <span class="op" style="color: #5E5E5E;">-</span> result.Synthetic</span>
<span id="cb8-2">fig <span class="op" style="color: #5E5E5E;">=</span> px.line(</span>
<span id="cb8-3">        data_frame <span class="op" style="color: #5E5E5E;">=</span> result, </span>
<span id="cb8-4">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"year"</span>, </span>
<span id="cb8-5">        y <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"California Effect"</span>, </span>
<span id="cb8-6">        template <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"plotly_dark"</span>,)</span>
<span id="cb8-7">fig.add_hline(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb8-8">fig.add_trace(</span>
<span id="cb8-9">    pgo.Scatter(</span>
<span id="cb8-10">        x<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">1988</span>,<span class="dv" style="color: #AD0000;">1988</span>],</span>
<span id="cb8-11">        y<span class="op" style="color: #5E5E5E;">=</span>[result[<span class="st" style="color: #20794D;">"California Effect"</span>].<span class="bu" style="color: null;">min</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">0.98</span>,result[<span class="st" style="color: #20794D;">"California Effect"</span>].<span class="bu" style="color: null;">max</span>()<span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">1.02</span>], </span>
<span id="cb8-12">        line<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb8-13">            <span class="st" style="color: #20794D;">'dash'</span>: <span class="st" style="color: #20794D;">'dash'</span>,</span>
<span id="cb8-14">        }, name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Proposition 99'</span></span>
<span id="cb8-15">    ))</span>
<span id="cb8-16"></span>
<span id="cb8-17">fig.update_layout(</span>
<span id="cb8-18">        title  <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb8-19">            <span class="st" style="color: #20794D;">'text'</span>:<span class="st" style="color: #20794D;">"Difference across time"</span>,</span>
<span id="cb8-20">            <span class="st" style="color: #20794D;">'y'</span>:<span class="fl" style="color: #AD0000;">0.95</span>,</span>
<span id="cb8-21">            <span class="st" style="color: #20794D;">'x'</span>:<span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb8-22">        },</span>
<span id="cb8-23">        legend <span class="op" style="color: #5E5E5E;">=</span>  <span class="bu" style="color: null;">dict</span>(y<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, x<span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span>, orientation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'v'</span>),</span>
<span id="cb8-24">        legend_title <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span>,</span>
<span id="cb8-25">        xaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Year"</span>, </span>
<span id="cb8-26">        yaxis_title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Gap in Per-capita cigarette sales (in packs)"</span>,</span>
<span id="cb8-27">        font <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>)</span>
<span id="cb8-28">)</span>
<span id="cb8-29">fig.show(renderer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'notebook'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="b3d2e8d9-0ca6-4a27-be8e-2a414ef4a415" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b3d2e8d9-0ca6-4a27-be8e-2a414ef4a415")) {                    Plotly.newPlot(                        "b3d2e8d9-0ca6-4a27-be8e-2a414ef4a415",                        [{"hovertemplate":"year=%{x}<br>California Effect=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"x":[1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0,2000.0],"xaxis":"x","y":[0.6058045620436587,-4.114849077160713,-6.204371532786354,-2.3539880580559185,0.42360639910711484,1.236145847757271,-1.6787136484898326,-0.3633848510501849,1.2743495209929279,0.36229282567435916,1.1267930141160605,-0.3446781248652826,-0.6390649494046556,-1.2021174393920404,-0.04744039907635056,0.8979269165999995,0.7109045146169137,0.010842853559822174,-0.47919413391346666,-4.778138069533725,-3.482524702152105,-9.189638822864524,-9.031691300782725,-14.188819370623456,-18.32113408610153,-20.567454498908923,-21.999891745012476,-23.107832617416705,-25.632153250574774,-29.719029874239624,-28.78613013210903],"yaxis":"y","type":"scatter"},{"line":{"dash":"dash"},"name":"Proposition 99","x":[1988,1988],"y":[-29.12464927675483,1.2998365114127866],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Gap in Per-capita cigarette sales (in packs)"}},"legend":{"tracegroupgap":0,"y":1,"x":0.8,"orientation":"v","title":{"text":""}},"margin":{"t":60},"shapes":[{"type":"line","x0":0,"x1":1,"xref":"x domain","y0":0,"y1":0,"yref":"y"}],"title":{"text":"Difference across time","y":0.95,"x":0.5},"font":{"size":15}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b3d2e8d9-0ca6-4a27-be8e-2a414ef4a415');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
<p>Fig - Gap in Per-capita cigarette sales in California w.r.t Synthetic Control</p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:51:54.340763Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:54.335232Z&quot;}" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Effect of Proposition 99 w.r.t Synthetic Control =&gt; </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span><span class="bu" style="color: null;">round</span>(result.loc[result.year<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">2000</span>,<span class="st" style="color: #20794D;">'California Effect'</span>].values[<span class="dv" style="color: #AD0000;">0</span>],<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> packs"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Effect of Proposition 99 w.r.t Synthetic Control =&gt; -28.8 packs</code></pre>
</div>
</div>
<p>Looking at the chart above, we can observe that by the year 2000, Proposition 99 has reduced the sales of cigarettes by ~29 packs. Now we will figure out if this is statistically significant.</p>
</section>
<section id="making-inference" class="level2">
<h2 class="anchored" data-anchor-id="making-inference">Making Inference</h2>
<p>In Synthetic control, to find if the effect we observed is significant or not, we run a placebo test. A placebo test is taking a random untreated unit and pretending all other units are in control and fit the Synthetic control over this randomly selected untreated unit to estimate the effect. Once we have repeated this placebo test multiple times, we can estimate the distribution of this randomly observed effect and see if the effect observed is significantly different from the placebo observed effect. In the <code>SparceSC</code> package, we can use the <code>estimate_effects</code> method to do this automatically for us. The estimate effects method takes a minimum of two arguments -</p>
<ol type="1">
<li><strong>outcomes</strong> - This is the NumPy matrix of I/p variables where each row represents a treatment/control unit (states in our case), and each column is the period of both pre-treatment and post-treatment period and the value in the matrix is the metric of interest (in this case it is the <code>cigsale</code> value)</li>
<li><strong>unit_treatment_periods</strong> - Vector of treatment periods for each unit, (if a unit is never treated then use np.NaN if vector refers to periods by numerical index)</li>
</ol>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:52:22.476821Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:51:57.343787Z&quot;}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;">## Creating unit treatment_periods</span></span>
<span id="cb11-2">unit_treatment_periods <span class="op" style="color: #5E5E5E;">=</span> np.full((df.values.shape[<span class="dv" style="color: #AD0000;">0</span>]), np.nan)</span>
<span id="cb11-3">unit_treatment_periods[treated_units] <span class="op" style="color: #5E5E5E;">=</span> [idx <span class="cf" style="color: #003B4F;">for</span> idx, colname <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(df.columns) <span class="cf" style="color: #003B4F;">if</span> colname <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1988</span>][<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="co" style="color: #5E5E5E;">## fitting estimate effects method</span></span>
<span id="cb11-6">sc <span class="op" style="color: #5E5E5E;">=</span> SparseSC.estimate_effects(</span>
<span id="cb11-7">    outcomes <span class="op" style="color: #5E5E5E;">=</span> df.values,  </span>
<span id="cb11-8">    unit_treatment_periods <span class="op" style="color: #5E5E5E;">=</span> unit_treatment_periods, </span>
<span id="cb11-9">    max_n_pl<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>, <span class="co" style="color: #5E5E5E;"># Number of placebos</span></span>
<span id="cb11-10">    level<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span> <span class="co" style="color: #5E5E5E;"># Level for confidence intervals</span></span>
<span id="cb11-11">)</span>
<span id="cb11-12"><span class="bu" style="color: null;">print</span>(sc)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pre-period fit diagnostic: Were the treated harder to match in the pre-period than the controls were.
Average difference in outcome for pre-period between treated and SC unit (concerning if p-value close to 0 ): 
1.8073663793403947 (p-value: 0.9743589743589743)

(Investigate per-period match quality more using self.pl_res_pre.effect_vec)

Average Effect Estimation: -16.965374734951705 (p-value: 0.07692307692307693)

Effect Path Estimation:
 -2.712194133284143 (p-value: 0.6923076923076923)
-6.424223898557088 (p-value: 0.20512820512820512)
-4.18303325159971 (p-value: 0.5128205128205128)
-10.210104128966762 (p-value: 0.28205128205128205)
-10.198518971790051 (p-value: 0.2564102564102564)
-14.921163932323616 (p-value: 0.15384615384615385)
-18.441946863077938 (p-value: 0.1282051282051282)
-21.27793738802989 (p-value: 0.1794871794871795)
-22.47075245885469 (p-value: 0.1794871794871795)
-23.397467590595554 (p-value: 0.20512820512820512)
-26.13637789807555 (p-value: 0.05128205128205128)
-30.504443997992325 (p-value: 0.02564102564102564)
-29.67170704122487 (p-value: 0.05128205128205128)

 </code></pre>
</div>
</div>
<p>The <code>estimate_effects</code> method returns an object which will print the treatment effect of each post-treatment year and estimate the significance of the observed difference. The information printed can also be found in the <code>pl_res_pre</code> function of the returned object.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2022-09-20T02:52:22.482984Z&quot;,&quot;start_time&quot;:&quot;2022-09-20T02:52:22.479393Z&quot;}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Estimated effect of sales in California state in year 2000 because of preposition 99 is </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span><span class="bu" style="color: null;">round</span>(sc.pl_res_post.effect_vec.effect[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, </span><span class="ch" style="color: #20794D;">\</span></span>
<span id="cb13-2"><span class="ss" style="color: #20794D;">with a p-value of  </span><span class="sc" style="color: #5E5E5E;">{</span>np<span class="sc" style="color: #5E5E5E;">.</span><span class="bu" style="color: null;">round</span>(sc.pl_res_post.effect_vec.p[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>],<span class="dv" style="color: #AD0000;">2</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated effect of sales in California state in year 2000 because of preposition 99 is -30.0, with a p-value of  0.05</code></pre>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Here are some key takeaways -</p>
<ol type="1">
<li>Synthetic control allows us to combine multiple control units to make them resemble the treated unit. With synthetic control, we can estimate what would have happened to our treated unit in the absence of treatment.</li>
<li>Microsoft SparseSC library provides a fast and easy-to-use API to run synthetic control groups and allows us to run a placebo test to estimate the significance of the effects observed.</li>
</ol>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://sparsesc.readthedocs.io/en/latest/overview.html#">Sparce SC documentation</a>↩︎</p></li>
<li id="fn2"><p><a href="https://sparsesc.readthedocs.io/en/latest/overview.html#">Sparce SC documentation</a>↩︎</p></li>
<li id="fn3"><p><a href="https://matheusfacure.github.io/python-causality-handbook/15-Synthetic-Control.html">Causal Inference for The Brave and True by Matheus Facure - Chapter 15</a>↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Causal Inference</category>
  <category>Synthetic Control</category>
  <guid>https://aayushmnit.github.io/posts/2022-09-19-SyntheticControl/2022-09-19_SyntheticControl.html</guid>
  <pubDate>Mon, 19 Sep 2022 07:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2022-09-19-SyntheticControl/california99.png" medium="image" type="image/png" height="90" width="144"/>
</item>
<item>
  <title>Finding similar images using Deep learning and Locality Sensitive Hashing</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html</link>
  <description><![CDATA[ 



<p>Blog Transferred to <a href="https://towardsdatascience.com/finding-similar-images-using-deep-learning-and-locality-sensitive-hashing-9528afee02f5">Medium.com</a>.</p>



 ]]></description>
  <category>Deep Learning</category>
  <category>FastAI</category>
  <category>Pytorch</category>
  <category>Vision</category>
  <guid>https://aayushmnit.github.io/posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/index.html</guid>
  <pubDate>Sun, 17 Mar 2019 07:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2019-03-17-Finding_similar_images_using_Deep_learning_and_Locality_Sensitive_Hashing/similar.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>Real-time Multi-Facial attribute detection using computer vision and deep learning with FastAI and OpenCV</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html</link>
  <description><![CDATA[ 



<p>Blog Transferred to <a href="https://medium.com/@aayushmnit/real-time-multi-facial-attribute-detection-using-transfer-learning-and-haar-cascades-with-fastai-47ff59e36df0">Medium.com</a>.</p>



 ]]></description>
  <category>Deep Learning</category>
  <category>FastAI</category>
  <category>Pytorch</category>
  <category>Vision</category>
  <category>Video</category>
  <guid>https://aayushmnit.github.io/posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/index.html</guid>
  <pubDate>Sun, 17 Feb 2019 08:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2019-02-17-Multi_Facial_attribute_detection_using_FastAI_and_OpenCV/face.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>MultiLayer Perceptron using Fastai and Pytorch</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html</link>
  <description><![CDATA[ 



<p>Blog Transferred to <a href="https://medium.com/@aayushmnit/multi-layer-perceptron-usingfastai-and-pytorch-9e401dd288b8">Medium.com</a>.</p>



 ]]></description>
  <category>Deep Learning</category>
  <category>FastAI</category>
  <category>Pytorch</category>
  <category>Vision</category>
  <guid>https://aayushmnit.github.io/posts/2019-01-05-Multi_Layer_perceptron_using_Fastai_and_Pytorch/index.html</guid>
  <pubDate>Sat, 05 Jan 2019 08:00:00 GMT</pubDate>
  <media:content url="https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/single_layer_mlp.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Leaf Disease detection by Tranfer learning using FastAI V1 library</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html</link>
  <description><![CDATA[ 



<p>Blog Transferred to <a href="https://towardsdatascience.com/transfer-learning-using-the-fastai-library-d686b238213e">Medium.com</a>.</p>



 ]]></description>
  <category>Deep Learning</category>
  <category>FastAI</category>
  <category>Vision</category>
  <guid>https://aayushmnit.github.io/posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/index.html</guid>
  <pubDate>Sun, 28 Oct 2018 07:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2018-10-28-Leaf_Disease_detection_by_Tranfer_learning_using_FastAI_V1_library/leaf_classification.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>Multi-Layer perceptron using Tensorflow</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html</link>
  <description><![CDATA[ 



<p>Blog Transferred to <a href="https://medium.com/@aayushmnit/multi-layer-perceptron-using-tensorflow-9f3e218a4809">Medium.com</a>.</p>



 ]]></description>
  <category>Machine Learning</category>
  <category>Deep Learning</category>
  <guid>https://aayushmnit.github.io/posts/2018-09-12-Multi_Layer_perceptron_using_Tensorflow/index.html</guid>
  <pubDate>Wed, 12 Sep 2018 07:00:00 GMT</pubDate>
  <media:content url="https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/single_layer_mlp.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Building Neural Network from scratch</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/index.html</link>
  <description><![CDATA[ 



<p>In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let’s start with the very basics of a neural network which is Multilayer Perceptron. You can find the same blog in notebook version <a href="https://github.com/aayushmnit/Deep_learning_explorations/tree/master/1_MLP_from_scratch">here</a>.</p>
<section id="what-is-a-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-neural-network">What is a neural network?</h2>
<p>A neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it’s the same with a neural network.</p>
</section>
<section id="what-is-a-multi-layer-perceptron" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-multi-layer-perceptron">What is a Multi layer perceptron?</h2>
Multi-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let’s start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.<br> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/eq_perceptron.png" align="center"> <br> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/https:/raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/perceptron.png" align="center">
<center>
Fig 1: Perceptron image
</center>
<br>
<div data-align="right">
Image credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/
</div>
Now, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology. <br> <strong>Dense layer,</strong> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/eq_dense.png"> <br> <em>Note that bias term is now a vector and W is a weight matrix</em> <br> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/https:/raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/single_layer_mlp.png" align="center">
<center>
Fig: Single dense layer perceptron network
</center>
<br>
<div data-align="right">
Image credit=http://www.texample.net/tikz/examples/neural-network/
</div>
<p>Now we understand dense layer let’s add a bunch of them, and that network becomes a multi-layer perceptron network.</p>
<img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/https:/raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/multi_layer_mlp.png" align="center">
<center>
Fig: Multi layer perceptron network
</center>
<br>
<div data-align="right">
Image credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2s
</div>
<p>If you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero. <br> <strong>ReLU function</strong> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/eq_relu.png"> <br> Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a <strong>feed-forward loop</strong> and <strong>backpropagation loop</strong>. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don’t understand the concept of gradient weight updates and SGD, I recommend you to watch <a href="https://www.coursera.org/ml">week 1 of Machine learning by Andrew NG lectures</a>.</p>
<p>So, to summarize a neural network needs few building blocks</p>
<ul>
<li><strong>Dense layer</strong> - a fully-connected layer, <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/eq_dense.png"></li>
<li><strong>ReLU layer</strong> (or any other activation function to introduce non-linearity)</li>
<li><strong>Loss function</strong> - (crossentropy in case of multi-class classification problem)</li>
<li><strong>Backprop algorithm</strong> - a stochastic gradient descent with backpropageted gradients</li>
</ul>
<p>Let’s approach them one at a time.</p>
</section>
<section id="coding-starts-here" class="level2">
<h2 class="anchored" data-anchor-id="coding-starts-here">Coding Starts here:</h2>
<p>Let’s start by importing some libraires required for creating our neural network.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> __future__ <span class="im" style="color: #00769E;">import</span> print_function</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np <span class="co" style="color: #5E5E5E;">## For numerical python</span></span>
<span id="cb1-3">np.random.seed(<span class="dv" style="color: #AD0000;">42</span>)</span></code></pre></div>
<p>Every layer will have a forward pass and backpass implementation. Let’s create a main class layer which can do a forward pass <em>.forward()</em> and Backward pass <em>.backward().</em></p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> Layer:</span>
<span id="cb2-2">    </span>
<span id="cb2-3">    <span class="co" style="color: #5E5E5E;">#A building block. Each layer is capable of performing two things:</span></span>
<span id="cb2-4"></span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;">#- Process input to get output:           output = layer.forward(input)</span></span>
<span id="cb2-6">    </span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;">#- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)</span></span>
<span id="cb2-8">    </span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;">#Some layers also have learnable parameters which they update during layer.backward.</span></span>
<span id="cb2-10">    </span>
<span id="cb2-11">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb2-12">        <span class="co" style="color: #5E5E5E;"># Here we can initialize layer parameters (if any) and auxiliary stuff.</span></span>
<span id="cb2-13">        <span class="co" style="color: #5E5E5E;"># A dummy layer does nothing</span></span>
<span id="cb2-14">        <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb2-15">    </span>
<span id="cb2-16">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, <span class="bu" style="color: null;">input</span>):</span>
<span id="cb2-17">        <span class="co" style="color: #5E5E5E;"># Takes input data of shape [batch, input_units], returns output data [batch, output_units]</span></span>
<span id="cb2-18">        </span>
<span id="cb2-19">        <span class="co" style="color: #5E5E5E;"># A dummy layer just returns whatever it gets as input.</span></span>
<span id="cb2-20">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">input</span></span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>, <span class="bu" style="color: null;">input</span>, grad_output):</span>
<span id="cb2-23">        <span class="co" style="color: #5E5E5E;"># Performs a backpropagation step through the layer, with respect to the given input.</span></span>
<span id="cb2-24">        </span>
<span id="cb2-25">        <span class="co" style="color: #5E5E5E;"># To compute loss gradients w.r.t input, we need to apply chain rule (backprop):</span></span>
<span id="cb2-26">        </span>
<span id="cb2-27">        <span class="co" style="color: #5E5E5E;"># d loss / d x  = (d loss / d layer) * (d layer / d x)</span></span>
<span id="cb2-28">        </span>
<span id="cb2-29">        <span class="co" style="color: #5E5E5E;"># Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.</span></span>
<span id="cb2-30">        </span>
<span id="cb2-31">        <span class="co" style="color: #5E5E5E;"># If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer</span></span>
<span id="cb2-32">        </span>
<span id="cb2-33">        <span class="co" style="color: #5E5E5E;"># The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly</span></span>
<span id="cb2-34">        num_units <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">input</span>.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb2-35">        </span>
<span id="cb2-36">        d_layer_d_input <span class="op" style="color: #5E5E5E;">=</span> np.eye(num_units)</span>
<span id="cb2-37">        </span>
<span id="cb2-38">        <span class="cf" style="color: #003B4F;">return</span> np.dot(grad_output, d_layer_d_input) <span class="co" style="color: #5E5E5E;"># chain rule</span></span></code></pre></div>
<section id="nonlinearity-relu-layer" class="level3">
<h3 class="anchored" data-anchor-id="nonlinearity-relu-layer">Nonlinearity ReLU layer</h3>
<p>This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">class</span> ReLU(Layer):</span>
<span id="cb3-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb3-3">        <span class="co" style="color: #5E5E5E;"># ReLU layer simply applies elementwise rectified linear unit to all inputs</span></span>
<span id="cb3-4">        <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb3-5">    </span>
<span id="cb3-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, <span class="bu" style="color: null;">input</span>):</span>
<span id="cb3-7">        <span class="co" style="color: #5E5E5E;"># Apply elementwise ReLU to [batch, input_units] matrix</span></span>
<span id="cb3-8">        relu_forward <span class="op" style="color: #5E5E5E;">=</span> np.maximum(<span class="dv" style="color: #AD0000;">0</span>,<span class="bu" style="color: null;">input</span>)</span>
<span id="cb3-9">        <span class="cf" style="color: #003B4F;">return</span> relu_forward</span>
<span id="cb3-10">    </span>
<span id="cb3-11">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>, <span class="bu" style="color: null;">input</span>, grad_output):</span>
<span id="cb3-12">        <span class="co" style="color: #5E5E5E;"># Compute gradient of loss w.r.t. ReLU input</span></span>
<span id="cb3-13">        relu_grad <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">input</span> <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;">return</span> grad_output<span class="op" style="color: #5E5E5E;">*</span>relu_grad </span></code></pre></div>
</section>
<section id="dense-layer" class="level3">
<h3 class="anchored" data-anchor-id="dense-layer">Dense layer</h3>
<p>Now let’s build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.</p>
<p>A dense layer applies affine transformation. In a vectorized form, it can be described as:<br> <img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/eq_dense.png"></p>
<p>Where * X is an object-feature matrix of shape [batch_size, num_features], * W is a weight matrix [num_features, num_outputs] * and b is a vector of num_outputs biases.</p>
<p>Both W and b are initialized during layer creation and updated each time backward is called. Note that we are using <strong>Xavier initialization</strong> which is a trick to train our model to converge faster <a href="http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization">read more</a>. Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">class</span> Dense(Layer):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, input_units, output_units, learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>):</span>
<span id="cb4-3">        <span class="co" style="color: #5E5E5E;"># A dense layer is a layer which performs a learned affine transformation:</span></span>
<span id="cb4-4">        <span class="co" style="color: #5E5E5E;"># f(x) = &lt;W*x&gt; + b</span></span>
<span id="cb4-5">        </span>
<span id="cb4-6">        <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">=</span> learning_rate</span>
<span id="cb4-7">        <span class="va" style="color: #111111;">self</span>.weights <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(loc<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.0</span>, </span>
<span id="cb4-8">                                        scale <span class="op" style="color: #5E5E5E;">=</span> np.sqrt(<span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">/</span>(input_units<span class="op" style="color: #5E5E5E;">+</span>output_units)), </span>
<span id="cb4-9">                                        size <span class="op" style="color: #5E5E5E;">=</span> (input_units,output_units))</span>
<span id="cb4-10">        <span class="va" style="color: #111111;">self</span>.biases <span class="op" style="color: #5E5E5E;">=</span> np.zeros(output_units)</span>
<span id="cb4-11">        </span>
<span id="cb4-12">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>,<span class="bu" style="color: null;">input</span>):</span>
<span id="cb4-13">        <span class="co" style="color: #5E5E5E;"># Perform an affine transformation:</span></span>
<span id="cb4-14">        <span class="co" style="color: #5E5E5E;"># f(x) = &lt;W*x&gt; + b</span></span>
<span id="cb4-15">        </span>
<span id="cb4-16">        <span class="co" style="color: #5E5E5E;"># input shape: [batch, input_units]</span></span>
<span id="cb4-17">        <span class="co" style="color: #5E5E5E;"># output shape: [batch, output units]</span></span>
<span id="cb4-18">        </span>
<span id="cb4-19">        <span class="cf" style="color: #003B4F;">return</span> np.dot(<span class="bu" style="color: null;">input</span>,<span class="va" style="color: #111111;">self</span>.weights) <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.biases</span>
<span id="cb4-20">    </span>
<span id="cb4-21">    <span class="kw" style="color: #003B4F;">def</span> backward(<span class="va" style="color: #111111;">self</span>,<span class="bu" style="color: null;">input</span>,grad_output):</span>
<span id="cb4-22">        <span class="co" style="color: #5E5E5E;"># compute d f / d x = d f / d dense * d dense / d x</span></span>
<span id="cb4-23">        <span class="co" style="color: #5E5E5E;"># where d dense/ d x = weights transposed</span></span>
<span id="cb4-24">        grad_input <span class="op" style="color: #5E5E5E;">=</span> np.dot(grad_output, <span class="va" style="color: #111111;">self</span>.weights.T)</span>
<span id="cb4-25">        </span>
<span id="cb4-26">        <span class="co" style="color: #5E5E5E;"># compute gradient w.r.t. weights and biases</span></span>
<span id="cb4-27">        grad_weights <span class="op" style="color: #5E5E5E;">=</span> np.dot(<span class="bu" style="color: null;">input</span>.T, grad_output)</span>
<span id="cb4-28">        grad_biases <span class="op" style="color: #5E5E5E;">=</span> grad_output.mean(axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">input</span>.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-29">        </span>
<span id="cb4-30">        <span class="cf" style="color: #003B4F;">assert</span> grad_weights.shape <span class="op" style="color: #5E5E5E;">==</span> <span class="va" style="color: #111111;">self</span>.weights.shape <span class="kw" style="color: #003B4F;">and</span> grad_biases.shape <span class="op" style="color: #5E5E5E;">==</span> <span class="va" style="color: #111111;">self</span>.biases.shape</span>
<span id="cb4-31">        </span>
<span id="cb4-32">        <span class="co" style="color: #5E5E5E;"># Here we perform a stochastic gradient descent step. </span></span>
<span id="cb4-33">        <span class="va" style="color: #111111;">self</span>.weights <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.weights <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> grad_weights</span>
<span id="cb4-34">        <span class="va" style="color: #111111;">self</span>.biases <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.biases <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> grad_biases</span>
<span id="cb4-35">        </span>
<span id="cb4-36">        <span class="cf" style="color: #003B4F;">return</span> grad_input</span></code></pre></div>
</section>
<section id="the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="the-loss-function">The loss function</h3>
<p>Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.</p>
<p>If we write down the expression for crossentropy as a function of softmax logits (a), you’ll see: <br></p>
<p><img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/loss_1.png"> <br> If we take a closer look, we’ll see that it can be rewritten as: <br></p>
<p><img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/loss_2.png"> <br> It’s called Log-softmax and it’s better than naive log(softmax(a)) in all aspects: * Better numerical stability * Easier to get derivative right * Marginally faster to compute</p>
<p>So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> softmax_crossentropy_with_logits(logits,reference_answers):</span>
<span id="cb5-2">    <span class="co" style="color: #5E5E5E;"># Compute crossentropy from logits[batch,n_classes] and ids of correct answers</span></span>
<span id="cb5-3">    logits_for_answers <span class="op" style="color: #5E5E5E;">=</span> logits[np.arange(<span class="bu" style="color: null;">len</span>(logits)),reference_answers]</span>
<span id="cb5-4">    </span>
<span id="cb5-5">    xentropy <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span> logits_for_answers <span class="op" style="color: #5E5E5E;">+</span> np.log(np.<span class="bu" style="color: null;">sum</span>(np.exp(logits),axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb5-6">    </span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;">return</span> xentropy</span>
<span id="cb5-8"></span>
<span id="cb5-9"><span class="kw" style="color: #003B4F;">def</span> grad_softmax_crossentropy_with_logits(logits,reference_answers):</span>
<span id="cb5-10">    <span class="co" style="color: #5E5E5E;"># Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers</span></span>
<span id="cb5-11">    ones_for_answers <span class="op" style="color: #5E5E5E;">=</span> np.zeros_like(logits)</span>
<span id="cb5-12">    ones_for_answers[np.arange(<span class="bu" style="color: null;">len</span>(logits)),reference_answers] <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb5-13">    </span>
<span id="cb5-14">    softmax <span class="op" style="color: #5E5E5E;">=</span> np.exp(logits) <span class="op" style="color: #5E5E5E;">/</span> np.exp(logits).<span class="bu" style="color: null;">sum</span>(axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>,keepdims<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-15">    </span>
<span id="cb5-16">    <span class="cf" style="color: #003B4F;">return</span> (<span class="op" style="color: #5E5E5E;">-</span> ones_for_answers <span class="op" style="color: #5E5E5E;">+</span> softmax) <span class="op" style="color: #5E5E5E;">/</span> logits.shape[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</section>
<section id="full-network" class="level3">
<h3 class="anchored" data-anchor-id="full-network">Full network</h3>
<p>Now let’s combine what we’ve just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let’s import it!.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;">import</span> keras</span>
<span id="cb6-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb6-3"><span class="op" style="color: #5E5E5E;">%</span>matplotlib inline</span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="kw" style="color: #003B4F;">def</span> load_dataset(flatten<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb6-6">    (X_train, y_train), (X_test, y_test) <span class="op" style="color: #5E5E5E;">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb6-7"></span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;"># normalize x</span></span>
<span id="cb6-9">    X_train <span class="op" style="color: #5E5E5E;">=</span> X_train.astype(<span class="bu" style="color: null;">float</span>) <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">255.</span></span>
<span id="cb6-10">    X_test <span class="op" style="color: #5E5E5E;">=</span> X_test.astype(<span class="bu" style="color: null;">float</span>) <span class="op" style="color: #5E5E5E;">/</span> <span class="fl" style="color: #AD0000;">255.</span></span>
<span id="cb6-11"></span>
<span id="cb6-12">    <span class="co" style="color: #5E5E5E;"># we reserve the last 10000 training examples for validation</span></span>
<span id="cb6-13">    X_train, X_val <span class="op" style="color: #5E5E5E;">=</span> X_train[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10000</span>], X_train[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10000</span>:]</span>
<span id="cb6-14">    y_train, y_val <span class="op" style="color: #5E5E5E;">=</span> y_train[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10000</span>], y_train[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10000</span>:]</span>
<span id="cb6-15"></span>
<span id="cb6-16">    <span class="cf" style="color: #003B4F;">if</span> flatten:</span>
<span id="cb6-17">        X_train <span class="op" style="color: #5E5E5E;">=</span> X_train.reshape([X_train.shape[<span class="dv" style="color: #AD0000;">0</span>], <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb6-18">        X_val <span class="op" style="color: #5E5E5E;">=</span> X_val.reshape([X_val.shape[<span class="dv" style="color: #AD0000;">0</span>], <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb6-19">        X_test <span class="op" style="color: #5E5E5E;">=</span> X_test.reshape([X_test.shape[<span class="dv" style="color: #AD0000;">0</span>], <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb6-20"></span>
<span id="cb6-21">    <span class="cf" style="color: #003B4F;">return</span> X_train, y_train, X_val, y_val, X_test, y_test</span>
<span id="cb6-22"></span>
<span id="cb6-23">X_train, y_train, X_val, y_val, X_test, y_test <span class="op" style="color: #5E5E5E;">=</span> load_dataset(flatten<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb6-24"></span>
<span id="cb6-25"><span class="co" style="color: #5E5E5E;">## Let's look at some example</span></span>
<span id="cb6-26">plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">6</span>])</span>
<span id="cb6-27"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb6-28">    plt.subplot(<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>,i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb6-29">    plt.title(<span class="st" style="color: #20794D;">"Label: </span><span class="sc" style="color: #5E5E5E;">%i</span><span class="st" style="color: #20794D;">"</span><span class="op" style="color: #5E5E5E;">%</span>y_train[i])</span>
<span id="cb6-30">    plt.imshow(X_train[i].reshape([<span class="dv" style="color: #AD0000;">28</span>,<span class="dv" style="color: #AD0000;">28</span>]),cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'gray'</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<p><img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/output_20_1.png" align="center"></p>
<p>We’ll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">network <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb7-2">network.append(Dense(X_train.shape[<span class="dv" style="color: #AD0000;">1</span>],<span class="dv" style="color: #AD0000;">100</span>))</span>
<span id="cb7-3">network.append(ReLU())</span>
<span id="cb7-4">network.append(Dense(<span class="dv" style="color: #AD0000;">100</span>,<span class="dv" style="color: #AD0000;">200</span>))</span>
<span id="cb7-5">network.append(ReLU())</span>
<span id="cb7-6">network.append(Dense(<span class="dv" style="color: #AD0000;">200</span>,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> forward(network, X):</span>
<span id="cb8-2">    <span class="co" style="color: #5E5E5E;"># Compute activations of all network layers by applying them sequentially.</span></span>
<span id="cb8-3">    <span class="co" style="color: #5E5E5E;"># Return a list of activations for each layer. </span></span>
<span id="cb8-4">    </span>
<span id="cb8-5">    activations <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb8-6">    <span class="bu" style="color: null;">input</span> <span class="op" style="color: #5E5E5E;">=</span> X</span>
<span id="cb8-7"></span>
<span id="cb8-8">    <span class="co" style="color: #5E5E5E;"># Looping through each layer</span></span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;">for</span> l <span class="kw" style="color: #003B4F;">in</span> network:</span>
<span id="cb8-10">        activations.append(l.forward(<span class="bu" style="color: null;">input</span>))</span>
<span id="cb8-11">        <span class="co" style="color: #5E5E5E;"># Updating input to last layer output</span></span>
<span id="cb8-12">        <span class="bu" style="color: null;">input</span> <span class="op" style="color: #5E5E5E;">=</span> activations[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb8-13">    </span>
<span id="cb8-14">    <span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(activations) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">len</span>(network)</span>
<span id="cb8-15">    <span class="cf" style="color: #003B4F;">return</span> activations</span>
<span id="cb8-16"></span>
<span id="cb8-17"><span class="kw" style="color: #003B4F;">def</span> predict(network,X):</span>
<span id="cb8-18">    <span class="co" style="color: #5E5E5E;"># Compute network predictions. Returning indices of largest Logit probability</span></span>
<span id="cb8-19"></span>
<span id="cb8-20">    logits <span class="op" style="color: #5E5E5E;">=</span> forward(network,X)[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb8-21">    <span class="cf" style="color: #003B4F;">return</span> logits.argmax(axis<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb8-22"></span>
<span id="cb8-23"><span class="kw" style="color: #003B4F;">def</span> train(network,X,y):</span>
<span id="cb8-24">    <span class="co" style="color: #5E5E5E;"># Train our network on a given batch of X and y.</span></span>
<span id="cb8-25">    <span class="co" style="color: #5E5E5E;"># We first need to run forward to get all layer activations.</span></span>
<span id="cb8-26">    <span class="co" style="color: #5E5E5E;"># Then we can run layer.backward going from last to first layer.</span></span>
<span id="cb8-27">    <span class="co" style="color: #5E5E5E;"># After we have called backward for all layers, all Dense layers have already made one gradient step.</span></span>
<span id="cb8-28">    </span>
<span id="cb8-29">    </span>
<span id="cb8-30">    <span class="co" style="color: #5E5E5E;"># Get the layer activations</span></span>
<span id="cb8-31">    layer_activations <span class="op" style="color: #5E5E5E;">=</span> forward(network,X)</span>
<span id="cb8-32">    layer_inputs <span class="op" style="color: #5E5E5E;">=</span> [X]<span class="op" style="color: #5E5E5E;">+</span>layer_activations  <span class="co" style="color: #5E5E5E;">#layer_input[i] is an input for network[i]</span></span>
<span id="cb8-33">    logits <span class="op" style="color: #5E5E5E;">=</span> layer_activations[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb8-34">    </span>
<span id="cb8-35">    <span class="co" style="color: #5E5E5E;"># Compute the loss and the initial gradient</span></span>
<span id="cb8-36">    loss <span class="op" style="color: #5E5E5E;">=</span> softmax_crossentropy_with_logits(logits,y)</span>
<span id="cb8-37">    loss_grad <span class="op" style="color: #5E5E5E;">=</span> grad_softmax_crossentropy_with_logits(logits,y)</span>
<span id="cb8-38">    </span>
<span id="cb8-39">    <span class="co" style="color: #5E5E5E;"># Propagate gradients through the network</span></span>
<span id="cb8-40">    <span class="co" style="color: #5E5E5E;"># Reverse propogation as this is backprop</span></span>
<span id="cb8-41">    <span class="cf" style="color: #003B4F;">for</span> layer_index <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(network))[::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]:</span>
<span id="cb8-42">        layer <span class="op" style="color: #5E5E5E;">=</span> network[layer_index]</span>
<span id="cb8-43">        </span>
<span id="cb8-44">        loss_grad <span class="op" style="color: #5E5E5E;">=</span> layer.backward(layer_inputs[layer_index],loss_grad) <span class="co" style="color: #5E5E5E;">#grad w.r.t. input, also weight updates</span></span>
<span id="cb8-45">        </span>
<span id="cb8-46">    <span class="cf" style="color: #003B4F;">return</span> np.mean(loss)</span></code></pre></div>
</section>
<section id="training-loop" class="level3">
<h3 class="anchored" data-anchor-id="training-loop">Training loop</h3>
<p>We split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;">from</span> tqdm <span class="im" style="color: #00769E;">import</span> trange</span>
<span id="cb9-2"><span class="kw" style="color: #003B4F;">def</span> iterate_minibatches(inputs, targets, batchsize, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb9-3">    <span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(inputs) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">len</span>(targets)</span>
<span id="cb9-4">    <span class="cf" style="color: #003B4F;">if</span> shuffle:</span>
<span id="cb9-5">        indices <span class="op" style="color: #5E5E5E;">=</span> np.random.permutation(<span class="bu" style="color: null;">len</span>(inputs))</span>
<span id="cb9-6">    <span class="cf" style="color: #003B4F;">for</span> start_idx <span class="kw" style="color: #003B4F;">in</span> trange(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">len</span>(inputs) <span class="op" style="color: #5E5E5E;">-</span> batchsize <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>, batchsize):</span>
<span id="cb9-7">        <span class="cf" style="color: #003B4F;">if</span> shuffle:</span>
<span id="cb9-8">            excerpt <span class="op" style="color: #5E5E5E;">=</span> indices[start_idx:start_idx <span class="op" style="color: #5E5E5E;">+</span> batchsize]</span>
<span id="cb9-9">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb9-10">            excerpt <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">slice</span>(start_idx, start_idx <span class="op" style="color: #5E5E5E;">+</span> batchsize)</span>
<span id="cb9-11">        <span class="cf" style="color: #003B4F;">yield</span> inputs[excerpt], targets[excerpt]</span></code></pre></div>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;">from</span> IPython.display <span class="im" style="color: #00769E;">import</span> clear_output</span>
<span id="cb10-2">train_log <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb10-3">val_log <span class="op" style="color: #5E5E5E;">=</span> []</span></code></pre></div>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">25</span>):</span>
<span id="cb11-2"></span>
<span id="cb11-3">    <span class="cf" style="color: #003B4F;">for</span> x_batch,y_batch <span class="kw" style="color: #003B4F;">in</span> iterate_minibatches(X_train,y_train,batchsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">32</span>,shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>):</span>
<span id="cb11-4">        train(network,x_batch,y_batch)</span>
<span id="cb11-5">    </span>
<span id="cb11-6">    train_log.append(np.mean(predict(network,X_train)<span class="op" style="color: #5E5E5E;">==</span>y_train))</span>
<span id="cb11-7">    val_log.append(np.mean(predict(network,X_val)<span class="op" style="color: #5E5E5E;">==</span>y_val))</span>
<span id="cb11-8">    </span>
<span id="cb11-9">    clear_output()</span>
<span id="cb11-10">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Epoch"</span>,epoch)</span>
<span id="cb11-11">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Train accuracy:"</span>,train_log[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-12">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Val accuracy:"</span>,val_log[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb11-13">    plt.plot(train_log,label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'train accuracy'</span>)</span>
<span id="cb11-14">    plt.plot(val_log,label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'val accuracy'</span>)</span>
<span id="cb11-15">    plt.legend(loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'best'</span>)</span>
<span id="cb11-16">    plt.grid()</span>
<span id="cb11-17">    plt.show()</span>
<span id="cb11-18">    </span></code></pre></div>
<pre><code>Epoch 24
Train accuracy: 1.0
Val accuracy: 0.9809</code></pre>
<p><img src="https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/output_27_1.png" align="center"></p>
<p>As we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!</p>


</section>
</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Deep Learning</category>
  <guid>https://aayushmnit.github.io/posts/2018-06-03-Building_neural_network_from_scratch/index.html</guid>
  <pubDate>Sun, 03 Jun 2018 07:00:00 GMT</pubDate>
  <media:content url="https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/single_layer_mlp.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Solving business usecases by recommender system using lightFM</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html</link>
  <description><![CDATA[ 



<p>In this post, I am going to write about Recommender systems, how they are used in many e-commerce websites. The post will also cover about building simple recommender system models using Matrix Factorization algorithm using <a href="https://github.com/lyst/lightfm">lightFM</a> package and my <a href="https://github.com/aayushmnit/cookbook/blob/master/recsys.py">recommender system cookbook</a>. The post will focus on business use cases and simple implementations. The post only cover basic intuition around algorithms and will provide links to resources if you want to understand the math behind the algorithm.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>I am an avid reader and a believer in open source education and continuously expand my knowledge around data science &amp; computer science using online courses, blogs, Github repositories and participating in data science competitions. While searching for quality content on the internet, I have come across various learning links which either focus on the implementation of the algorithm using specific data/modeling technique in ABC language or focus on business impact/results using the broad concept of a family of algorithms(like classification, forecasting, recommender systems etc.) but don’t go into details of how to do it. So the idea is to write some blogs which can combine both business use cases with codes &amp; algorithmic intuition to provide a holistic view of how data science is used in business scenarios. <br></p>
<p>As the world is becoming more digital, we are already getting used to a lot of personalized experience and the algorithm which help us achieve this falls in the family of recommender systems. Almost every web-based platform is using some recommender system to provide customized content. Following are the companies I admire the most. <img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/admired_companies.PNG"></p>
</section>
<section id="what-is-personalization" class="level2">
<h2 class="anchored" data-anchor-id="what-is-personalization">What is personalization?</h2>
<p>Personalization is a technique of dynamically tailoring your content based on needs of each user. Simple examples of personalization could be movie recommendation on Netflix, personalized email targeting/re-targeting by e-commerce platforms, item recommendation on Amazon, etc. Personalization helps us achieve these four Rs - - <strong>Recognize:</strong> Know customer’s and prospects’ profiles, including demographics, geography, and expressed and shared interests. - <strong>Remember:</strong> Recall customers’ history, primarily how they act as expressed by what they browse and buy - <strong>Reach:</strong> Deliver the right promotion, content, recommendation for a customer based on actions, preferences, and interests - <strong>Relevance:</strong> Deliver personalization within the context of the digital experience based on who customers are, where they are located and what time of year it is</p>
<p><img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/4r_personalization.PNG"></p>
</section>
<section id="why-personalization" class="level2">
<h2 class="anchored" data-anchor-id="why-personalization">Why personalization?</h2>
<p>Personalization has a lot of benefits for both users and companies. For users, it makes their life easy as they only get to see more relevant stuff to them (unless it’s an advertisement, even they are personalized). For business benefits are countless but here are few which I would like to mention - - <strong>Enhance customer experience:</strong> Personalization reduces the clutter and enhances the customer experience by showing relevant content - <strong>Cross-sell/ Up-sell opportunities:</strong> Relevant product offerings based on customer preferences can lead to increasing products visibility and eventually selling more products - <strong>Increased basket size:</strong> Personalized experience and targeting ultimately leads to increased basket size and frequent purchases - <strong>Increased customer loyalty:</strong> In the digital world, customer retention/loyalty is the most prominent problem faced by many companies as finding a replacement for a particular service is quite easy. According to a <a href="https://www.forbes.com/sites/shephyken/2017/10/29/personalized-customer-experience-increases-revenue-and-loyalty/#36e9f054bd61">Forbes article</a>, Forty-four percent of consumers say they will likely repeat after a personalized experience</p>
<p><img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/personalization_benefits.PNG"></p>
</section>
<section id="introduction-to-matrix-factorization" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-matrix-factorization">Introduction to Matrix factorization</h2>
<p>Matrix factorization is one of the algorithms from recommender systems family and as the name suggests it factorize a matrix, i.e., decompose a matrix in two(or more) matrices such that once you multiply them you get your original matrix back. In case of the recommendation system, we will typically start with an interaction/rating matrix between users and items and matrix factorization algorithm will decompose this matrix in user and item feature matrix which is also known as embeddings. Example of interaction matrix would be user-movie ratings for movie recommender, user-product purchase flag for transaction data, etc. <br> <img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/matrix_decomposition.png"></p>
<p><br> Typically user/item embeddings capture latent features about attributes of users and item respectively. Essentially, latent features are the representation of user/item in an arbitrary space which represents how a user rate a movie. In the example of a movie recommender, an example of user embedding might represent affinity of a user to watch serious kind of movie when the value of the latent feature is high and comedy type of movie when the value is low. Similarly, a movie latent feature may have a high value when the movie is more male driven and when it’s more female-driven the value is typically low. <br> <img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/latent_feature.gif"></p>
<p>For more information on matrix factorization and factorization machines you can read these articles - <br> <a href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/">Matrix Factorization: A Simple Tutorial and Implementation in Python</a> <br> <a href="https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/">Introductory Guide – Factorization Machines &amp; their application on huge datasets (with codes in Python)</a></p>
</section>
<section id="handon-building-recommender-system-using-lightfm-package-in-python" class="level2">
<h2 class="anchored" data-anchor-id="handon-building-recommender-system-using-lightfm-package-in-python">HandOn: Building recommender system using LightFM package in Python</h2>
<p>In the hands-on section, we will be building recommender system for different scenarios which we typically see in many companies using LightFM package and <a href="https://grouplens.org/datasets/movielens/">MovieLens</a> data. We are using <a href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip">small size data</a> which contains 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users</p>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>Let’s start by importing data, <a href="https://github.com/aayushmnit/cookbook/blob/master/recsys.py">recommender system cookbook</a> and <a href="https://github.com/aayushmnit/cookbook/blob/master/generic_preprocessing.py">preprocessing cookbook</a> files for this hands-on section. I have written these reusable generic <a href="https://github.com/aayushmnit/cookbook">cookbook codes</a> to increase productivity and write clean/modular codes; you will see we can build a recommender system using 10-15 lines of code by using these cookbooks(do more with less!).</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Importing Libraries and cookbooks</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> recsys <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="co" style="color: #5E5E5E;">## recommender system cookbook</span></span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> generic_preprocessing <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="co" style="color: #5E5E5E;">## pre-processing code</span></span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> IPython.display <span class="im" style="color: #00769E;">import</span> HTML <span class="co" style="color: #5E5E5E;">## Setting display options for Ipython Notebook</span></span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Importing rating data and having a look</span></span>
<span id="cb2-2">ratings <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(<span class="st" style="color: #20794D;">'./ml-latest-small/ratings.csv'</span>)</span>
<span id="cb2-3">ratings.head()</span></code></pre></div>
<div>


<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
userId
</th>
<th>
movieId
</th>
<th>
rating
</th>
<th>
timestamp
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
31
</td>
<td>
2.5
</td>
<td>
1260759144
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
1029
</td>
<td>
3.0
</td>
<td>
1260759179
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
1061
</td>
<td>
3.0
</td>
<td>
1260759182
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
1129
</td>
<td>
2.0
</td>
<td>
1260759185
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1
</td>
<td>
1172
</td>
<td>
4.0
</td>
<td>
1260759205
</td>
</tr>
</tbody>

</table>
</div>
<p>As we can see rating data contain user id, movie id and a rating between 0.5 to 5 with a timestamp representing when the rating was given.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Importing movie data and having a look at first five columns</span></span>
<span id="cb3-2">movies <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(<span class="st" style="color: #20794D;">'./ml-latest-small/movies.csv'</span>)</span>
<span id="cb3-3">movies.head()</span></code></pre></div>
<div>


<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
movieId
</th>
<th>
title
</th>
<th>
genres
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
Toy Story (1995)
</td>
<td>
Adventure|Animation|Children|Comedy|Fantasy
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
Jumanji (1995)
</td>
<td>
Adventure|Children|Fantasy
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
Grumpier Old Men (1995)
</td>
<td>
Comedy|Romance
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
Waiting to Exhale (1995)
</td>
<td>
Comedy|Drama|Romance
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
Father of the Bride Part II (1995)
</td>
<td>
Comedy
</td>
</tr>
</tbody>

</table>
</div>
<p>Movie data consist of movie id, their title, and genre they belong.</p>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>As I mentioned before, to create a recommender system we need to start by creating an interaction matrix. For this task, we will use the <strong>create_interaction_matrix</strong> function from the recsys cookbook. This function requires you to input a pandas dataframe and necessary information like column name for user id, item id, and rating. It also takes an additional parameter <strong>threshold</strong> if norm=True which means any rating above the mentioned threshold is considered a positive rating. In our case, we don’t have to normalize our data, but in cases of retail data any purchase of a particular type of item can be considered a positive rating, quantity doesn’t matter.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># Creating interaction matrix using rating data</span></span>
<span id="cb4-2">interactions <span class="op" style="color: #5E5E5E;">=</span> create_interaction_matrix(df <span class="op" style="color: #5E5E5E;">=</span> ratings,</span>
<span id="cb4-3">                                         user_col <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'userId'</span>,</span>
<span id="cb4-4">                                         item_col <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'movieId'</span>,</span>
<span id="cb4-5">                                         rating_col <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'rating'</span>)</span>
<span id="cb4-6">interactions.head()</span></code></pre></div>
<div>


<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
movieId
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
…
</th>
<th>
161084
</th>
<th>
161155
</th>
<th>
161594
</th>
<th>
161830
</th>
<th>
161918
</th>
<th>
161944
</th>
<th>
162376
</th>
<th>
162542
</th>
<th>
162672
</th>
<th>
163949
</th>
</tr>
<tr>
<th>
userId
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
…
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
4.0
</td>
<td>
…
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
…
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
4.0
</td>
<td>
…
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
4.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
…
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>

</table>
<p>
5 rows × 9066 columns
</p>
</div>
<p>As we can see the data is created in an interaction format where rows represent each user and columns represent each movie id with ratings as values. <br> We will also create user and item dictionaries to later convert user_id to user_name or movie_id to movie_name by using <strong>create_user_dict</strong> and <strong>create_item dict</strong> function.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># Create User Dict</span></span>
<span id="cb5-2">user_dict <span class="op" style="color: #5E5E5E;">=</span> create_user_dict(interactions<span class="op" style="color: #5E5E5E;">=</span>interactions)</span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;"># Create Item dict</span></span>
<span id="cb5-4">movies_dict <span class="op" style="color: #5E5E5E;">=</span> create_item_dict(df <span class="op" style="color: #5E5E5E;">=</span> movies,</span>
<span id="cb5-5">                               id_col <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'movieId'</span>,</span>
<span id="cb5-6">                               name_col <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'title'</span>)</span></code></pre></div>
</section>
<section id="building-matrix-factorization-model" class="level3">
<h3 class="anchored" data-anchor-id="building-matrix-factorization-model">Building Matrix Factorization model</h3>
<p>To build a matrix factorization model, we will use the <strong>runMF</strong> function which will take following input -<br>
- <strong>interaction matrix:</strong> Interaction matrix created in the previous section - <strong>n_components:</strong> Number of embedding generated for each user and item - <strong>loss:</strong> We need to define a loss function, in this case, we are using <a href="https://lyst.github.io/lightfm/docs/examples/warp_loss.html">warp loss</a> because we mostly care about the ranking of data, i.e, which items should we show first - <strong>epoch:</strong> Number of times to run - <strong>n_jobs:</strong> Number of cores to use in parallel processing</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">mf_model <span class="op" style="color: #5E5E5E;">=</span> runMF(interactions <span class="op" style="color: #5E5E5E;">=</span> interactions,</span>
<span id="cb6-2">                 n_components <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span>,</span>
<span id="cb6-3">                 loss <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'warp'</span>,</span>
<span id="cb6-4">                 epoch <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span>,</span>
<span id="cb6-5">                 n_jobs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
<p>Now we have built our matrix factorization model we can now do some interesting things. There are various use cases which can be solved by using this model for a web platform let’s look into them.</p>
</section>
<section id="usecase-1-item-recommendation-to-a-user" class="level3">
<h3 class="anchored" data-anchor-id="usecase-1-item-recommendation-to-a-user">Usecase 1: Item recommendation to a user</h3>
<p>In this use case, we want to show a user, items he might be interested in buying/viewing based on his/her interactions done in the past. Typical industry examples for this are like “Deals recommended for you” on Amazon or “Top pics for a user” on Netflix or personalized email campaigns. <img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/user_item_recommendation.PNG"></p>
<p>We can use the <strong>sample_recommendation_user</strong> function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, user_id and the number of items as input and return the list of item id’s a user may be interested in interacting.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;">## Calling 10 movie recommendation for user id 11</span></span>
<span id="cb7-2">rec_list <span class="op" style="color: #5E5E5E;">=</span> sample_recommendation_user(model <span class="op" style="color: #5E5E5E;">=</span> mf_model, </span>
<span id="cb7-3">                                      interactions <span class="op" style="color: #5E5E5E;">=</span> interactions, </span>
<span id="cb7-4">                                      user_id <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">11</span>, </span>
<span id="cb7-5">                                      user_dict <span class="op" style="color: #5E5E5E;">=</span> user_dict,</span>
<span id="cb7-6">                                      item_dict <span class="op" style="color: #5E5E5E;">=</span> movies_dict, </span>
<span id="cb7-7">                                      threshold <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb7-8">                                      nrec_items <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb7-9">                                      show <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
<pre><code>Known Likes:
1- The Hunger Games: Catching Fire (2013)
2- Gravity (2013)
3- Dark Knight Rises, The (2012)
4- The Hunger Games (2012)
5- Town, The (2010)
6- Exit Through the Gift Shop (2010)
7- Bank Job, The (2008)
8- Departed, The (2006)
9- Bourne Identity, The (1988)
10- Step Into Liquid (2002)
11- SLC Punk! (1998)
12- Last of the Mohicans, The (1992)
13- Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)
14- Robin Hood: Prince of Thieves (1991)
15- Citizen Kane (1941)
16- Trainspotting (1996)
17- Pulp Fiction (1994)
18- Usual Suspects, The (1995)

 Recommended Items:
1- Dark Knight, The (2008)
2- Inception (2010)
3- Iron Man (2008)
4- Shutter Island (2010)
5- Fight Club (1999)
6- Avatar (2009)
7- Forrest Gump (1994)
8- District 9 (2009)
9- WALL·E (2008)
10- Matrix, The (1999)</code></pre>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;">print</span>(rec_list)</span></code></pre></div>
<pre><code>[593L, 260L, 110L, 480L, 47L, 527L, 344L, 858L, 231L, 780L]</code></pre>
<p>As we can see in this case user is interested in <em>“Dark Knight Rises(2012)”</em> so the first recommendation is <em>“The Dark Knight(2008)”</em>. This user also seems to have a strong liking towards movies in drama, sci-fi and thriller genre and there are many movies recommended in the same genre like Dark Knight(Drama/Crime), Inception(Sci-Fi, Thriller), Iron Man(Sci-FI thriller), Shutter Island(Drame/Thriller), Fight club(drama), Avatar(Sci-fi), Forrest Gump(Drama), District 9(Thriller), Wall-E(Sci-fi), The Matrix(Sci-Fi) <br></p>
<p>Similar models can also be used for building sections like “Based on your recent browsing history” recommendations by just changing the rating matrix only to contain interaction which is recent and based on browsing history visits on specific items.</p>
</section>
<section id="usecase-2-user-recommendation-to-a-item" class="level3">
<h3 class="anchored" data-anchor-id="usecase-2-user-recommendation-to-a-item">Usecase 2: User recommendation to a item</h3>
<p>In this use case, we will discuss how we can recommend a list of users specific to a particular item. Example of such cases is when you are running a promotion on an item and want to run an e-mail campaign around this promotional item to only 10,000 users who might be interested in this item.</p>
<p><img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/item_user_recommendation.jpg"></p>
<p>We can use the <strong>sample_recommendation_item</strong> function for this case. This functions take matrix factorization model, interaction matrix, user dictionary, item dictionary, item_id and the number of users as input and return the list of user id’s who are more likely be interested in the item.</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;">## Calling 15 user recommendation for item id 1</span></span>
<span id="cb11-2">sample_recommendation_item(model <span class="op" style="color: #5E5E5E;">=</span> mf_model,</span>
<span id="cb11-3">                           interactions <span class="op" style="color: #5E5E5E;">=</span> interactions,</span>
<span id="cb11-4">                           item_id <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb11-5">                           user_dict <span class="op" style="color: #5E5E5E;">=</span> user_dict,</span>
<span id="cb11-6">                           item_dict <span class="op" style="color: #5E5E5E;">=</span> movies_dict,</span>
<span id="cb11-7">                           number_of_user <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">15</span>)</span></code></pre></div>
<pre><code>[116, 410, 449, 657, 448, 633, 172, 109, 513, 44, 498, 459, 317, 415, 495]</code></pre>
<p>As you can see function return a list of userID who might be interested in item id 1. Another example why you might need such model is when there is an old inventory sitting in your warehouse which needs to clear up otherwise you might have to write it off, and you want to clear it by giving some discount to users who might be interested in buying.</p>
</section>
<section id="usecase-3-item-recommendation-to-items" class="level3">
<h3 class="anchored" data-anchor-id="usecase-3-item-recommendation-to-items">Usecase 3: Item recommendation to items</h3>
<p>In this use case, we will discuss how we can recommend a list of items specific to a particular item. This kind of models will help you to find similar/related items or items which can be bundled together. Typical industry use case for such models are in cross-selling and up-selling opportunities on product page like “Products related to this item”, “Frequently bought together”, “Customers who bought this also bought this” and “Customers who viewed this item also viewed”. <br> <em>“Customers who bought this also bought this” and “Customers who viewed this item also viewed” can also be solved through market basket analysis.</em></p>
<p><img src="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/crossell_bundling.PNG"></p>
<p>To achieve this use case, we will create a cosine distance matrix using item embeddings generated by matrix factorization model. This will help us calculate similarity b/w items, and then we can recommend top N similar item to an item of interest. First step is to create a item-item distance matrix using the <strong>create_item_emdedding_distance_matrix</strong> function. This function takes matrix factorization models and interaction matrix as input and returns an item_embedding_distance_matrix.</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;">## Creating item-item distance matrix</span></span>
<span id="cb13-2">item_item_dist <span class="op" style="color: #5E5E5E;">=</span> create_item_emdedding_distance_matrix(model <span class="op" style="color: #5E5E5E;">=</span> mf_model,</span>
<span id="cb13-3">                                                       interactions <span class="op" style="color: #5E5E5E;">=</span> interactions)</span></code></pre></div>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="co" style="color: #5E5E5E;">## Checking item embedding distance matrix</span></span>
<span id="cb14-2">item_item_dist.head()</span></code></pre></div>
<div>


<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
movieId
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
<th>
…
</th>
<th>
161084
</th>
<th>
161155
</th>
<th>
161594
</th>
<th>
161830
</th>
<th>
161918
</th>
<th>
161944
</th>
<th>
162376
</th>
<th>
162542
</th>
<th>
162672
</th>
<th>
163949
</th>
</tr>
<tr>
<th>
movieId
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
1.000000
</td>
<td>
0.760719
</td>
<td>
0.491280
</td>
<td>
0.427250
</td>
<td>
0.484597
</td>
<td>
0.740024
</td>
<td>
0.486644
</td>
<td>
0.094009
</td>
<td>
-0.083986
</td>
<td>
0.567389
</td>
<td>
…
</td>
<td>
-0.732112
</td>
<td>
-0.297997
</td>
<td>
-0.451733
</td>
<td>
-0.767141
</td>
<td>
-0.501647
</td>
<td>
-0.270280
</td>
<td>
-0.455277
</td>
<td>
-0.292823
</td>
<td>
-0.337935
</td>
<td>
-0.636147
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.760719
</td>
<td>
1.000000
</td>
<td>
0.446414
</td>
<td>
0.504502
</td>
<td>
0.525171
</td>
<td>
0.572113
</td>
<td>
0.364393
</td>
<td>
0.290633
</td>
<td>
0.231926
</td>
<td>
0.653033
</td>
<td>
…
</td>
<td>
-0.748452
</td>
<td>
-0.307634
</td>
<td>
-0.165400
</td>
<td>
-0.526614
</td>
<td>
-0.146751
</td>
<td>
-0.156305
</td>
<td>
-0.223818
</td>
<td>
-0.138412
</td>
<td>
-0.209538
</td>
<td>
-0.733489
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.491280
</td>
<td>
0.446414
</td>
<td>
1.000000
</td>
<td>
0.627473
</td>
<td>
0.769991
</td>
<td>
0.544175
</td>
<td>
0.632008
</td>
<td>
0.336824
</td>
<td>
0.392284
</td>
<td>
0.510592
</td>
<td>
…
</td>
<td>
-0.331028
</td>
<td>
-0.264556
</td>
<td>
-0.308592
</td>
<td>
-0.285085
</td>
<td>
-0.046424
</td>
<td>
-0.165821
</td>
<td>
-0.183842
</td>
<td>
-0.143613
</td>
<td>
-0.156418
</td>
<td>
-0.378811
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.427250
</td>
<td>
0.504502
</td>
<td>
0.627473
</td>
<td>
1.000000
</td>
<td>
0.582582
</td>
<td>
0.543208
</td>
<td>
0.602390
</td>
<td>
0.655708
</td>
<td>
0.527346
</td>
<td>
0.471166
</td>
<td>
…
</td>
<td>
-0.380431
</td>
<td>
-0.163091
</td>
<td>
-0.232833
</td>
<td>
-0.334746
</td>
<td>
-0.052832
</td>
<td>
-0.266185
</td>
<td>
-0.158415
</td>
<td>
-0.211618
</td>
<td>
-0.232351
</td>
<td>
-0.469629
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0.484597
</td>
<td>
0.525171
</td>
<td>
0.769991
</td>
<td>
0.582582
</td>
<td>
1.000000
</td>
<td>
0.354141
</td>
<td>
0.639958
</td>
<td>
0.396447
</td>
<td>
0.432026
</td>
<td>
0.385051
</td>
<td>
…
</td>
<td>
-0.273074
</td>
<td>
-0.280585
</td>
<td>
-0.306195
</td>
<td>
-0.265243
</td>
<td>
0.012961
</td>
<td>
-0.225142
</td>
<td>
-0.317043
</td>
<td>
-0.136875
</td>
<td>
-0.122382
</td>
<td>
-0.312858
</td>
</tr>
</tbody>

</table>
<p>
5 rows × 9066 columns
</p>
</div>
<p>As we can see the matrix have movies as both row and columns and the value represents the cosine distance between them. Next step is to use <strong>item_item_recommendation</strong> function to get top N items with respect to an item_id. This function takes item embedding distance matrix, item_id, item_dictionary and number of items to be recommended as input and return similar item list as output.</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;">## Calling 10 recommended items for item id </span></span>
<span id="cb15-2">rec_list <span class="op" style="color: #5E5E5E;">=</span> item_item_recommendation(item_emdedding_distance_matrix <span class="op" style="color: #5E5E5E;">=</span> item_item_dist,</span>
<span id="cb15-3">                                    item_id <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5378</span>,</span>
<span id="cb15-4">                                    item_dict <span class="op" style="color: #5E5E5E;">=</span> movies_dict,</span>
<span id="cb15-5">                                    n_items <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
<pre><code>Item of interest :Star Wars: Episode II - Attack of the Clones (2002)
Item similar to the above item:
1- Star Wars: Episode III - Revenge of the Sith (2005)
2- Lord of the Rings: The Two Towers, The (2002)
3- Lord of the Rings: The Fellowship of the Ring, The (2001)
4- Lord of the Rings: The Return of the King, The (2003)
5- Matrix Reloaded, The (2003)
6- Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)
7- Gladiator (2000)
8- Spider-Man (2002)
9- Minority Report (2002)
10- Mission: Impossible II (2000)</code></pre>
<p>As we can see for “Star Wars: Episode II - Attack of the Clones (2002)” movie we are getting it’s next released movies which is “Star Wars: Episode III - Revenge of the Sith (2005)” as the first recommendation.</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Like any other blog, this method isn’t perfect for every application, but the same ideas can work if we use it effectively. There is a lot of advancements in recommender systems with the advent of Deep learning. While there is room for improvement, I am pleased with how it has been working for me so far. I might write about deep learning based recommender systems later sometime.</p>
<p>In the meantime, I hope you enjoyed reading, and feel free to use my code to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on <a href="https://www.linkedin.com/in/aayushmnit/">LinkedIn</a> or email me at aayushmnit@gmail.com.</p>


</section>

 ]]></description>
  <category>Recommender System</category>
  <category>Machine learning</category>
  <category>Business</category>
  <guid>https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/index.html</guid>
  <pubDate>Tue, 17 Apr 2018 07:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2018-04-17-Business_usecases_by_recommender_system_using_lightFM/personalization_benefits.PNG" medium="image"/>
</item>
<item>
  <title>Website launch</title>
  <dc:creator>Aayush Agrawal</dc:creator>
  <link>https://aayushmnit.github.io/posts/2018-02-19-launch/index.html</link>
  <description><![CDATA[ 



<p>Finally got time to build my own website on <a href="https://github.com/">Github</a>. This is my first post and I would like to thank <a href="https://github.com/academicpages/academicpages.github.io">Academic Pages</a> which provided with a wonderful repository to help me get started with building this website.</p>
<p>I am intending to use this website to publish blogs, workshop and any material which would be releveant in Data science field.</p>
<p>Will see you guys in next post.</p>
<p>-Aayush</p>



 ]]></description>
  <category>launch</category>
  <category>announcement</category>
  <guid>https://aayushmnit.github.io/posts/2018-02-19-launch/index.html</guid>
  <pubDate>Mon, 19 Feb 2018 08:00:00 GMT</pubDate>
  <media:content url="https://aayushmnit.github.io/posts/2018-02-19-launch/launch.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
